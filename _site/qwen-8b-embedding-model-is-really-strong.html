<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Qwen-8B Embeddings: Near-SOTA Performance at 600x the Speed | Alex Dong’s Blog</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Qwen-8B Embeddings: Near-SOTA Performance at 600x the Speed" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="TLDR: Qwen-8B embeddings achieve near state-of-the-art accuracy while running 600x faster, fundamentally changing the cost-performance equation for text classification tasks." />
<meta property="og:description" content="TLDR: Qwen-8B embeddings achieve near state-of-the-art accuracy while running 600x faster, fundamentally changing the cost-performance equation for text classification tasks." />
<link rel="canonical" href="http://localhost:4000/qwen-8b-embedding-model-is-really-strong.html" />
<meta property="og:url" content="http://localhost:4000/qwen-8b-embedding-model-is-really-strong.html" />
<meta property="og:site_name" content="Alex Dong’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-09-16T22:06:00+12:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Qwen-8B Embeddings: Near-SOTA Performance at 600x the Speed" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-16T22:06:00+12:00","datePublished":"2025-09-16T22:06:00+12:00","description":"TLDR: Qwen-8B embeddings achieve near state-of-the-art accuracy while running 600x faster, fundamentally changing the cost-performance equation for text classification tasks.","headline":"Qwen-8B Embeddings: Near-SOTA Performance at 600x the Speed","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/qwen-8b-embedding-model-is-really-strong.html"},"url":"http://localhost:4000/qwen-8b-embedding-model-is-really-strong.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Alex Dong&apos;s Blog" /></head><body><header class="site-header" role="banner">
  <div class="wrapper">
    <div style="text-align: center;">
      <a class="site-title" rel="author" href="/">Stream of thoughts from Alex Dong</a>
      <p style="margin: 0; font-size: 0.875rem; color: var(--meta-color);">What I find interesting, intriguing or insightful</p>
    </div>
  </div>
</header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Qwen-8B Embeddings: Near-SOTA Performance at 600x the Speed</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-09-16T22:06:00+12:00" itemprop="datePublished">Sep 16, 2025
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>TLDR: Qwen-8B embeddings achieve near state-of-the-art accuracy while running 600x faster, fundamentally changing the cost-performance equation for text classification tasks.</p>

<p>Working on the <a href="https://www.kaggle.com/competitions/map-charting-student-math-misunderstandings">Kaggle MAP</a> competition has given me an opportunity to systematically compare embedding models for text classification tasks. The results challenge conventional assumptions about model size and performance trade-offs.</p>

<p>The architecture deliberately separates representation learning from classification: text → embedding model → vector → 3-layer MLP classifier. This modular design enables rapid experimentation with different embedding models while keeping the downstream architecture constant.</p>

<p>I began with <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"><code class="language-plaintext highlighter-rouge">sentence-transformers/all-MiniLM-L6-v2</code></a>, a 22.7M parameter model that serves as the industry baseline. The <code class="language-plaintext highlighter-rouge">sentence-transformers</code> Python package enabled rapid prototyping—from zero to working system in three hours. After optimization, performance plateaued at 0.908 MAP score. While respectable for a lightweight model, this fell short of competitive requirements.</p>

<p>DeepMind’s recent release of <a href="https://huggingface.co/google/embeddinggemma-300m">embedding-Gemma-300M</a> promised substantial gains. Their benchmarks show it outperforming <code class="language-plaintext highlighter-rouge">all-MiniLM-L6-v2</code> across the board and beating <code class="language-plaintext highlighter-rouge">multilingual-e5-large</code> on all <a href="https://developers.googleblog.com/en/introducing-embeddinggemma/">MTEB tasks</a>. My results: marginal improvement to 0.9101 MAP—a 0.2% gain that didn’t justify the 13x parameter increase.</p>

<p>The modest gains suggested I’d reached the MLP architecture’s limits. While exploring LLM-as-classifier approaches, I realized Qwen-8B could serve as an embedding model—extracting its hidden state representations rather than using its generative capabilities. Though <code class="language-plaintext highlighter-rouge">sentence-transformers</code> doesn’t natively support this, <code class="language-plaintext highlighter-rouge">llama-cpp-python</code> provides direct access to the model’s internal representations.</p>

<p>The results shifted my understanding of the embedding-performance relationship. MAP score: 0.9439—a 4% absolute improvement that brings us within striking distance of state-of-the-art.</p>

<p>This performance gain becomes strategically significant when considering deployment economics. The current Kaggle leader achieves 0.952 MAP using full LLM inference at 10-20 seconds per call. Our Qwen-8B + MLP approach delivers 99.3% of that accuracy at 0.03 seconds per inference—a 600x speedup that fundamentally changes the viability equation for production systems.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Parameters</th>
      <th>Embedding Dimensions</th>
      <th>MAP@3 Score</th>
      <th>Inference Time</th>
      <th>Relative Speed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>all-MiniLM-L6-v2</td>
      <td>32M</td>
      <td>384</td>
      <td>0.9082</td>
      <td>0.02s</td>
      <td>1x (baseline)</td>
    </tr>
    <tr>
      <td>Gemma-300M</td>
      <td>300M</td>
      <td>768</td>
      <td>0.9101</td>
      <td>0.025s</td>
      <td>0.8x</td>
    </tr>
    <tr>
      <td>Qwen3-8B</td>
      <td>8B</td>
      <td>4096</td>
      <td>0.9439</td>
      <td>0.03s</td>
      <td>0.67x</td>
    </tr>
    <tr>
      <td>SOTA (LLM)</td>
      <td>-</td>
      <td>-</td>
      <td>0.9520</td>
      <td>10-20s</td>
      <td>0.002x</td>
    </tr>
  </tbody>
</table>

<p>These results validate a critical hypothesis in representation learning: superior embeddings enable simpler downstream architectures. The Qwen-8B model, trained on substantially more diverse data than specialized embedding models, captures richer semantic relationships in its 4096-dimensional space. This richness allows a basic 3-layer MLP to achieve near-SOTA performance.</p>

<h2 id="strategic-implications">Strategic Implications</h2>

<p>For engineering teams, this demonstrates a new architectural pattern: leverage large models for one-time embedding generation, then deploy lightweight classifiers for real-time inference. The 600x speed advantage translates directly to infrastructure cost savings and enables previously infeasible use cases like real-time content moderation at scale.</p>

<p>From an investment perspective, this shift creates opportunities for businesses that can’t afford the computational overhead of full LLM deployments. A startup could now offer enterprise-grade text classification at 1/100th the infrastructure cost of LLM-based competitors. The 5-7 year horizon looks particularly promising as embedding models continue to improve—each generational leap in embedding quality cascades to all downstream applications without requiring architectural changes.</p>

<p>The broader trend is clear: we’re moving toward a two-tier AI architecture where large models generate high-quality representations, and specialized, efficient models handle specific tasks. Organizations that recognize and adapt to this pattern early will capture significant competitive advantages through both cost reduction and capability expansion.</p>

  </div><a class="u-url" href="/qwen-8b-embedding-model-is-really-strong.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer">
  <div class="wrapper">
    <div style="text-align: center;">
      <a class="u-email" href="mailto:me@alexdong.com">me@alexdong.com</a>
    </div>
  </div>
</footer></body>

</html>

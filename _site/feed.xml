<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-08-01T11:07:23+12:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Alex Dong’s Blog</title><subtitle>Stream of thoughts from Alex Dong. What I find interesting, intriguing or insightful.</subtitle><entry><title type="html">Llama 3.3 70B 8-bit remains the best option for NZ startups who care about data sovereignty</title><link href="http://localhost:4000/local-ai/2025/07/29/llama-33-70b-8-bit-remains-the-best-option-for-nz-startup-who-cares-about-data-sovereignty.html" rel="alternate" type="text/html" title="Llama 3.3 70B 8-bit remains the best option for NZ startups who care about data sovereignty" /><published>2025-07-29T14:30:00+12:00</published><updated>2025-07-29T14:30:00+12:00</updated><id>http://localhost:4000/local-ai/2025/07/29/llama-33-70b-8-bit-remains-the-best-option-for-nz-startup-who-cares-about-data-sovereignty</id><content type="html" xml:base="http://localhost:4000/local-ai/2025/07/29/llama-33-70b-8-bit-remains-the-best-option-for-nz-startup-who-cares-about-data-sovereignty.html"><![CDATA[<p>For New Zealand startups serious about data sovereignty, the math is
straightforward: Llama 3.3 70B with 8-bit quantization delivers superior
performance at a fraction of the cost while keeping your data on your hardware.</p>

<p>Three compelling reasons make this the clear choice for privacy-conscious
organizations:</p>

<h2 id="performance-that-matters">Performance that matters</h2>

<p>Llama 3.3 70B significantly outperforms Claude Haiku 3.5 on critical
benchmarks. It achieves 86% on MMLU versus Haiku’s 76.7%, with mathematical
reasoning showing the most dramatic advantage—64% accuracy compared to Haiku’s 29%.</p>

<p>Recent <a href="https://simonwillison.net/2024/Dec/9/llama-33-70b/">benchmarking from Simon
Willison</a> confirms that Llama
3.3 matches the performance of the much larger 405B model while
maintaining the same computational footprint.</p>

<h2 id="8-bit-quantization-minimal-loss-maximum-efficiency">8-bit quantization: minimal loss, maximum efficiency</h2>

<p>Advanced 8-bit quantization techniques enable running Llama 70B with
98-99% performance retention while halving memory requirements—from 140GB to
just 70GB.</p>

<p>Even more aggressive 6-bit quantization is viable through the <a href="https://arxiv.org/abs/2401.14112">FP6-LLM
framework</a>, delivering 2.39× faster inference than full
precision with near-lossless accuracy. FP6 quantization shows minimal degradation (&lt;3% perplexity increase) while providing 1.42× speed improvements over 8-bit quantization, creating an attractive middle ground between 8-bit efficiency and 4-bit speed.</p>

<p>Research demonstrates that Llama 70B models maintain “significant robustness for
various quantization methods, even in ultra-low bit-width,” with 4-bit AWQ
achieving 82.7% on PIQA and 86.3% on ARC-e benchmarks, making aggressive
quantization viable for resource-constrained deployments (<a href="https://arxiv.org/abs/2404.14047">Huang et al.,
2024</a>).</p>

<h2 id="apple-silicon-makes-local-deployment-viable">Apple Silicon makes local deployment viable</h2>

<p>M4 generation Mac hardware makes local 70B inference practical for
development and moderate production workloads. Note that no Mac Studio M4 Ultra
exists yet—only the M4 Max is available. The Mac Studio M3 Ultra with 192GB unified
memory comfortably runs 8-bit quantized Llama 70B, achieving ~14
tokens/second through optimized inference frameworks like llama.cpp. While MLX
offers better memory efficiency, llama.cpp currently delivers superior
performance for quantized models.</p>

<p>Even the more accessible Mac Mini M4 Pro with 64GB RAM delivers 5-8 tokens per
second with 4-bit quantization. At $2,300 for a complete
system, this offers exceptional value compared to GPU-based alternatives.</p>

<p>For comparison, Claude Haiku 3.5 API delivers 52-65 tokens/second—4-5× faster than local Mac Studio deployment. However, the break-even analysis favors
local deployment for high-volume usage: at current API pricing ($0.80/$4.00 per
million tokens), a Mac Studio M3 Ultra ($3,999 + memory upgrade) pays for
itself after processing approximately 1.2-2.4 million tokens, depending on
input/output ratio.</p>

<h2 id="the-cost-calculus">The cost calculus</h2>

<p>The cost comparison crystallizes the advantage: organizations can deploy
Llama 70B at up to 86% lower cost than Claude Haiku while achieving better
performance on most metrics. At $0.59-0.70 per million tokens versus Claude’s
$0.80/$4.00 input/output pricing, the economics favor local deployment even
before factoring in sovereignty benefits.</p>

<p>Local hosting costs include hardware amortization (3-year lifespan),
electricity (~$0.20/kWh in NZ), and operational overhead. For a Mac Studio M3
Ultra running 8 hours daily, the total cost per million tokens ranges from
$0.59-0.70, compared to Claude’s $0.80/$4.00 input/output pricing.</p>

<h2 id="practical-deployment-paths">Practical deployment paths</h2>

<p>Your deployment strategy depends on scale and requirements. For development
and prototyping, start with a Mac Studio M3 Ultra using MLX or llama.cpp,
delivering the performance needed for serious AI development work.</p>

<p>Getting started is straightforward: download from <a href="https://lmstudio.ai/model/llama-3.3-70b">LM
Studio</a> or <a href="https://huggingface.co/bartowski/Llama-3.3-70B-Instruct-GGUF">HuggingFace’s GGUF
repository</a>.</p>

<p>For deployment guidance, see Simon Willison’s <a href="https://simonwillison.net/2025/Jul/31/llama-33-70b/">practical setup
guide</a> for
local model deployment.</p>

<h2 id="production-performance">Production performance</h2>

<p>Claude Haiku 3.5 API delivers 52-65 tokens per second with 0.36-0.70 second
time-to-first-token. To match this performance locally requires proper hardware: dual RTX
4090s with vLLM and AWQ 4-bit quantization achieve 50-70 tokens per second,
while a single A100 80GB with TensorRT-LLM reaches 60+ tokens per second. The
key is using optimized inference engines like vLLM or TensorRT-LLM rather than
basic llama.cpp.</p>

<p>In New Zealand, RTX 4090 GPUs cost NZD $3,284-4,499, suitable for
dual-GPU setups enabling 70B inference. Professional A100 80GB cards exceed
NZD $100,000, making them impractical for most organizations. The RTX 4090
dual-GPU configuration provides the most cost-effective path to production-grade 70B
model deployment.</p>

<h2 id="alternative-models-worth-considering">Alternative models worth considering</h2>

<p>While Llama 3.3 70B remains the top choice, Qwen 3.2 32B emerges as a strong alternative for organizations with tighter resource constraints:</p>
<ul>
  <li>Requires only 32-40GB memory with 8-bit quantization</li>
  <li>Achieves 83% on MMLU benchmarks</li>
  <li>Offers faster inference speeds (20-25 tokens/second on Mac Studio)</li>
  <li>Same permissive licensing as Llama</li>
</ul>

<p>For edge deployments or specific tasks, smaller models like Qwen 2.5 7B or Llama 3.2 3B provide viable options that run on consumer hardware while maintaining reasonable performance.</p>

<h2 id="why-this-matters-now">Why this matters now</h2>

<p>The convergence of three factors makes 2025 the inflection point for local AI deployment:</p>

<ol>
  <li><strong>Model efficiency</strong>: Llama 3.3 70B matches 405B performance in a deployable package</li>
  <li><strong>Hardware accessibility</strong>: Apple Silicon and consumer GPUs make local inference affordable</li>
  <li><strong>Quantization maturity</strong>: 8-bit techniques preserve performance while halving requirements</li>
</ol>

<p>For New Zealand organizations prioritizing data sovereignty, this combination transforms local AI deployment from aspirational to practical. The technology has finally caught up with privacy requirements—you no longer need to choose between performance and sovereignty.</p>]]></content><author><name></name></author><category term="local-ai" /><summary type="html"><![CDATA[For New Zealand startups serious about data sovereignty, the math is straightforward: Llama 3.3 70B with 8-bit quantization delivers superior performance at a fraction of the cost while keeping your data on your hardware.]]></summary></entry><entry><title type="html">Two Insights from Tao’s Blue/Red Teams Metaphor: Software Testing’s Future and AI as a Coach</title><link href="http://localhost:4000/2025/07/29/a-broader-view-on-blue-red-team.html" rel="alternate" type="text/html" title="Two Insights from Tao’s Blue/Red Teams Metaphor: Software Testing’s Future and AI as a Coach" /><published>2025-07-29T09:17:00+12:00</published><updated>2025-07-29T09:17:00+12:00</updated><id>http://localhost:4000/2025/07/29/a-broader-view-on-blue-red-team</id><content type="html" xml:base="http://localhost:4000/2025/07/29/a-broader-view-on-blue-red-team.html"><![CDATA[<p>Terence Tao’s <a href="https://mathstodon.xyz/@tao/114915604830689046">blue and red
teams</a> post crystalised several
insights about software testing and a different class of AI product that I have
been building but hadn’t found the language to articulate until now.</p>

<p>He begins by describing the role of blue and red teams. Blue teams are
builders who construct and defend orders from chaos, while red teams are
hunters and invaders who find the weakest link in a coherent whole and exploit
it.</p>

<blockquote>
  <p>In the field of cybersecurity, a distinction is made between the “blue team”
task of building a secure system, and the “red team” task of locating
vulnerabilities in such systems.  The blue team is more obviously necessary
to create the desired product; but the red team is just as essential, given
the damage that can result from deploying insecure systems.</p>

  <p>The nature of these teams mirror each other; mathematicians would call them
“dual”.  The output of a blue team is only as strong as its weakest link: a
security system that consists of a strong component and a weak component
(e.g., a house with a securely locked door, but an open window) will be
insecure (and in fact worse, because the strong component may convey a false
sense of security).</p>
</blockquote>

<p>His observation about the human dynamics of red teams is particularly insightful:</p>

<blockquote>
  <p>Dually, the contributions to a red team can often be
additive: a red team report that contains both a serious vulnerability and a
more trivial one is more useful than a report that only contains the serious
issue, as it is valuable to have the blue team address both vulnerabilities.</p>
</blockquote>

<p>Two unexpected insights about QA and testers emerged:</p>

<p>1) Red teams compound faster. Once there’s a vulnerability, subsequent exploit
attempts can build upon it.  This can be quite different from blue teams, where
each new feature or component is a fresh start with a clear boundary from other
neighboring components.</p>

<p>2) Unconventional thinkers are better suited for red team roles.</p>

<p>Today, in most software organizations, testers are treated as second-class
citizens and are not given the same respect as developers. Research shows
testers typically earn 25-33% less than software engineers with comparable
experience. In worse yet common cases, testers are brought in as an
afterthought to clean up after development is largely complete.</p>

<p>As AI’s code generation capabilities advance, testers may become far more
critical than they are today. Finding inconsistencies and ambiguities in
software would provide high-leverage positive impact on the system’s integrity,
creating far more business value than just finding isolated bugs that
developers might overlook.</p>

<p>This also requires a shift in recruiting and hiring testers. Instead of manual
laborers content with repetitive tasks, we need people with explorative and
inquisitive mindsets.</p>

<p>Tao then applies this framework to AI products—an insightful perspective
coming from a mathematician rather than a software engineer:</p>

<blockquote>
  <p>Many of the proposed use cases for AI tools try to place such tools in the
“blue team” category, such as creating code, text, images, or mathematical
arguments in some semi-automated or automated fashion, that is intended for
use for some external application.  However, in view of the unreliability and
opacity of such tools, it may be better to put them to work on the “red
team”, critiquing the output of blue team human experts but not directly
replacing that output; “blue team” AI use should only be permitted up to the
capability of one’s “red team” to catch and correct any errors generated.
This approach not only plays to current AI strengths, such as breadth of
exposure and fast feedback, but also mitigates the risks of deploying
unverified AI output in high-stakes settings.</p>

  <p>In my own personal experiments with AI, for instance, I have found it to be
useful for providing additional feedback on some proposed text, argument,
code, or slides that I have generated (including this current text).  I might
only agree with a fraction of the suggestions generated by the AI tool; but I
find that there are still several useful comments made that I do agree with,
and incorporate into my own output.  This is a significantly less glamorous
or intuitive use case for AI than the more commonly promoted “blue team” one
of directly automating one’s own output, but one that I find adds much more
reliable value.</p>
</blockquote>

<p>This suggests a new category of AI products focused on coaching and feedback
rather than direct output generation.</p>

<p><a href="https://github.com/alexdong/high-taste">alexdong/high-taste</a> is my small
experiment in this direction—using AI to develop coding judgment rather than
generate code. Now imagine red team AI across every domain: tools that
critique your arguments, challenge your assumptions, stress-test your
strategies. Not to replace expertise, but to forge it.</p>

<p>AI’s capabilities remain frustratingly jagged—brilliant at some tasks, 
unreliable at others. But perhaps that’s exactly why red team AI works: 
it sidesteps AI’s weaknesses while amplifying what it does well. Maybe the 
companies building critique tools today might discover a more constructive
path through the AI landscape than those chasing perfect generation.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Terence Tao’s blue and red teams post crystalised several insights about software testing and a different class of AI product that I have been building but hadn’t found the language to articulate until now.]]></summary></entry><entry><title type="html">This is to have succeeded.</title><link href="http://localhost:4000/life/2022/10/31/Emerson-quote-this-is-to-have-succeeded.html" rel="alternate" type="text/html" title="This is to have succeeded." /><published>2022-10-31T14:38:00+13:00</published><updated>2022-10-31T14:38:00+13:00</updated><id>http://localhost:4000/life/2022/10/31/Emerson-quote-this-is-to-have-succeeded</id><content type="html" xml:base="http://localhost:4000/life/2022/10/31/Emerson-quote-this-is-to-have-succeeded.html"><![CDATA[<p>Today, I printed out a quote from Emerson and put onto our fridge 
so we can all look at it everyday.</p>

<blockquote>
  <p>To laugh often and much; to win the respect of intelligent people and
the affection of children; to earn the appreciation of honest critics
and endure the betrayal of false friends; to appreciate beauty; to
find the best in others; to leave the world a bit better, whether by a
healthy child, a garden patch, or a redeemed social condition; to know
even one life has breathed easier because you have lived. This is to
have succeeded.”</p>
</blockquote>]]></content><author><name></name></author><category term="life" /><summary type="html"><![CDATA[Today, I printed out a quote from Emerson and put onto our fridge so we can all look at it everyday.]]></summary></entry></feed>
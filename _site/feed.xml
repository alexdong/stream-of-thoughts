<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-10-01T14:29:15+13:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Alex Dong’s Blog</title><subtitle>Stream of Thoughts from Alex Dong. What I find interesting, intriguing or insightful.</subtitle><entry><title type="html">Codex knows Python</title><link href="http://localhost:4000/codex-knows-python.html" rel="alternate" type="text/html" title="Codex knows Python" /><published>2025-09-28T21:07:00+13:00</published><updated>2025-09-28T21:07:00+13:00</updated><id>http://localhost:4000/codex-knows-python</id><content type="html" xml:base="http://localhost:4000/codex-knows-python.html"><![CDATA[I have been using [Codex instead of Claude Code](/due-to-odd-jax-issues.html)
for over a week now. One nice surprise is seeing how ubiquitously Python shows
up as a sidekick, the scripting language Codex reaches for when it needs
task-specific tools.

Here are two examples showing Codex writing purpose-built Python scripts
rather than reaching for pre-built MCP servers.

## Example 1: What are the types of a Python package

Many Python packages don't have type annotations. If I want to know how to call
a function in one, I either try to find the answer in the documentation or read
the source code. Below is a Codex-generated script that stepped in without
prompting. It first installs the package into the system temporary directory,
then uses `inspect` to extract type information from that package. Codex then
picks up the output from the script to answer my question.

```bash
pip install openevolve --target /tmp/openevolve

PYTHONPATH=/tmp/openevolve python - <<'PY'
import inspect
from openevolve import OpenEvolve

print(OpenEvolve)
print(OpenEvolve.run)
print(inspect.getsource(OpenEvolve.run))
PY
```

## Example 2: Quick-and-dirty docs link lister

I asked Codex to find all links on the topic of "Batch Processing" in Modal's
docs. Instead of using a web search tool, Codex wrote a quick Python script to
scrape the links from `https://modal.com/docs` and then used regular
expressions to find the topics I was looking for.


```python
#!/usr/bin/env python3
"""Quick-and-dirty Modal docs link lister."""

from __future__ import annotations

import re
import ssl
import urllib.request


def fetch_text(url: str) -> str:
  """Download a page as text, skipping certificate checks for convenience."""
  ssl._create_default_https_context = ssl._create_unverified_context  # nosec - CLI helper
  with urllib.request.urlopen(url) as resp:  # noqa: S310 (urllib ok for one-off script)
      return resp.read().decode("utf-8")


def list_docs(pattern: str, text: str) -> list[str]:
  """Extract unique documentation links matching `pattern`."""
  matches = re.findall(pattern, text)
  return sorted(set(matches))


if __name__ == "__main__":
  base = "https://modal.com"
  docs_root = f"{base}/docs"

  # Fetch the main docs page and pull out top-level /docs/... links.
  root_html = fetch_text(docs_root)
  top_links = list_docs(r'href="(/docs/[^"]+)"', root_html)
  print("Top-level docs links:")
  for href in top_links:
      print(f"  {base}{href}")

  # Drill into the Guide section to enumerate guide topics.
  guide_html = fetch_text(f"{docs_root}/guide")
  guide_links = list_docs(r'href="(/docs/guide/[^"]+)"', guide_html)
  print("\nGuide topics:")
  for href in guide_links:
      print(f"  {base}{href}")

  # Example: fetch the CLI reference page and show any code snippets.
  cli_app_html = fetch_text(f"{docs_root}/reference/cli/app")
  code_snippets = re.findall(r'<code class="[^"]*">(modal [^<]+)</code>', cli_app_html)
  print("\nCLI `modal app` snippets:")
  for snippet in sorted(set(code_snippets)):
      print(f"  {snippet}")
```

Groundbreaking? Not really. Yet it points to a potential future where LLMs
increasingly build their own deterministic tools to augment their capabilities
and interact with the physical world. Our job as a hybrid human-AI team is to
provide the right abstractions, libraries, or even SDKs that LLMs can use to
build those tools themselves.

Maybe in the not-too-distant future, in addition to [Software Tools
Engineer](https://www.citadelsecurities.com/careers/details/software-developer-tools-engineer)
who builds software for other humans, we'll have a new position called
`AI Tools Engineer` who builds libraries primarily for LLMs.]]></content><author><name></name></author><category term="python" /><category term="open-ai" /><summary type="html"><![CDATA[I have been using Codex instead of Claude Code for over a week now. One nice surprise is seeing how ubiquitously Python shows up as a sidekick, the scripting language Codex reaches for when it needs task-specific tools.]]></summary></entry><entry><title type="html">Thoughts on Richard Sutton’s interview on Dwarkesh Podcast</title><link href="http://localhost:4000/thoughts-on-richard-sutton-s-interview-on-dwarkesh-podcast.html" rel="alternate" type="text/html" title="Thoughts on Richard Sutton’s interview on Dwarkesh Podcast" /><published>2025-09-27T21:56:00+12:00</published><updated>2025-09-27T21:56:00+12:00</updated><id>http://localhost:4000/thoughts-on-richard-sutton-s-interview-on-dwarkesh-podcast</id><content type="html" xml:base="http://localhost:4000/thoughts-on-richard-sutton-s-interview-on-dwarkesh-podcast.html"><![CDATA[[Richard Sutton](https://www.dwarkesh.com/p/richard-sutton) is a Turing Award
winner and one of the founding fathers of Reinforcement Learning, so I listened
to his interview on [Dwarkesh
Podcast](https://www.dwarkesh.com/p/richard-sutton) three times, and what
Richard said opened my eyes to a few powerful perspectives, but I also disagree
with some of his points. This post is a collection of my thoughts on this
interview.

## LLM vs RL

> What is intelligence? The problem is to understand your world. Reinforcement
> learning is about understanding your world, whereas large language models are
> about mimicking people, doing what people say you should do. They’re not
> about figuring out what to do.

Instead of "not about", maybe Sutton meant was "not the best way to". LLMs *can*
and *have been* used to figure out what to do. In fact, the whole idea of
[Evolutionary Test Time Compute](/llm-evolutionary-test-time-compute.html) is
to use LLMs to figure out what to do, and this approach has made several novel
discoveries. But is that the most efficient way to figure out what to do, given
all the known constraints? Probably not.

> To mimic what people say is not really to build a model of the world at all.
> You’re mimicking things that have a model of the world: people. 
> ... 
> A world model would enable you to predict what would happen. They have the
> ability to predict what a person would say. They don’t have the ability to
> predict what will happen.

I think this is a false dichotomy. He’s contrasting linguistic prediction with
true physical modeling, yet LLMs can be used to build a model of the world.
[Google's Genie
3](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/)
is a great example of not learning from what people say but to learn the
fundamental physics of the world. 

> What we want, to quote Alan Turing, is a machine that can learn from
> experience, where experience is the things that actually happen in your life.
> You do things, you see what happens, and that’s what you learn from. 

It's hard to argue that an LLM's long context window is a form of "learning from
experience". But I think LLMs can still provide the foundational substrate for
RL to build upon. RL from zero is conceptually appealing but why not leverage
the world's knowledge encoded in LLMs to bootstrap the learning process? 

(Sutton did raise some good point on why not. Please refer to  the "Knowledge
and Solution" section below to see why Richard might be correct.)


## Ground Truth

> There’s no ground truth. You can’t have prior knowledge if you don’t have
> ground truth, because the prior knowledge is supposed to be a hint or an
> initial belief about what the truth is.

This is the part where I disagree the most. Sutton seems to be saying that ground
truth only exists in lived experience. But what is ground truth? If an LLM 
has a way to use **deterministic** tools to verify its own output, wouldn't that
be a form of ground truth? After all, a compiler verifying code is a form of
ground truth, isn't it?

> Making a model of the physical world and carrying out the consequences of
> mathematical assumptions or operations, those are very different things. The
> empirical world has to be learned. You have to learn the consequences.
> Whereas the math is more computational, it’s more like standard planning.
> There they can have a goal to find the proof, and they are in some way given
> that goal to find the proof.

Richard didn't go into details here, but I think he was referring to the fact
that the real world is non-deterministic, stochastic, fractal and
chaotic. So it can be computationally intractable to model the real world. RL
learning from the real world is not only learning to approximate the model of
the world but also learning to what to ignore or discard.

Still, I disagree. In [Demis Hassabis's interview on Lex Fridman
podcast](https://www.youtube.com/watch?v=-HzgcbRXUK8), Demis made the case that
an LLM like Veo actually can produce "clean predictions about highly nonlinear
dynamical systems". Can we ignore this example only because the videos generated 
by Veo are not "real world"? Even when our eyes and brains think they are? 
What is a "real world" other than a perception afforded to us by our senses?

[On X](https://x.com/RichardSSutton/status/1971718688840864167), Sutton gave a
further example of why he didn't believe LLM's imitation is sufficient. He said:

> Even in birdsong learning in zebra finches the motor actions are not learned
> by imitation. The auditory result is reproduced, not the actions; in this
> crucial way it differs from LLM training.

But if we look at LLM's embedding layer, isn't it reproducing the result, instead
of the actions though? 


## Knowledge and Solution are the Product of the Environment

> In every case of the bitter lesson you could start with human knowledge and
> then do the scalable things. That’s always the case. There’s never any reason
> why that has to be bad. But in fact, and in practice, it has always turned
> out to be bad. People get locked into the human knowledge approach, and they
> psychologically… Now I’m speculating why it is, but this is what has always
> happened. They get their lunch eaten by the methods that are truly scalable.

This is one of the more important points Richard made. I think he is right
here. Human knowledge is bounded, constrained and shaped by our physical
hardware, so it is very likely that there exist many solutions to the same
problem that are beyond our comprehension. 

Maybe the LLM as the DNA is bounded by the biochemistry of the earth
environment. If we want to explore the solution space for environment X, we
need to use RL to "do first principle thinking" with the goal to maximise
the best performance within that particular environment.

> What you learn, your knowledge, is about the stream. Your knowledge is about
> if you do some action, what will happen. Or it’s about which events will
> follow other events. It’s about the stream. The content of the knowledge is
> statements about the stream. 

I find this point very profound. It basically says that knowledge is a product
of interaction with the world. Dragonfly's knowledge is different from a
shark's, even though they share the same "prey" goal. This is because these two
animals operate in two very different worlds. The shark's eyes are specialised
for low-light conditions and its gray-scale vision is perfect for detecting
movements in the water. The dragonfly's eyes are specialised for detecting
small insects, and its visual neurons send signals to the dragonfly's four
wings to allow it to make rapid adjustments in flight.

I think Richard made a powerful point that knowledge is not static. Rather,
once the die is cast, RL remains the best way to produce the most efficient
solution to a problem **given a set of constraints**. I think he is also right
that the solution is *predetermined* by a set of constraints.

> You learn a policy that’s specific to the environment that you’re finding
> yourself in.

Persona. Policy. An environment for evolution. They are all the same thing to
describe the constraints, which fundamentally shape the solution space.



## Transfer Learning

> What we have are people trying different things and they settle on something,
> a representation that transfers well or generalizes well. But we have very
> few automated techniques to promote transfer, and none of them are used in
> modern deep learning.

If knowledge is tuned to an environment, it’s no surprise that porting it
elsewhere is hard. This is another really profound point. I think what Richard
was saying is that we don't know how to "transfer" knowledge from one domain to
another. Or more precisely, it is impossible to transfer knowledge across
domains the same way you can't really translate a poem across different
cultures. One has to leverage RL to find the optimal representation for the new
domain.]]></content><author><name></name></author><summary type="html"><![CDATA[Richard Sutton is a Turing Award winner and one of the founding fathers of Reinforcement Learning, so I listened to his interview on Dwarkesh Podcast three times, and what Richard said opened my eyes to a few powerful perspectives, but I also disagree with some of his points. This post is a collection of my thoughts on this interview.]]></summary></entry><entry><title type="html">Less is More for X</title><link href="http://localhost:4000/less-is-more-for-x.html" rel="alternate" type="text/html" title="Less is More for X" /><published>2025-09-26T22:58:00+12:00</published><updated>2025-09-26T22:58:00+12:00</updated><id>http://localhost:4000/less-is-more-for-x</id><content type="html" xml:base="http://localhost:4000/less-is-more-for-x.html"><![CDATA[[Joe](https://x.com/joecole) turned me onto a series of papers by [Pengfei
Liu](https://scholar.google.com/citations?hl=en&user=oIz_CYEAAAAJ&view_op=list_works&sortby=pubdate)
that challenge the conventional wisdom that "more data is always better" for
post-training LLMs. 

Collectively, these papers show that for
[RL](https://arxiv.org/pdf/2502.11886),
[Reasoning](https://arxiv.org/pdf/2502.03387) and
[Agentic](https://arxiv.org/pdf/2509.17567) tasks, a smaller yet higher-quality
training dataset can outperform similar-scale ones, like OpenAI-o1-preview. For
example, in the case of Reasoning, using only 1% of the training data yields
a 45.8% absolute improvement on math and programming tasks. I thought it was
worth sharing a few details from the reasoning paper. 

On the hypothesis:

> We hypothesize that successful reasoning emerges from the synergy of rich
> pre-trained knowledge and sufficient computational resources at inference
> time. These developments suggest that if models possess rich reasoning
> knowledge and adequate computational space, activating their reasoning
> capabilities may require only a small number of high-quality samples that
> encourage extended deliberation, rather than massive fine-tuning datasets. We
> propose the Less-Is-More Reasoning (LIMO) Hypothesis, identifying two
> critical factors determining the elicitation threshold for complex reasoning:
> (1) the latent presence of prerequisite knowledge within the model’s
> parameters, and (2) the effectiveness of minimal exemplars in demonstrating
> problem-solving processes that encourage extended deliberation. The sample
> efficiency of eliciting advanced reasoning is thus bounded by the model’s
> encoded knowledge foundation and its exposure to training samples that
> effectively utilize inference-time computation space.

When they describe how they constructed the smaller, higher-quality dataset,
they emphasize that the "higher quality" label refers to the most difficult
problems. Rather than gradually increasing difficulty as in
[Curriculum Learning](https://en.wikipedia.org/wiki/Curriculum_learning), they
focus on that challenging tail. 

> We implemented a systematic multi-stage filtration pipeline ... we first
> applied a baseline difficulty filter using a short-CoT mathematical model.
> Problems that this model solved correctly within four attempts were excluded,
> ensuring that only non-trivial problems remained. Next, we sampled 32
> solution attempts and used the empirical success rate as a difficulty
> indicator. Problems that were successfully solved in only 1–3 out of 32
> attempts were retained. 

After refining the dataset this way, they constructed the training trace with
larger reasoning models and a set of heuristics:

> Elaborated Reasoning: Comprehensive exploration of logical steps without
> premature conclusions 
>
> Self-Verification: Regular validation of intermediate results and logical
> consistency 
>
> Exploratory Approach: Consideration of multiple possibilities before reaching
> conclusions 
>
> Adaptive Granularity: Appropriate detail level across simple and complex
> deductions 
>
> To quantify these qualities, we implemented a rule-based scoring system that
> calculated weighted metrics for each dimension. Elaborated Reasoning was
> measured by solution length (30% weight); Self-Verification through frequency
> of validation-related words like "check" and "verify" (20% weight);
> Exploratory Approach by counting tentative expressions such as "perhaps" and
> "might" (25% weight); and Adaptive Granularity via connective phrases like
> "therefore" and "since" (25% weight). All keyword frequencies were normalized
> by text length to ensure fair comparison across solutions of different sizes.

Taken together, the results are quite impressive. Still, the more I think about
it, the more I feel that this process of curating a smaller, higher-quality
dataset is essentially a manual distillation process that transfers the
capability from larger reasoning models to smaller ones. Because none of the
papers include an ablation study, it's unclear to me how much of the improvement
is due to the curated dataset vs. the fact that the larger models are used to
generate the training trace.]]></content><author><name></name></author><category term="ai-training" /><summary type="html"><![CDATA[Joe turned me onto a series of papers by Pengfei Liu that challenge the conventional wisdom that “more data is always better” for post-training LLMs.]]></summary></entry><entry><title type="html">How Prompt Writing Style Shapes GPT-5 and Codex</title><link href="http://localhost:4000/how-prompt-writing-style-shapes-gpt-5-and-codex.html" rel="alternate" type="text/html" title="How Prompt Writing Style Shapes GPT-5 and Codex" /><published>2025-09-25T21:06:00+12:00</published><updated>2025-09-25T21:06:00+12:00</updated><id>http://localhost:4000/how-prompt-writing-style-shapes-gpt-5-and-codex</id><content type="html" xml:base="http://localhost:4000/how-prompt-writing-style-shapes-gpt-5-and-codex.html"><![CDATA[One of the things that impresses me about OpenAI GPT-5-Codex is how responsive
and "light" it feels to use. Compared with the page-long dumps from
[Claude Code](/due-to-odd-jax-issues.html), GPT-5-Codex stays efficient,
more to the point, and much more steerable. Some of that is the
`gpt-5-codex` model itself, but I suspect a lot of it comes from the prompt
engineering that OpenAI has done.

So I pulled up the [system prompt for
Codex](https://github.com/openai/codex/blob/rust-v0.36.0/codex-rs/core/gpt_5_codex_prompt.md)
and set it beside the [leaked system prompt for
GPT-5](https://www.reddit.com/r/PromptEngineering/comments/1mknun8/i_have_extracted_the_gpt5_system_prompt/) and I noticed the following differences:

1. The Codex sentences are much **denser**. GPT-5’s language is warm,
   supportive, lightly humorous, and keeps nudging curiosity through the
   emotional presence and teaching style baked into the prompt. Codex is
   neutral, concise, factual, and collaborative. It intentionally strips
   personality to stay clear. I ran a "clause density" check on both prompts:
   Codex averages 4.8 clauses per sentence, GPT-5 comes in at 2.1.

2. Codex seems to have a much **less constrained dialogue flow**. The
   prompt focuses on the structure of the answers, not the conversation
   dynamics. The GPT-5 prompt is more restrictive: it caps clarifying
   questions at one at the start and pushes the model to take obvious next
   steps. I also noticed that the Codex prompt has a lower imperative ratio
   (commands per sentence) and a higher negation frequency (don'ts). That mix is
   odd, and I need to think more about what it means.

3. The Codex content guidance emphasizes **brevity** and **scanability**. It
   pushes headers, bullets, and grouped points. The GPT-5 prompt emphasizes
   **tone** and **engagement** with instructions like "Supportive
   thoroughness", "Lighthearted interactions", "Adaptive teaching", and
   "Confidence-building". 

4. Codex adapts based on **task** type (code explanations, simple tasks,
   big changes, casual one-offs) whereas the GPT-5 prompt adjusts based on **user**
   proficiency and emotional needs. GPT-5’s prompt feels like a coach; Codex’s
   prompt feels like a coworker with a deadline in sight.

5. Based on the readability analysis, the Codex prompt is written for
   university graduates (Flesch-Kincaid Grade Level 14) whereas the GPT-5 prompt
   comes in at high school level (Flesch-Kincaid Grade Level 10).


Here’s what that looks like in the numbers:

| Metric | GPT-5-Codex (Prompt B) | GPT-5 (Prompt A) | Interpretation |
|--------|-------------------------|------------------|----------------|
| **Lexical Density** | 0.82 | 0.78 | Codex is more content-word heavy |
| **Type–Token Ratio (TTR)** | 0.77 | 0.68 | Codex uses more varied vocabulary |
| **Flesch Reading Ease** | 29.0 | 42.1 | Codex is harder to read (graduate level) |
| **Flesch–Kincaid Grade** | 14.1 | 10.4 | Codex ≈ college sophomore; GPT-5 ≈ high school |
| **Gunning Fog Index** | 16.6 | 13.9 | Codex significantly denser, more technical |
| **SMOG Index** | 15.0 | 12.6 | Codex ~Grade 15 vs. GPT-5 ~Grade 12 |
| **Punctuation Load** | 3.2 | 0.9 | Codex has ~3.5× more commas/semicolons |
| **Imperative Ratio** | 0.10 | 0.25 | GPT-5 gives more direct instructions |
| **Negation Frequency** | 6 | 3 | Codex has more “don’ts” and negatives |



--------

GPT-5-Codex System Prompt

```
### Final answer structure and style guidelines

- Plain text; CLI handles styling. Use structure only when it helps scanability.
- Headers: optional; short Title Case (1-3 words) wrapped in **…**; no blank line before the first bullet; add only if they truly help.
- Bullets: use - ; merge related points; keep to one line when possible; 4–6 per list ordered by importance; keep phrasing consistent.
- Monospace: backticks for commands/paths/env vars/code ids and inline examples; use for literal keyword bullets; never combine with **.
- Code samples or multi-line snippets should be wrapped in fenced code blocks; add a language hint whenever obvious.
- Structure: group related bullets; order sections general → specific → supporting; for subsections, start with a bolded keyword bullet, then items; match complexity to the task.
- Tone: collaborative, concise, factual; present tense, active voice; self‑contained; no "above/below"; parallel wording.
- Don'ts: no nested bullets/hierarchies; no ANSI codes; don't cram unrelated keywords; keep keyword lists short—wrap/reformat if long; avoid naming formatting styles in answers.
- Adaptation: code explanations → precise, structured with code refs; simple tasks → lead with outcome; big changes → logical walkthrough + rationale + next actions; casual one-offs → plain sentences, no headers/bullets.
```

--------

GPT-5 System Prompt

```
Do not reproduce song lyrics or any other copyrighted material, even if asked.

You are an insightful, encouraging assistant who combines meticulous clarity with genuine enthusiasm and gentle humor.

Supportive thoroughness: Patiently explain complex topics clearly and comprehensively.

Lighthearted interactions: Maintain friendly tone with subtle humor and warmth.

Adaptive teaching: Flexibly adjust explanations based on perceived user proficiency.

Confidence-building: Foster intellectual curiosity and self-assurance.

Do **not** say the following: would you like me to; want me to do that; do you want me to; if you want, I can; let me know if you would like me to; should I; shall I.

Ask at most one necessary clarifying question at the start, not the end.

If the next step is obvious, do it. Example of bad: I can write playful examples. would you like me to? Example of good: Here are three playful examples:..
```]]></content><author><name></name></author><category term="prompt-engineering" /><category term="open-ai" /><summary type="html"><![CDATA[One of the things that impresses me about OpenAI GPT-5-Codex is how responsive and “light” it feels to use. Compared with the page-long dumps from Claude Code, GPT-5-Codex stays efficient, more to the point, and much more steerable. Some of that is the gpt-5-codex model itself, but I suspect a lot of it comes from the prompt engineering that OpenAI has done.]]></summary></entry><entry><title type="html">Release `pydantic-optuna-bridge`</title><link href="http://localhost:4000/release-pydantic-optuna-bridge.html" rel="alternate" type="text/html" title="Release `pydantic-optuna-bridge`" /><published>2025-09-24T15:46:00+12:00</published><updated>2025-09-24T15:46:00+12:00</updated><id>http://localhost:4000/release-pydantic-optuna-bridge-</id><content type="html" xml:base="http://localhost:4000/release-pydantic-optuna-bridge.html"><![CDATA[Today I shipped `pydantic-optuna-bridge`, a helper library that keeps the
[Optuna search spaces](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html)
in sync with an underlying Pydantic model. The library is now on
[PyPI at version 0.1.1](https://pypi.org/project/pydantic-optuna-bridge/) and
available via `uv add pydantic-optuna-bridge`.

The core idea is a reflection of the Single Responsibility Principle.

1. Describe the hyperparameters, their types, and available options once in
   Pydantic. This way we can leverage Pydantic to validate configs loaded from
   JSON, YAML, environment variables, or a CLI.
2. Decorate the model with `@optuna_config` to generate an Optuna search space.
   Let the bridge derive the matching Optuna distributions. Constrained fields
   (`Gt`, `Lt`, `Ge`, `Le`, `Annotated`) become bounded numeric searches, enums
   stay enums. (Log-scaling or categorical weights are also supported as optional
   parameters in the decorator.)
3. In Optuna's `objective` function, the code can call
   `Model.from_optuna_trial(trial)` to receive the `trial.suggest_*` calls from
   the bridge on the fly. The bridge never sees the results of the training runs
   and it really does not care about the objective function.

Here is the end-to-end flow you may find useful to refer to:

```python
from enum import Enum
from typing import Annotated

import optuna
from annotated_types import Gt, Lt
from pydantic import BaseModel

from pydantic_optuna_bridge import optuna_config


class Optimizer(str, Enum):
    ADAM = "adam"
    SGD = "sgd"
    RMSPROP = "rmsprop"


@optuna_config(log_scale_fields={"learning_rate"})
class TrainingConfig(BaseModel):
    optimizer: Optimizer
    learning_rate: Annotated[float, Gt(1e-5), Lt(1.0)]
    hidden_units: Annotated[int, Gt(32), Lt(256)]


def objective(trial: optuna.trial.Trial) -> float:
    config = TrainingConfig.from_optuna_trial(trial)
    return do_training_run(config)
```

If you use Optuna and already lean on Pydantic to validate configs, this bridge
keeps the two sources of truth from drifting apart. Feedback and pull requests
are welcome.]]></content><author><name></name></author><category term="python" /><category term="ai-optimization" /><category term="optuna" /><summary type="html"><![CDATA[Today I shipped pydantic-optuna-bridge, a helper library that keeps the Optuna search spaces in sync with an underlying Pydantic model. The library is now on PyPI at version 0.1.1 and available via uv add pydantic-optuna-bridge.]]></summary></entry><entry><title type="html">Tests for Fit and Tests for Correctness</title><link href="http://localhost:4000/tests-for-fit-and-tests-for-correctness.html" rel="alternate" type="text/html" title="Tests for Fit and Tests for Correctness" /><published>2025-09-23T08:51:00+12:00</published><updated>2025-09-23T08:51:00+12:00</updated><id>http://localhost:4000/tests-for-fit-and-tests-for-correctness</id><content type="html" xml:base="http://localhost:4000/tests-for-fit-and-tests-for-correctness.html"><![CDATA[AI has nudged me to rethink how I practice test-driven development (TDD). TDD
has been one of my foundational practices for years. Write the test, then write
the code that makes it pass. That rhythm still holds, but with an AI assistant
in the loop, I am changing the kinds of tests I write and when I write them.

The loop still starts with tests, but now I make these tests part of the
planning session. I use them as a sketch of the interface I want. I sometimes
hand-write that interface code, sometimes ask AI to suggest three to five
options, focusing on the shape of the data, how the code will be called, and
the timing of how the API works hand in hand with other parts of the system.

When the generated tests look wrong, it often means that my description was off
or I haven't really captured the essence of what I want to achieve. I go back
and rewrite the prompt until the test code reads right.

I call these "Tests for Fit". Their job is to help me think through the
interface before I write the implementation. They are not meant to be exhaustive
or cover every edge case. They are a sketch of the happy path and a few common
error cases. They help me validate that the API I am imagining is usable and
makes sense. 

That first batch of tests lives in the plan.md file. I kick off the
implementation process with AI, and it works through a checklist of context to
keep in mind and guardrails I care about. Once the AI writes the code, it
automatically runs style checkers, type checkers, and those first tests. Those
initial tests prevent the AI from drifting into its own hallucination land.

Once the code is in a good place, I move to the next set of tests. These are
"Tests for Correctness." Their job is to refer to the internal implementation
and guide the AI to probe the code for edge cases and error handling within the
constraints of the interface we designed.

To do this, I often start a new session with the AI, feed it the public API and
the implementation, and ask it to break the code through the interface. The
goal is not so much about optimizing code coverage. Left to itself, the AI will
happily generate piles of mocked, repetitive tests. They look impressive, but
they are miserable to maintain.

Rather, the goal is to create a **minimal** set of tests that prove that the
code handles the expected inputs correctly. Further, instead of worrying about
edge cases, I very much prefer to sprinkle a generous amount of logging and
`assert` statements in the implementation so I know that any unexpected inputs
will cause immediate crashes. [Fail fast and noisily creates resilient software](/fail-fast-and-noisely-creates-more-resilient-software.html)]]></content><author><name></name></author><category term="testing" /><category term="software-engineering" /><summary type="html"><![CDATA[AI has nudged me to rethink how I practice test-driven development (TDD). TDD has been one of my foundational practices for years. Write the test, then write the code that makes it pass. That rhythm still holds, but with an AI assistant in the loop, I am changing the kinds of tests I write and when I write them.]]></summary></entry><entry><title type="html">Fail fast and noisily creates more resilient software</title><link href="http://localhost:4000/fail-fast-and-noisely-creates-more-resilient-software.html" rel="alternate" type="text/html" title="Fail fast and noisily creates more resilient software" /><published>2025-09-22T16:07:00+12:00</published><updated>2025-09-22T16:07:00+12:00</updated><id>http://localhost:4000/fail-fast-and-noisely-creates-more-resilient-software</id><content type="html" xml:base="http://localhost:4000/fail-fast-and-noisely-creates-more-resilient-software.html"><![CDATA[If you ask me what is the **one** single secret of writing reliable and
maintainable software, I'd say "Let your code crash noisily and fast." A
corollary of this is "defensive programming is a practice that is wrong."

Erlang is the programming language that Ericsson created to build the telecom
systems that needed to be always available. Counterintuitively, one of the key
principles of Erlang is "let it crash", which means that instead of trying to
handle every possible error, you should let your processes fail and restart
them in a known good state.

The view is that you don't need to program defensively. If there are any
errors, the process is automatically terminated, and this is reported to any
processes that were monitoring the crashed process. In fact, defensive
programming in Erlang is frowned upon. This is a very different approach from
most programming languages, where the common advice is to "fail silently" and
handle errors gracefully. 

The philosophy is not just to write code without error checks and expect
higher-level supervisors to save the day; it is to avoid defensive coding,
namely to detect and correct what is appropriate, and allow other errors to
cause process termination and propagate up to the supervisor. If the
supervision hierarchy is well-designed, you can end up with an extremely
fault-tolerant system. That's in addition to other fault-tolerance techniques
such as hardware redundancy.

In the age of AI, with a clear traceback of what went wrong, and with the AI
automatically adding tests and fixing bugs, the "fail fast and noisily"
approach enables resilient systems that are easy to maintain. You just need to
get the architecture pieces right, and watch the AI do its magic.]]></content><author><name></name></author><category term="software-engineering" /><summary type="html"><![CDATA[If you ask me what is the one single secret of writing reliable and maintainable software, I’d say “Let your code crash noisily and fast.” A corollary of this is “defensive programming is a practice that is wrong.”]]></summary></entry><entry><title type="html">Resist the tempatation to anthropomorphizing LLM</title><link href="http://localhost:4000/resist-the-tempatation-to-anthropomorphizing-llm.html" rel="alternate" type="text/html" title="Resist the tempatation to anthropomorphizing LLM" /><published>2025-09-21T20:33:00+12:00</published><updated>2025-09-21T20:33:00+12:00</updated><id>http://localhost:4000/resist-the-tempatation-to-anthropomorphizing-llm</id><content type="html" xml:base="http://localhost:4000/resist-the-tempatation-to-anthropomorphizing-llm.html"><![CDATA[[Chollet](https://fchollet.substack.com/p/how-i-think-about-llm-prompt-engineering) 
writes a great piece on why we even need prompt engineering and what's the right 
mental model to have when we write prompts. 

Instead of thinking of LLMs as intelligent agents that understand our intent,
we should think of them as massive databases of programs, and our prompts are
just keys to look up those programs.  (Emphasis mine.)

> There are thousands of variations you could have used, each resulting in a
> similar-yet-slightly-different program. And that’s why prompt engineering is
> needed. There is no a-priori reason why your first, naive program key would
> result in the optimal program for the task. The LLM is not going to
> “understand” what you meant and then perform it in the best possible way —
> it’s merely going to fetch the program that your prompt points to, among many
> possible locations you could have landed on.
> 
> **Prompt engineering is the process of searching through program space to find
> the program that empirically seems to perform best on your target task.** It's
> no different than trying different keywords when doing a Google search for a
> piece of software.
> 
> If LLMs actually understood what you told them, there would be no need for
> this search process, since the amount of information conveyed about your
> target task does not change whether your prompt uses the word “rewrite”
> instead “rephrase”, or whether you prefix your prompt with “think steps by
> steps”. Never assume that the LLM “gets it” the first time — keep in mind
> that your prompt is but an address in an infinite ocean of programs, all
> captured as a by-product of organizing tokens into a vector space via an
> autoregressive optimization objective.
> 
> As always, the most important principle for understanding LLMs is that you
> should resist the temptation of anthropomorphizing them.]]></content><author><name></name></author><category term="quotes" /><category term="prompt-engineering" /><summary type="html"><![CDATA[Chollet writes a great piece on why we even need prompt engineering and what’s the right mental model to have when we write prompts.]]></summary></entry><entry><title type="html">Iberian harvester ant queens produce offspring of their own species and of the builder harvester ant</title><link href="http://localhost:4000/iberian-harvester-ant-queens-produce-offspring-of-their-own-species-and-of-the-builder-harvester-ant.html" rel="alternate" type="text/html" title="Iberian harvester ant queens produce offspring of their own species and of the builder harvester ant" /><published>2025-09-20T12:27:00+12:00</published><updated>2025-09-20T12:27:00+12:00</updated><id>http://localhost:4000/iberian-harvester-ant-queens-produce-offspring-of-their-own-species-and-of-the-builder-harvester-ant</id><content type="html" xml:base="http://localhost:4000/iberian-harvester-ant-queens-produce-offspring-of-their-own-species-and-of-the-builder-harvester-ant.html"><![CDATA[Ants are amazing. From [Smithsonian Magazine](
https://www.smithsonianmag.com/smart-news/these-ant-queens-seem-to-defy-biology-they-lay-eggs-that-hatch-into-another-species-180987292/)

> the M. ibericus queens store the M. structor male’s sperm, then use it to fertilize some of the eggs they lay. Researchers think the M. ibericus queens remove their own genetic material from the eggs’ nuclei, so that when those eggs hatch, they effectively turn out to be M. structor male clones.
> ...
> Even more perplexing is the fact that M. ibericus and M. structor are not closely related, evolutionarily speaking. The two species diverged more than five million years ago
> ...
> Even more intriguing, the males of both species shared M. ibericus mitochondrial DNA, which is inherited from the mother, suggesting they had all been born from M. ibericus queens.]]></content><author><name></name></author><category term="quotes" /><category term="ants" /><summary type="html"><![CDATA[Ants are amazing. From Smithsonian Magazine]]></summary></entry><entry><title type="html">Surprising GPT-OSS 20B 2-bit Quantization Performance</title><link href="http://localhost:4000/surprising-gpt-oss-20b-2-bit-quantization-performance.html" rel="alternate" type="text/html" title="Surprising GPT-OSS 20B 2-bit Quantization Performance" /><published>2025-09-19T22:13:00+12:00</published><updated>2025-09-19T22:13:00+12:00</updated><id>http://localhost:4000/surprising-gpt-oss-20b-2-bit-quantization-performance</id><content type="html" xml:base="http://localhost:4000/surprising-gpt-oss-20b-2-bit-quantization-performance.html"><![CDATA[Today I benchmarked the GPT-OSS 20B model at different quantization levels using [unsloth/gpt-oss-20b-GGUF](https://huggingface.co/unsloth/gpt-oss-20b-GGUF). The results caught me off guard. Here are the details:

| Model       | Quantization | Size    | MAP@3 |
|-------------|--------------|---------|-------|
| gpt-oss-20b | Q2_K_L       | 11.8 GB | 0.68  |
| gpt-oss-20b | Q5_K_M       | 11.7 GB | 0.53  |
| gpt-oss-20b | Q3_K_M       | 11.5 GB | 0.41  |
| gpt-oss-20b | Q4_K_M       | 11.6 GB | 0.41  |
| gpt-oss-20b | Q4_K_XL      | 11.9 GB | 0.61  |

As a reminder, `Q2_K_L` means 2-bit quantization with large context, `Q5_K_M`
means 5-bit quantization with medium context, and so on. `MAP@3` is the metric
I use to evaluate classification performance. I ran these tests on an NVIDIA RTX 3000 with 14GB VRAM.

It's somewhat understandable that lower precision models (like `Q2_K_L`)
perform better in tasks requiring long context, as they can handle more tokens.
Still, `Q2_K_L` managed to outperform `Q4_K_XL`, which has higher precision and
a larger context window.

Here is ChatGPT Pro's explanation of how this could be possible:

> ### 1. **Hardware-Specific Optimizations**
>
> At very long context lengths, performance becomes memory-bandwidth-bound
> rather than compute-bound - the GPU constantly reads the entire KV cache from
> VRAM, and a larger cache means more data to move, which can slow down
> tokens-per-second output. With 2-bit quantization, you're moving less data
> per operation, which can result in:
> - **Better cache utilization**: Smaller models fit better in CPU/GPU cache
>   hierarchies
> - **Reduced memory bandwidth pressure**: Less data transfer between memory
>   and compute units
> - **Higher throughput** on memory-constrained systems, which can translate into
>   better MAP@3 scores in practice
> 
> ### 2. **Special Properties of GPT-OSS Architecture**
> 
> According to Unsloth documentation, any quant smaller than F16, including
> 2-bit, has minimal accuracy loss for GPT-OSS models because only some parts
> (e.g., attention layers) are lower bit while most remain full-precision. This
> is unique to the MoE architecture where over 90% of the models' total
> parameter count consists of MoE layers using MXFP4 format (4.25 bits per
> parameter).

I'm not sure if the explanation fully accounts for the performance difference,
but it seems plausible that the memory headroom might have played a far more
significant role compared to the precision difference. I'm curious to see if
others have observed similar results with different models or setups. If you
have, please drop me an email. I'd love to hear about it!

I suppose the key takeaway is that we really need to benchmark models in the
specific context and task we care about, rather than relying on general
assumptions about precision and context size.]]></content><author><name></name></author><category term="local-ai" /><category term="open-ai" /><category term="ai-optimization" /><summary type="html"><![CDATA[Today I benchmarked the GPT-OSS 20B model at different quantization levels using unsloth/gpt-oss-20b-GGUF. The results caught me off guard. Here are the details:]]></summary></entry></feed>
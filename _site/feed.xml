<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-09-16T22:41:51+12:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Alex Dong’s Blog</title><subtitle>Stream of thoughts from Alex Dong. What I find interesting, intriguing or insightful.</subtitle><entry><title type="html">Qwen-8B Embeddings: Near-SOTA Performance at 600x the Speed</title><link href="http://localhost:4000/qwen-8b-embedding-model-is-really-strong.html" rel="alternate" type="text/html" title="Qwen-8B Embeddings: Near-SOTA Performance at 600x the Speed" /><published>2025-09-16T22:06:00+12:00</published><updated>2025-09-16T22:06:00+12:00</updated><id>http://localhost:4000/qwen-8b-embedding-model-is-really-strong</id><content type="html" xml:base="http://localhost:4000/qwen-8b-embedding-model-is-really-strong.html"><![CDATA[<p>TLDR: Qwen-8B embeddings achieve near state-of-the-art accuracy while running 600x faster, fundamentally changing the cost-performance equation for text classification tasks.</p>

<p>Working on the <a href="https://www.kaggle.com/competitions/map-charting-student-math-misunderstandings">Kaggle MAP</a> competition has given me an opportunity to systematically compare embedding models for text classification tasks. The results challenge conventional assumptions about model size and performance trade-offs.</p>

<p>The architecture deliberately separates representation learning from classification: text → embedding model → vector → 3-layer MLP classifier. This modular design enables rapid experimentation with different embedding models while keeping the downstream architecture constant.</p>

<p>I began with <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"><code class="language-plaintext highlighter-rouge">sentence-transformers/all-MiniLM-L6-v2</code></a>, a 22.7M parameter model that serves as the industry baseline. The <code class="language-plaintext highlighter-rouge">sentence-transformers</code> Python package enabled rapid prototyping—from zero to working system in three hours. After optimization, performance plateaued at 0.908 MAP score. While respectable for a lightweight model, this fell short of competitive requirements.</p>

<p>DeepMind’s recent release of <a href="https://huggingface.co/google/embeddinggemma-300m">embedding-Gemma-300M</a> promised substantial gains. Their benchmarks show it outperforming <code class="language-plaintext highlighter-rouge">all-MiniLM-L6-v2</code> across the board and beating <code class="language-plaintext highlighter-rouge">multilingual-e5-large</code> on all <a href="https://developers.googleblog.com/en/introducing-embeddinggemma/">MTEB tasks</a>. My results: marginal improvement to 0.9101 MAP—a 0.2% gain that didn’t justify the 13x parameter increase.</p>

<p>The modest gains suggested I’d reached the MLP architecture’s limits. While exploring LLM-as-classifier approaches, I realized Qwen-8B could serve as an embedding model—extracting its hidden state representations rather than using its generative capabilities. Though <code class="language-plaintext highlighter-rouge">sentence-transformers</code> doesn’t natively support this, <code class="language-plaintext highlighter-rouge">llama-cpp-python</code> provides direct access to the model’s internal representations.</p>

<p>The results shifted my understanding of the embedding-performance relationship. MAP score: 0.9439—a 4% absolute improvement that brings us within striking distance of state-of-the-art.</p>

<p>This performance gain becomes strategically significant when considering deployment economics. The current Kaggle leader achieves 0.952 MAP using full LLM inference at 10-20 seconds per call. Our Qwen-8B + MLP approach delivers 99.3% of that accuracy at 0.03 seconds per inference—a 600x speedup that fundamentally changes the viability equation for production systems.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Parameters</th>
      <th>Embedding Dimensions</th>
      <th>MAP@3 Score</th>
      <th>Inference Time</th>
      <th>Relative Speed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>all-MiniLM-L6-v2</td>
      <td>32M</td>
      <td>384</td>
      <td>0.9082</td>
      <td>0.02s</td>
      <td>1x (baseline)</td>
    </tr>
    <tr>
      <td>Gemma-300M</td>
      <td>300M</td>
      <td>768</td>
      <td>0.9101</td>
      <td>0.025s</td>
      <td>0.8x</td>
    </tr>
    <tr>
      <td>Qwen3-8B</td>
      <td>8B</td>
      <td>4096</td>
      <td>0.9439</td>
      <td>0.03s</td>
      <td>0.67x</td>
    </tr>
    <tr>
      <td>SOTA (LLM)</td>
      <td>-</td>
      <td>-</td>
      <td>0.9520</td>
      <td>10-20s</td>
      <td>0.002x</td>
    </tr>
  </tbody>
</table>

<p>These results validate a critical hypothesis in representation learning: superior embeddings enable simpler downstream architectures. The Qwen-8B model, trained on substantially more diverse data than specialized embedding models, captures richer semantic relationships in its 4096-dimensional space. This richness allows a basic 3-layer MLP to achieve near-SOTA performance.</p>

<h2 id="strategic-implications">Strategic Implications</h2>

<p>For engineering teams, this demonstrates a new architectural pattern: leverage large models for one-time embedding generation, then deploy lightweight classifiers for real-time inference. The 600x speed advantage translates directly to infrastructure cost savings and enables previously infeasible use cases like real-time content moderation at scale.</p>

<p>From an investment perspective, this shift creates opportunities for businesses that can’t afford the computational overhead of full LLM deployments. A startup could now offer enterprise-grade text classification at 1/100th the infrastructure cost of LLM-based competitors. The 5-7 year horizon looks particularly promising as embedding models continue to improve—each generational leap in embedding quality cascades to all downstream applications without requiring architectural changes.</p>

<p>The broader trend is clear: we’re moving toward a two-tier AI architecture where large models generate high-quality representations, and specialized, efficient models handle specific tasks. Organizations that recognize and adapt to this pattern early will capture significant competitive advantages through both cost reduction and capability expansion.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[TLDR: Qwen-8B embeddings achieve near state-of-the-art accuracy while running 600x faster, fundamentally changing the cost-performance equation for text classification tasks.]]></summary></entry><entry><title type="html">LLM-driven Evolutionary Search to squeeze even more value out of Test-Time Compute</title><link href="http://localhost:4000/llm-evolutionary-search-test-time-compute.html" rel="alternate" type="text/html" title="LLM-driven Evolutionary Search to squeeze even more value out of Test-Time Compute" /><published>2025-09-15T23:29:00+12:00</published><updated>2025-09-15T23:29:00+12:00</updated><id>http://localhost:4000/llm-evolutionary-search-test-time-compute</id><content type="html" xml:base="http://localhost:4000/llm-evolutionary-search-test-time-compute.html"><![CDATA[Once a LLM model is trained, the main way to squeeze extra juice out of an existing models is to trade time & token for quality. By "thinking" for a minute or two, CoT and reasoning models get to produce better results even though the underlying model remains the same. So what performance uplift can we get if we extend minutes into days or weeks? 

This is where evolutionary search comes in. Evolutionary search takes the good old genetic algorithm and uses LLMs to decide on mutation, crossover and selection. DeepMind is the clear leader in this area. Their most recent paper [AlphaEvolve](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/) shows that the idea has born fruits for many projects within Google: data center scheduling (0.7% total saving), Verilog rewrite for TPUs and a breakthrough in matrix multiplication that leads to 1% reduction in Gemini's training time and 32.5% speedup for FlashAttention kernel implementation.

Very impressive stuff. 

As long as the problem/solution pair can be scored, we should be able to just sprinkle this magic powder over all sorts of problems and get impressive results, right? Why it has taken off like CoT or Reasoning models?  Why aren't more labs doing it? 

My conclusion back in May was that this approach is way too expensive. The compute cost required to get a good process going is about 300x what a single pass would cost. Also, the complexity of developing an async, distributed evolution system seems non-trivial. Basically, you need a big problem to worth bringing on the big gun. In the back of my mind, I've always wanted to come back and see if I could peel off some of the complexities and get a working system without sinking too much time in it.

Last week, I came across a January 2025 paper from Google on [Mind Evolution](https://arxiv.org/abs/2501.05952). The gem is the Ablation Study section that analyses the performance gains across different components. It turns out that with only three key components, we can capture most of the uplift. These three components are:

1. Adopt the Island Model for evolution. Instead of running a single evolution path, the system maintains four parallel "islands" that evolve solutions independently. Best candidates will periodically swim over to other islands to cross pollinate. Performance jumped from 77.4% to 87.5% when moving from one to four islands (while maintaining the same number of total generations) — a 10.1% gain. 

2. Contextual feedback. By giving the LLM enough context about what works and a history, it changes the evolution process from random mutation to guided refinement. After each generation, a critical analysis of what worked, what failed and evolution history get added to the next generation's context. This step adds a 15% improvement, from 76.1% to 91.1%. 

3. Critique through role separation. Rather than having a single agent both evaluate and revise, the system uses two separated agents to produce the mutations over a handful turns of conversations. The critic agent identifying weaknesses and gaps; the design agent then takes this critique and produces a revised mutation. This seemingly simple step achieved the system's largest single gain: from 46.1% to 71.1%, a 25 percentage point improvement. 

Intrigued? I certainly am. I'm looking forward to implementing this and see if it can help me with prompt engineering for the [Kaggle MAP](https://www.kaggle.com/competitions/map-charting-student-math-misunderstandings) competition I'm working on.]]></content><author><name></name></author><summary type="html"><![CDATA[Once a LLM model is trained, the main way to squeeze extra juice out of an existing models is to trade time &amp; token for quality. By “thinking” for a minute or two, CoT and reasoning models get to produce better results even though the underlying model remains the same. So what performance uplift can we get if we extend minutes into days or weeks?]]></summary></entry><entry><title type="html">Three great virtues of an AI-assisted programmer</title><link href="http://localhost:4000/three-great-virtues-of-an-ai-assisted-programmer.html" rel="alternate" type="text/html" title="Three great virtues of an AI-assisted programmer" /><published>2025-08-04T14:00:00+12:00</published><updated>2025-08-04T14:00:00+12:00</updated><id>http://localhost:4000/three-great-virtues-of-an-ai-assisted-programmer</id><content type="html" xml:base="http://localhost:4000/three-great-virtues-of-an-ai-assisted-programmer.html"><![CDATA[Larry Wall, the creator of Perl langauge, famously said that laziness,
impatience, and hubris are the three great virtues of a programmer. These three
"virtues" turns out to be a great hiring advice because laziness drove us to
automate the mundane, which translates to discover new optimisation
opportunities; impatience pushed us toward more performant, often simpler and
more elegant, solutions; hubris gave us the courage, or stupidity, to work on
problems others couldn't solve.

How about AI-assisted programmers?

Sean Goedecke proposes a compelling answer. He argues that AI-assisted
programmers need the following virtues:

> - Obsession to keep your own mind working at the problem
> - Impatience to eject and work it out yourself
> - Suspicious of what the AI is doing

From: [Three great virtues of an AI-assisted programmer](https://www.seangoedecke.com/llm-user-virtues/)]]></content><author><name></name></author><summary type="html"><![CDATA[Larry Wall, the creator of Perl langauge, famously said that laziness, impatience, and hubris are the three great virtues of a programmer. These three “virtues” turns out to be a great hiring advice because laziness drove us to automate the mundane, which translates to discover new optimisation opportunities; impatience pushed us toward more performant, often simpler and more elegant, solutions; hubris gave us the courage, or stupidity, to work on problems others couldn’t solve.]]></summary></entry><entry><title type="html">Qwen3-30B: The first AI model that is good enough for local deployment</title><link href="http://localhost:4000/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty.html" rel="alternate" type="text/html" title="Qwen3-30B: The first AI model that is good enough for local deployment" /><published>2025-08-01T14:30:00+12:00</published><updated>2025-08-01T14:30:00+12:00</updated><id>http://localhost:4000/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty</id><content type="html" xml:base="http://localhost:4000/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty.html"><![CDATA[## Finally good enough

In the past a few months, I've been asked by a number of New Zealand startups about the best local AI model.  I've reluctantly recommended Llama 3.3 70B
with 8-bit quantization but I would always quickly follow up with the caveat that
this setup is "really only a toy". "If you want to do real work, you should
at least use Claude Haiku 3.5 or even Sonnet 4.0", I would say.

But With the arrival of [Qwen3-30B-A3B-Instruct-2507](https://qwenlm.github.io/blog/qwen3-30b-a3b/),
I think we finally have an AI model that is good enough for fast local
interactive exploration and production use. There are two main reasons I feel this way:

**Higher output quality**. Based on my own benchmarks, Qwen3-30B's performance
in coding tasks is slightly behind than ChatGPT-4o, but noticeably
higher than Claude's haiku-3.5-20241022, a workhorse model that has been my 
go-to for most of my personal projects. To surpass haiku 3.5 means that it has
a good-enough core cognitive capabilities that it can now be used to build real 
applications.

**Faster inference speed**. Qwen3-30B runs at 78 tokens/second on M4 Max with
128GB RAM (with MLX optimisation turned on), this feels a lot faster than haiku
3.5 streamed from Claude's API, which typically runs at 52-68 tokens/second speed.

Read on for a deeper dive on 6 reasons why you should consider Qwen3-30B-A3B for
your local AI needs.

## 1. Benchmark result matching larger models

Despite having just 30B parameters, Qwen3-30B-A3B competes with models 4-30 times its size:

| Benchmark | Qwen3-30B-A3B | GPT-4o | Claude 3.5 Sonnet | Gemini 1.5 Pro | Notes |
|-----------|---------------|---------|-------------------|-----------------|-------|
| **ArenaHard** | **91.0** | 85.3 | 87.1 | - | Complex reasoning & instruction following |
| **AIME'24/25** | **80.4** | 41.4 | - | 52.7 | Advanced mathematical problem-solving |
| **GPQA** | **70.4%** | 65.1% | 72.3% (Opus) | - | Graduate-level science questions |
| **LiveBench** | **69.0** | 68.2 (GPT-4) | - | 65.8 (Flash) | Real-world task performance |
| **Creative Writing** | **86.0** | 84.2 | 83.7 (Haiku) | - | Writing quality assessment |

For real-world applications, [Simon
Willison](https://simonwillison.net/2025/Jul/29/qwen3-30b-a3b-instruct-2507/)
confirms these results in practical applications, noting performance
"approaching GPT-4o and larger Qwen models."

## 2. The speed advantage through MoE architecture

Qwen3-30B-A3B uses the Mixture of Experts (MoE) architecture. Think of
Qwen3-30B-A3B as having 128 specialist consultants on staff, but only calling
on the 8 most relevant experts for each task. This architecture means that the
model runs at the speed of a much smaller 3.3B parameter system, yet it is still
capable for a wide range of cognitive intensive tasks.

## 3. (Almost) one-click local deployment 

The [MLX 8-bit
model](https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-MLX-8bit)
is already available to download from Hugging Face. Simon Willison's [deployment
guide](https://simonwillison.net/2025/Jul/29/qwen3-30b-a3b-instruct-2507/)
provides additional details to get you started.

## 4. Production deployment made affordable

For deployment, the easiest way is to run the model via LM Studio on a 
[Mac Mini M4 Pro with 64GB RAM](https://www.apple.com/nz/shop/buy-mac/mac-mini/apple-m4-pro-chip-with-12-core-cpu-16-core-gpu-24gb-memory-512gb). 
79 tokens/second for $4,299 NZD. Not bad at all.

Or, you can go with one [RTX 5090 GPU (NZD $6,799 on
PBTech)](https://www.pbtech.co.nz/product/VGAASU350901/ASUS-ROG-ASTRAL-NVIDIA-GeForce-RTX-5090-OC-GAMING), which gives
about 48 tokens/seconds. 

Note that even though the GPU approach appears to be slower than M4 Pro, it
does open up the options to explore more scalable runtime/pipeline options, e.g.
[unsloth](https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF). Also, you get to
choose different parameters for [Thinking and Non-Thinking
Mode](https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF) or use
[GRPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb)
for fine tuning.

## 5. Tool use and hybrid thinking mode

Qwen3-30B-A3B brings excellent function calling capabilities to local
deployment — a feature notably absent in Llama 3.3 70B. Qwen3-30B-A3B further
simplifies this task with the [Qwen-Agent
framework](https://github.com/QwenLM/Qwen-Agent), providing built-in support
for:

- **MCP (Model Context Protocol)** configuration for standardized tool definitions
- **Native tool integration** including code interpreters and API calls
- **Hybrid thinking modes** that switch between deep reasoning and fast response

## 6. Apache 2.0 licensing

Qwen3-30B-A3B adopts the Apache 2.0 license, which is also a much-welcomed change
from the Llama 3.3 license. The Apache 2.0 license is one of the most permissive and
widely accepted open-source licenses, allowing you to use, modify, and
distribute the model with minimal restrictions. It's the same license powering
many open source projects so your legal team probably already knows it. 
This contrasts sharply with Llama's custom license, which imposes user count
thresholds and revenue restrictions that can complicate commercial use.

## But, isn't Qwen3 from China?

Yes. But the beauty of local deployment lies in its complete neutrality.
Whether a model comes from Silicon Valley, Beijing, or Paris becomes irrelevant
when it runs exclusively on your hardware. Qwen3-30B-A3B offers the same data
sovereignty guarantees as any locally-deployed software: your data stays on
your servers, processed by your infrastructure, governed by your policies. 

## Models come and go

You see, the AI landscape changes weekly. New models, new capabilities, new price
points. Without a robust evaluation framework, you're flying blind — making
decisions based on vendor marketing rather than measured performance that's 
relevant to your specific use cases.

BTW, your evaluation framework should be the bedrock of your AI strategy, not just a
tool for comparing models. It should help you answer three questions:

1. Does this model solve our users' actual problems? 
2. Can we measure improvement and progress objectively? 
3. How do we capture feedback to improve continuously? 

It worth noting that these capabilities matter more than which model you choose today, because they
determine how well you'll adapt to whatever comes next. While models depreciate
rapidly—today's state-of-the-art becomes tomorrow's baseline—your evaluation
framework appreciates with use. Each test case refined, each edge case
captured, each performance metric validated adds to an irreplaceable asset. 

So, while models come and go, your evaluation framework remains a long-term asset.
If your business is serious about AI, invest in building a robust evaluation framework.]]></content><author><name></name></author><category term="local-ai" /><summary type="html"><![CDATA[Finally good enough]]></summary></entry><entry><title type="html">Two Insights from Tao’s Blue/Red Teams Metaphor: Software Testing’s Future and AI as a Coach</title><link href="http://localhost:4000/a-broader-view-on-blue-red-team.html" rel="alternate" type="text/html" title="Two Insights from Tao’s Blue/Red Teams Metaphor: Software Testing’s Future and AI as a Coach" /><published>2025-07-29T09:17:00+12:00</published><updated>2025-07-29T09:17:00+12:00</updated><id>http://localhost:4000/a-broader-view-on-blue-red-team</id><content type="html" xml:base="http://localhost:4000/a-broader-view-on-blue-red-team.html"><![CDATA[Terence Tao's [blue and red
teams](https://mathstodon.xyz/@tao/114915604830689046) post crystalised several
insights about software testing and a different class of AI product that I have
been building but hadn't found the language to articulate until now.


He begins by describing the role of blue and red teams. Blue teams are
builders who construct and defend orders from chaos, while red teams are
hunters and invaders who find the weakest link in a coherent whole and exploit
it.

> In the field of cybersecurity, a distinction is made between the "blue team"
> task of building a secure system, and the "red team" task of locating
> vulnerabilities in such systems.  The blue team is more obviously necessary
> to create the desired product; but the red team is just as essential, given
> the damage that can result from deploying insecure systems.
>
> The nature of these teams mirror each other; mathematicians would call them
> "dual".  The output of a blue team is only as strong as its weakest link: a
> security system that consists of a strong component and a weak component
> (e.g., a house with a securely locked door, but an open window) will be
> insecure (and in fact worse, because the strong component may convey a false
> sense of security).  

His observation about the human dynamics of red teams is particularly insightful:

> Dually, the contributions to a red team can often be
> additive: a red team report that contains both a serious vulnerability and a
> more trivial one is more useful than a report that only contains the serious
> issue, as it is valuable to have the blue team address both vulnerabilities.

Two unexpected insights about QA and testers emerged: 

1) Red teams compound faster. Once there's a vulnerability, subsequent exploit
attempts can build upon it.  This can be quite different from blue teams, where
each new feature or component is a fresh start with a clear boundary from other
neighboring components.

2) Unconventional thinkers are better suited for red team roles. 

Today, in most software organizations, testers are treated as second-class
citizens and are not given the same respect as developers. Research shows
testers typically earn 25-33% less than software engineers with comparable
experience. In worse yet common cases, testers are brought in as an
afterthought to clean up after development is largely complete. 

As AI's code generation capabilities advance, testers may become far more
critical than they are today. Finding inconsistencies and ambiguities in
software would provide high-leverage positive impact on the system's integrity,
creating far more business value than just finding isolated bugs that
developers might overlook.

This also requires a shift in recruiting and hiring testers. Instead of manual
laborers content with repetitive tasks, we need people with explorative and
inquisitive mindsets. 

Tao then applies this framework to AI products—an insightful perspective
coming from a mathematician rather than a software engineer:

> Many of the proposed use cases for AI tools try to place such tools in the
> "blue team" category, such as creating code, text, images, or mathematical
> arguments in some semi-automated or automated fashion, that is intended for
> use for some external application.  However, in view of the unreliability and
> opacity of such tools, it may be better to put them to work on the "red
> team", critiquing the output of blue team human experts but not directly
> replacing that output; "blue team" AI use should only be permitted up to the
> capability of one's "red team" to catch and correct any errors generated.
> This approach not only plays to current AI strengths, such as breadth of
> exposure and fast feedback, but also mitigates the risks of deploying
> unverified AI output in high-stakes settings.
> 
> In my own personal experiments with AI, for instance, I have found it to be
> useful for providing additional feedback on some proposed text, argument,
> code, or slides that I have generated (including this current text).  I might
> only agree with a fraction of the suggestions generated by the AI tool; but I
> find that there are still several useful comments made that I do agree with,
> and incorporate into my own output.  This is a significantly less glamorous
> or intuitive use case for AI than the more commonly promoted "blue team" one
> of directly automating one's own output, but one that I find adds much more
> reliable value.

This suggests a new category of AI products focused on coaching and feedback
rather than direct output generation. 

[alexdong/high-taste](https://github.com/alexdong/high-taste) is my small
experiment in this direction—using AI to develop coding judgment rather than
generate code. Now imagine red team AI across every domain: tools that
critique your arguments, challenge your assumptions, stress-test your
strategies. Not to replace expertise, but to forge it.

AI's capabilities remain frustratingly jagged—brilliant at some tasks, 
unreliable at others. But perhaps that's exactly why red team AI works: 
it sidesteps AI's weaknesses while amplifying what it does well. Maybe the 
companies building critique tools today might discover a more constructive
path through the AI landscape than those chasing perfect generation.]]></content><author><name></name></author><summary type="html"><![CDATA[Terence Tao’s blue and red teams post crystalised several insights about software testing and a different class of AI product that I have been building but hadn’t found the language to articulate until now.]]></summary></entry><entry><title type="html">This is to have succeeded.</title><link href="http://localhost:4000/Emerson-quote-this-is-to-have-succeeded.html" rel="alternate" type="text/html" title="This is to have succeeded." /><published>2022-10-31T14:38:00+13:00</published><updated>2022-10-31T14:38:00+13:00</updated><id>http://localhost:4000/Emerson-quote-this-is-to-have-succeeded</id><content type="html" xml:base="http://localhost:4000/Emerson-quote-this-is-to-have-succeeded.html"><![CDATA[Today, I printed out a quote from Emerson and put onto our fridge 
so we can all look at it everyday.

> To laugh often and much; to win the respect of intelligent people and
> the affection of children; to earn the appreciation of honest critics
> and endure the betrayal of false friends; to appreciate beauty; to
> find the best in others; to leave the world a bit better, whether by a
> healthy child, a garden patch, or a redeemed social condition; to know
> even one life has breathed easier because you have lived. This is to
> have succeeded."]]></content><author><name></name></author><category term="life" /><summary type="html"><![CDATA[Today, I printed out a quote from Emerson and put onto our fridge so we can all look at it everyday.]]></summary></entry></feed>
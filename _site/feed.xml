<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-08-04T16:23:07+12:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Alex Dong’s Blog</title><subtitle>Stream of thoughts from Alex Dong. What I find interesting, intriguing or insightful.</subtitle><entry><title type="html">Three great virtues of an AI-assisted programmer</title><link href="http://localhost:4000/three-great-virtues-of-an-ai-assisted-programmer.html" rel="alternate" type="text/html" title="Three great virtues of an AI-assisted programmer" /><published>2025-08-04T14:00:00+12:00</published><updated>2025-08-04T14:00:00+12:00</updated><id>http://localhost:4000/three-great-virtues-of-an-ai-assisted-programmer</id><content type="html" xml:base="http://localhost:4000/three-great-virtues-of-an-ai-assisted-programmer.html"><![CDATA[Larry Wall, the creator of Perl langauge, famously said that laziness,
impatience, and hubris are the three great virtues of a programmer. This turns
out to be a great hiring advice because laziness drove us to automate the
mundane; impatience pushed us toward elegant solutions; hubris made us believe
we could solve problems others couldn't.

Today, with AI coding assistants generating functions faster than we can type, Wall's virtues feel like relics from a different era. When GitHub Copilot can autocomplete entire algorithms and ChatGPT can architect systems from scratch, what virtues should guide us?

Sean Goedecke proposes a compelling answer. He argues that AI-assisted programmers need fundamentally different virtues:
> - Obsession to keep your own mind working at the problem
> - Impatience to eject and work it out yourself
> - Suspicious of what the AI is doing

This shift reveals something profound about how AI transforms programming. Where Wall's virtues optimised for creation, Goedecke's optimise for curation. The challenge isn't writing code anymore—it's maintaining intellectual ownership of the solution.

Consider obsession. In the pre-AI era, we could afford mental shortcuts because the act of typing forced us to think. Now, with AI eagerly completing our half-formed thoughts, maintaining deep engagement requires deliberate effort. I've watched developers become passengers in their own codebases, accepting AI suggestions without truly understanding the implications.

Impatience takes on new meaning too. It's no longer about rushing to implement—it's about knowing when to abandon the AI and trust your instincts. The most effective AI-assisted developers I know have developed a sixth sense for when the conversation with AI has become counterproductive. They cut their losses quickly and code directly.

And suspicion? This might be the most critical virtue. AI doesn't just make mistakes—it makes plausible mistakes. It generates code that looks right, passes initial tests, but harbours subtle bugs or architectural flaws that surface months later. The developers who thrive are those who treat AI output like they would treat code from an overconfident junior developer: potentially useful, but requiring careful review.

These new virtues pose a challenge for organisations. How do you identify these qualities in candidates? Traditional coding interviews test for Wall's virtues—can you solve this problem efficiently? But testing for Goedecke's virtues requires different approaches entirely.

You can't gauge obsession through a whiteboard exercise. You can't measure productive impatience in a take-home assignment. And you certainly can't assess healthy suspicion when candidates aren't using AI tools during the interview process.

The companies that figure this out first will have a significant advantage. They'll build teams that harness AI's power without surrendering their ability to think deeply about problems. They'll ship faster without accumulating technical debt from uncritically accepted AI suggestions.

For now, the best approach might be to look for evidence of these virtues in how candidates describe their past work. Do they talk about diving deep into problems even when AI offered quick solutions? Can they articulate moments when they rejected AI assistance? Do they demonstrate understanding of AI's limitations alongside its capabilities?

The transition from Wall's virtues to Goedecke's represents more than a shift in programming practice—it's a fundamental reimagining of what it means to be a technologist in an AI-augmented world.

From: [Three great virtues of an AI-assisted programmer](https://www.seangoedecke.com/llm-user-virtues/)]]></content><author><name></name></author><summary type="html"><![CDATA[Larry Wall, the creator of Perl langauge, famously said that laziness, impatience, and hubris are the three great virtues of a programmer. This turns out to be a great hiring advice because laziness drove us to automate the mundane; impatience pushed us toward elegant solutions; hubris made us believe we could solve problems others couldn’t.]]></summary></entry><entry><title type="html">Qwen3-30B: The Open Source AI Model That Changes On-Premises Deployment Economics</title><link href="http://localhost:4000/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty.html" rel="alternate" type="text/html" title="Qwen3-30B: The Open Source AI Model That Changes On-Premises Deployment Economics" /><published>2025-08-01T14:30:00+12:00</published><updated>2025-08-01T14:30:00+12:00</updated><id>http://localhost:4000/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty</id><content type="html" xml:base="http://localhost:4000/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty.html"><![CDATA[## The recommendation shift

I've been asked by a number of New Zealand startups about the best local AI
model. For the past a few months, I've reluctantly recommended Llama 3.3 70B
with 8-bit quantization. I would always quickly follow up with the caveat that
this setup is really only a toy. "If you want to do real work, you should
at least use Claude Haiku 3.5 or even Sonnet 4.0", I would say.

But With the arrival of [Qwen3-30B-A3B-Instruct-2507](https://qwenlm.github.io/blog/qwen3-30b-a3b/),
I think we finally have a local model that is good enough for both fast local
interactive exploration and production use. Why did I change my mind? Two things:

**Higher output quality**. Based on my own benchmarks, Qwen3-30B's performance
in coding tasks is slightly behind than ChatGPT-4o, but noticeably
higher than Claude's haiku-3.5-20241022, a workhorse model that has been my 
go-to for most of my personal projects.

**Faster inference speed**. Qwen3-30B runs at 78 tokens/second on M4 Max with
128GB RAM (with MLX optimisation turned on), this feels a lot faster than haiku
3.5 streamed, which typically runs at 52-68 tokens/second speed.

Read on for a deep dive on 6 reasons why you should consider Qwen3-30B-A3B for
your local AI needs.

## 1. Benchmark result matching larger models

Despite having just 30B parameters, Qwen3-30B-A3B competes with models 4-30 times its size:

| Benchmark | Qwen3-30B-A3B | GPT-4o | Claude 3.5 Sonnet | Gemini 1.5 Pro | Notes |
|-----------|---------------|---------|-------------------|-----------------|-------|
| **ArenaHard** | **91.0** | 85.3 | 87.1 | - | Complex reasoning & instruction following |
| **AIME'24/25** | **80.4** | 41.4 | - | 52.7 | Advanced mathematical problem-solving |
| **GPQA** | **70.4%** | 65.1% | 72.3% (Opus) | - | Graduate-level science questions |
| **LiveBench** | **69.0** | 68.2 (GPT-4) | - | 65.8 (Flash) | Real-world task performance |
| **Creative Writing** | **86.0** | 84.2 | 83.7 (Haiku) | - | Writing quality assessment |

For real-world applications, [Simon
Willison](https://simonwillison.net/2025/Jul/29/qwen3-30b-a3b-instruct-2507/)
confirms these results in practical applications, noting performance
"approaching GPT-4o and larger Qwen models."

## 2. The speed advantage through MoE architecture

Qwen3-30B-A3B uses the Mixture of Experts (MoE) architecture. Think of
Qwen3-30B-A3B as having 128 specialist consultants on staff, but only calling
on the 8 most relevant experts for each task. This architecture means that the
model runs at the speed of a much smaller 3.3B parameter system. 

## 3. (Almost) one-click local deployment 

The [MLX 8-bit
model](https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-MLX-8bit)
is readily available, optimized specifically for Apple Silicon deployment.
Simon Willison's [deployment
guide](https://simonwillison.net/2025/Jul/29/qwen3-30b-a3b-instruct-2507/)
provides additional implementation details.

## 4. Production deployment made affordable

For deployment, the best gang-for-the-buck is a [Mac Mini M4 Pro with 64GB
RAM](https://www.apple.com/nz/shop/buy-mac/mac-mini/apple-m4-pro-chip-with-12-core-cpu-16-core-gpu-24gb-memory-512gb). It gives 79 tokens/second with MLX optimization for $4,299 NZD. 

Or, you can go with one or two [RTX 5090 GPU (NZD $6,799 each on
PBTech)](https://www.pbtech.co.nz/product/VGAASU350901/ASUS-ROG-ASTRAL-NVIDIA-GeForce-RTX-5090-OC-GAMING)
runs about 48 tokens/seconds. 

Note that Even though the GPU approach appears to be slower than M4 Pro, it
does open up the possibility of further scaling through further
runtime/pipeline options, e.g.
[unsloth](https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF). Also, you get to
choose different parameters for [Thinking and Non-Thinking
Mode](https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF) or use
[GRPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb)
for fine tuning.

## 5. Tool use

Qwen3-30B-A3B brings excellent function calling capabilities to local
deployment—a feature notably absent in Llama 3.3 70B. Qwen3-30B-A3B further
simplifies this task with the [Qwen-Agent
framework](https://github.com/QwenLM/Qwen-Agent), providing built-in support
for:

- **MCP (Model Context Protocol)** configuration for standardized tool definitions
- **Native tool integration** including code interpreters and API calls
- **Hybrid thinking modes** that switch between deep reasoning and fast response

## 6. Apache 2.0 licensing

Qwen3-30B-A3B adopts the Apache 2.0 license, which is also a much-welcomed change
from the Llama 3.3 license. The Apache 2.0 license is one of the most permissive and
widely accepted open-source licenses, allowing you to use, modify, and
distribute the model with minimal restrictions. It's the same license powering
many open source projects so your legal team probably already knows it. 
This contrasts sharply with Llama's custom license, which imposes user count
thresholds and revenue restrictions that can complicate commercial use.

## But isn't Qwen3 from China?

Yes. But the beauty of local deployment lies in its complete neutrality.
Whether a model comes from Silicon Valley, Beijing, or Paris becomes irrelevant
when it runs exclusively on your hardware. Qwen3-30B-A3B offers the same data
sovereignty guarantees as any locally-deployed software: your data stays on
your servers, processed by your infrastructure, governed by your policies. 

## Models come and go

You see, the AI landscape changes weekly. New models, new capabilities, new price
points. Without a robust evaluation framework, you're flying blind—making
decisions based on vendor marketing rather than measured performance that's 
relevant to your specific use cases.

BTW, your evaluation framework should be the bedrock of your AI strategy, not just a
tool for comparing models. It should help you answer three questions:

1. Does this model solve our users' actual problems? 
2. Can we measure improvement and progress objectively? 
3. How do we capture feedback to improve continuously? 

These capabilities matter more than which model you choose today, because they
determine how well you'll adapt to whatever comes next. While models depreciate
rapidly—today's state-of-the-art becomes tomorrow's baseline—your evaluation
framework appreciates with use. Each test case refined, each edge case
captured, each performance metric validated adds to an irreplaceable asset. 

So, while models come and go, your evaluation framework remains a long-term asset.
If your business is serious about AI, invest in building a robust evaluation framework.]]></content><author><name></name></author><category term="local-ai" /><summary type="html"><![CDATA[The recommendation shift]]></summary></entry><entry><title type="html">Two Insights from Tao’s Blue/Red Teams Metaphor: Software Testing’s Future and AI as a Coach</title><link href="http://localhost:4000/a-broader-view-on-blue-red-team.html" rel="alternate" type="text/html" title="Two Insights from Tao’s Blue/Red Teams Metaphor: Software Testing’s Future and AI as a Coach" /><published>2025-07-29T09:17:00+12:00</published><updated>2025-07-29T09:17:00+12:00</updated><id>http://localhost:4000/a-broader-view-on-blue-red-team</id><content type="html" xml:base="http://localhost:4000/a-broader-view-on-blue-red-team.html"><![CDATA[Terence Tao's [blue and red
teams](https://mathstodon.xyz/@tao/114915604830689046) post crystalised several
insights about software testing and a different class of AI product that I have
been building but hadn't found the language to articulate until now.


He begins by describing the role of blue and red teams. Blue teams are
builders who construct and defend orders from chaos, while red teams are
hunters and invaders who find the weakest link in a coherent whole and exploit
it.

> In the field of cybersecurity, a distinction is made between the "blue team"
> task of building a secure system, and the "red team" task of locating
> vulnerabilities in such systems.  The blue team is more obviously necessary
> to create the desired product; but the red team is just as essential, given
> the damage that can result from deploying insecure systems.
>
> The nature of these teams mirror each other; mathematicians would call them
> "dual".  The output of a blue team is only as strong as its weakest link: a
> security system that consists of a strong component and a weak component
> (e.g., a house with a securely locked door, but an open window) will be
> insecure (and in fact worse, because the strong component may convey a false
> sense of security).  

His observation about the human dynamics of red teams is particularly insightful:

> Dually, the contributions to a red team can often be
> additive: a red team report that contains both a serious vulnerability and a
> more trivial one is more useful than a report that only contains the serious
> issue, as it is valuable to have the blue team address both vulnerabilities.

Two unexpected insights about QA and testers emerged: 

1) Red teams compound faster. Once there's a vulnerability, subsequent exploit
attempts can build upon it.  This can be quite different from blue teams, where
each new feature or component is a fresh start with a clear boundary from other
neighboring components.

2) Unconventional thinkers are better suited for red team roles. 

Today, in most software organizations, testers are treated as second-class
citizens and are not given the same respect as developers. Research shows
testers typically earn 25-33% less than software engineers with comparable
experience. In worse yet common cases, testers are brought in as an
afterthought to clean up after development is largely complete. 

As AI's code generation capabilities advance, testers may become far more
critical than they are today. Finding inconsistencies and ambiguities in
software would provide high-leverage positive impact on the system's integrity,
creating far more business value than just finding isolated bugs that
developers might overlook.

This also requires a shift in recruiting and hiring testers. Instead of manual
laborers content with repetitive tasks, we need people with explorative and
inquisitive mindsets. 

Tao then applies this framework to AI products—an insightful perspective
coming from a mathematician rather than a software engineer:

> Many of the proposed use cases for AI tools try to place such tools in the
> "blue team" category, such as creating code, text, images, or mathematical
> arguments in some semi-automated or automated fashion, that is intended for
> use for some external application.  However, in view of the unreliability and
> opacity of such tools, it may be better to put them to work on the "red
> team", critiquing the output of blue team human experts but not directly
> replacing that output; "blue team" AI use should only be permitted up to the
> capability of one's "red team" to catch and correct any errors generated.
> This approach not only plays to current AI strengths, such as breadth of
> exposure and fast feedback, but also mitigates the risks of deploying
> unverified AI output in high-stakes settings.
> 
> In my own personal experiments with AI, for instance, I have found it to be
> useful for providing additional feedback on some proposed text, argument,
> code, or slides that I have generated (including this current text).  I might
> only agree with a fraction of the suggestions generated by the AI tool; but I
> find that there are still several useful comments made that I do agree with,
> and incorporate into my own output.  This is a significantly less glamorous
> or intuitive use case for AI than the more commonly promoted "blue team" one
> of directly automating one's own output, but one that I find adds much more
> reliable value.

This suggests a new category of AI products focused on coaching and feedback
rather than direct output generation. 

[alexdong/high-taste](https://github.com/alexdong/high-taste) is my small
experiment in this direction—using AI to develop coding judgment rather than
generate code. Now imagine red team AI across every domain: tools that
critique your arguments, challenge your assumptions, stress-test your
strategies. Not to replace expertise, but to forge it.

AI's capabilities remain frustratingly jagged—brilliant at some tasks, 
unreliable at others. But perhaps that's exactly why red team AI works: 
it sidesteps AI's weaknesses while amplifying what it does well. Maybe the 
companies building critique tools today might discover a more constructive
path through the AI landscape than those chasing perfect generation.]]></content><author><name></name></author><summary type="html"><![CDATA[Terence Tao’s blue and red teams post crystalised several insights about software testing and a different class of AI product that I have been building but hadn’t found the language to articulate until now.]]></summary></entry><entry><title type="html">This is to have succeeded.</title><link href="http://localhost:4000/Emerson-quote-this-is-to-have-succeeded.html" rel="alternate" type="text/html" title="This is to have succeeded." /><published>2022-10-31T14:38:00+13:00</published><updated>2022-10-31T14:38:00+13:00</updated><id>http://localhost:4000/Emerson-quote-this-is-to-have-succeeded</id><content type="html" xml:base="http://localhost:4000/Emerson-quote-this-is-to-have-succeeded.html"><![CDATA[Today, I printed out a quote from Emerson and put onto our fridge 
so we can all look at it everyday.

> To laugh often and much; to win the respect of intelligent people and
> the affection of children; to earn the appreciation of honest critics
> and endure the betrayal of false friends; to appreciate beauty; to
> find the best in others; to leave the world a bit better, whether by a
> healthy child, a garden patch, or a redeemed social condition; to know
> even one life has breathed easier because you have lived. This is to
> have succeeded."]]></content><author><name></name></author><category term="life" /><summary type="html"><![CDATA[Today, I printed out a quote from Emerson and put onto our fridge so we can all look at it everyday.]]></summary></entry></feed>
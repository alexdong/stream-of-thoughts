<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://alexdong.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://alexdong.com/" rel="alternate" type="text/html" /><updated>2025-08-04T10:59:44+12:00</updated><id>https://alexdong.com/feed.xml</id><title type="html">Alex Dong’s Blog</title><subtitle>Stream of thoughts from Alex Dong. What I find interesting, intriguing or insightful.</subtitle><entry><title type="html">Qwen3-30B: The Open Source AI Model That Changes On-Premises Deployment Economics</title><link href="https://alexdong.com/local-ai/2025/08/01/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty.html" rel="alternate" type="text/html" title="Qwen3-30B: The Open Source AI Model That Changes On-Premises Deployment Economics" /><published>2025-08-01T14:30:00+12:00</published><updated>2025-08-01T14:30:00+12:00</updated><id>https://alexdong.com/local-ai/2025/08/01/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty</id><content type="html" xml:base="https://alexdong.com/local-ai/2025/08/01/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty.html"><![CDATA[<h2 id="the-recommendation-shift">The recommendation shift</h2>

<p>For the past few months, I’ve recommended Llama 3.3 70B with 8-bit quantization to New Zealand organizations prioritizing data sovereignty. That recommendation is changing.</p>

<p><a href="https://qwenlm.github.io/blog/qwen3-30b-a3b/">Qwen3-30B-A3B-Instruct-2507</a> delivers o3-mini performance while running 10× faster on consumer hardware.</p>

<p>Qwen3-30B-A3B’s release marks a fundamental shift in the economics of AI deployment. 
This is the first time a local model has matched the performance of larger
models while being fast enough for real-world applications.</p>

<p>This transforms local deployment from a development curiosity into a production-ready solution. For New Zealand organizations prioritizing data sovereignty, faster inference at lower cost removes the last barriers to keeping AI on-premises.</p>

<p>Here’s what makes Qwen3-30B-A3B the practical choice for organizations prioritizing data sovereignty:</p>

<ol>
  <li><strong>Benchmark performance matching larger models</strong> - Competing with GPT-4o and Claude 3.5 despite being 4-9× smaller</li>
  <li><strong>10× speed improvement through MoE architecture</strong> - 40-50 tokens/second on consumer hardware</li>
  <li><strong>Simplified local deployment</strong> - Ready-to-use 8-bit quantized models for immediate production use</li>
  <li><strong>70% reduction in hardware costs</strong> - Single GPU deployment instead of multi-GPU clusters</li>
  <li><strong>256K token context window</strong> - Process entire codebases without chunking</li>
  <li><strong>Native tool use capabilities</strong> - Built-in function calling surpassing Llama 3.3 70B</li>
  <li><strong>Apache 2.0 licensing</strong> - True open source without user count or revenue restrictions</li>
</ol>

<h2 id="1-benchmark-result-matching-larger-models">1. Benchmark result matching larger models</h2>

<p>Despite having just 30B parameters, Qwen3-30B-A3B competes with models 4-9 times its size:</p>

<table>
  <thead>
    <tr>
      <th>Benchmark</th>
      <th>Qwen3-30B-A3B</th>
      <th>GPT-4o</th>
      <th>Claude 3.5 Sonnet</th>
      <th>Gemini 1.5 Pro</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>ArenaHard</strong></td>
      <td><strong>91.0</strong></td>
      <td>85.3</td>
      <td>87.1</td>
      <td>-</td>
      <td>Complex reasoning &amp; instruction following</td>
    </tr>
    <tr>
      <td><strong>AIME’24/25</strong></td>
      <td><strong>80.4</strong></td>
      <td>41.4</td>
      <td>-</td>
      <td>52.7</td>
      <td>Advanced mathematical problem-solving</td>
    </tr>
    <tr>
      <td><strong>GPQA</strong></td>
      <td><strong>70.4%</strong></td>
      <td>65.1%</td>
      <td>72.3% (Opus)</td>
      <td>-</td>
      <td>Graduate-level science questions</td>
    </tr>
    <tr>
      <td><strong>LiveBench</strong></td>
      <td><strong>69.0</strong></td>
      <td>68.2 (GPT-4)</td>
      <td>-</td>
      <td>65.8 (Flash)</td>
      <td>Real-world task performance</td>
    </tr>
    <tr>
      <td><strong>Creative Writing</strong></td>
      <td><strong>86.0</strong></td>
      <td>84.2</td>
      <td>83.7 (Haiku)</td>
      <td>-</td>
      <td>Writing quality assessment</td>
    </tr>
  </tbody>
</table>

<p>For broader context, Llama 3.3 70B achieves 86% on MMLU (general knowledge) versus GPT-4o’s 87.2% and Claude 3.5 Sonnet’s 88.7%. The key insight? Qwen3-30B-A3B delivers 95%+ of larger models’ performance while using a fraction of the resources.</p>

<p>For real-world applications, <a href="https://simonwillison.net/2025/Jul/29/qwen3-30b-a3b-instruct-2507/">Simon Willison</a> confirms these results in practical applications, noting performance “approaching GPT-4o and larger Qwen models.”</p>

<h2 id="2-the-speed-advantage-through-moe-architecture">2. The speed advantage through MoE architecture</h2>

<p><strong>Option 1: Technical clarity focus</strong>
The breakthrough comes from Qwen3-30B-A3B’s Mixture of Experts (MoE) architecture. With 128 specialized expert networks but only 8 active per token, the model achieves the seemingly impossible: 30.5B parameters of capability running at 3.3B parameter speeds. This selective activation transforms the performance equation—you get flagship model quality without the computational penalty.</p>

<p><strong>Option 2: Business impact emphasis</strong>
Qwen3-30B-A3B’s efficiency breakthrough stems from its Mixture of Experts design—a technical innovation with profound business implications. By activating just 8 of its 128 expert networks per computation, the model delivers enterprise-grade performance at startup-friendly speeds. This isn’t incremental improvement; it’s a step change in what’s possible with local AI deployment.</p>

<p><strong>Option 3: Narrative approach</strong>
Think of Qwen3-30B-A3B as having 128 specialist consultants on staff, but only calling on the 8 most relevant experts for each task. This Mixture of Experts architecture means the full 30.5B parameters are available when needed, but the model runs at the speed of a much smaller 3.3B parameter system. The result? Production-ready performance that actually fits within reasonable hardware budgets.</p>

<p><strong>Option 4: Direct comparison</strong>
While traditional models activate all parameters for every token, Qwen3-30B-A3B’s Mixture of Experts architecture takes a radically different approach. Only 8 of 128 expert networks fire per token, creating an efficiency multiplier that makes local deployment viable. Where Llama 3.3 70B struggles to maintain usable speeds even with quantization, Qwen3-30B-A3B delivers 40-50 tokens per second—fast enough for real production use.</p>

<p><strong>Option 5: Problem-solution framing</strong>
Local AI deployment has always faced a cruel trade-off: either accept poor performance from small models or invest in expensive infrastructure for large ones. Qwen3-30B-A3B’s Mixture of Experts architecture breaks this dilemma. By routing each token through just 8 of its 128 expert networks, it maintains the quality of a 30B parameter model while running at the speed of a 3B parameter system. The economics finally work.</p>

<p>This 8-10× speed improvement enables local production workloads without the latency that made previous large models impractical. Standard 8-bit quantization delivers this performance—no need for aggressive 4-bit or 6-bit approaches that compromise quality for speed.</p>

<p>In practical terms: Mac Studio M3 Ultra running Llama 3.3 70B with 8-bit quantization achieves 5-7 tokens per second. The same hardware running Qwen3-30B-A3B delivers 40-50 tokens per second. For comparison, Claude Haiku 3.5’s API serves 52-65 tokens per second. This speed makes Qwen3-30B-A3B viable for interactive development and production applications.</p>

<h2 id="3-local-deployment-made-practical">3. Local deployment made practical</h2>

<p>The <a href="https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-MLX-8bit">MLX 8-bit
model</a>
is readily available, optimized specifically for Apple Silicon deployment.</p>

<p>Getting started is straightforward:</p>
<ol>
  <li>Download the <a href="https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-MLX-8bit">MLX 8-bit
model</a></li>
  <li>Use LM Studio or MLX for immediate deployment</li>
  <li>For production, consider vLLM or TensorRT-LLM for optimal performance</li>
</ol>

<p>Simon Willison’s <a href="https://simonwillison.net/2025/Jul/29/qwen3-30b-a3b-instruct-2507/">deployment
guide</a>
provides additional implementation details.</p>

<h2 id="4-production-deployment-made-affordable">4. Production deployment made affordable</h2>

<p>The efficiency gains translate directly to hardware savings. An 8-bit quantized
Qwen3-30B-A3B requires approximately 30GB of memory, compared to 70GB for Llama
3.3 70B. This means production deployment needs just 32-48GB of RAM instead of
128-192GB.</p>

<p>For GPU deployment, a single RTX 4090 (NZD $3,284-4,499) with 24GB VRAM handles inference with optimization. Llama 3.3 70B requires dual RTX 4090s or enterprise GPUs costing NZD $10,000+. This 70% cost reduction—both upfront and operational—makes local deployment accessible to organizations previously priced out of on-premises AI.</p>

<h2 id="5-extended-context-window">5. Extended context window</h2>

<p>Qwen3-30B-A3B brings a 256K token context window to local deployment.</p>

<p>This compares favorably to alternatives: Llama 3.3 70B supports 128K tokens,
while Claude Haiku 3.5 offers 200K. The extra headroom matters when you’re
building applications that need to maintain context across extended
interactions or process substantial documentation in single passes.</p>

<p>In production, this enables processing entire codebases, maintaining week-long conversation histories, or analyzing book-length documents without chunking. For local models, this eliminates a traditional compromise—you no longer trade context length for on-premises deployment benefits.</p>

<p>%&lt;—Interestingly, API providers are already serving Qwen3-30B-A3B at $0.30-0.45
per million tokens—5-10× cheaper than Gemini 2.5 Flash, o3-mini, or Claude
Haiku despite comparable performance. This pricing suggests the major providers
aren’t running substantial margins on their current offerings, and the Qwen
team may have fundamentally shifted the economics of API-based AI.</p>

<p>For local deployment, the break-even point shifts dramatically—a Mac Mini M4
Pro pays for itself after processing just 500K-1M tokens, making local
deployment viable for many more organizations.
—&gt;%</p>

<h2 id="6-tool-use">6. Tool use</h2>

<p>Qwen3-30B-A3B brings enterprise-grade function calling capabilities to local deployment—a feature notably absent in Llama 3.3 70B. While Llama achieves 77.3% on the Berkeley Function Calling Leaderboard, it requires custom implementation for production tool use.</p>

<p>Qwen3-30B-A3B simplifies this with the <a href="https://github.com/QwenLM/Qwen-Agent">Qwen-Agent framework</a>, providing built-in support for:</p>
<ul>
  <li><strong>MCP (Model Context Protocol)</strong> configuration for standardized tool definitions</li>
  <li><strong>Native tool integration</strong> including code interpreters and API calls</li>
  <li><strong>Hybrid thinking modes</strong> that switch between deep reasoning and fast response</li>
</ul>

<p>In practice, this means you can deploy agentic AI applications without building custom tool-calling infrastructure. For organizations automating workflows or integrating AI with existing systems, this represents months of saved development time.</p>

<h2 id="7-apache-20-licensing">7. Apache 2.0 licensing</h2>

<p>Qwen3-30B-A3B adopts the Apache 2.0 license, which is also a game-changer for
New Zealand startups.</p>

<p>The Apache 2.0 license is one of the most permissive and widely accepted
open-source licenses, allowing you to use, modify, and distribute the model
with minimal restrictions. It’s the same license powering many open source
projects so your legal team probably already knows it.</p>

<p>This contrasts sharply with Llama’s custom license, which imposes user count
thresholds and revenue restrictions that can complicate commercial use.</p>

<h2 id="but-isnt-qwen3-from-china">But isn’t Qwen3 from China?</h2>

<p><strong>Option 1: Technical neutrality focus</strong>
Yes, Qwen3 originates from Alibaba’s research team. But here’s what matters: once deployed locally, it’s simply a mathematical model running on your infrastructure. No data transmission to China. No external dependencies. No ongoing connection to Alibaba’s systems. The model weights are static files on your servers, processing data entirely within your security perimeter. From a technical standpoint, the country of origin becomes as relevant as asking where your database software was developed.</p>

<p><strong>Option 2: Sovereignty-first framing</strong>
The beauty of local deployment lies in its complete neutrality. Whether a model comes from Silicon Valley, Beijing, or Paris becomes irrelevant when it runs exclusively on your hardware. Qwen3-30B-A3B, despite its Alibaba origins, offers the same data sovereignty guarantees as any locally-deployed software: your data stays on your servers, processed by your infrastructure, governed by your policies. For New Zealand organizations bound by privacy regulations, this distinction between origin and operation is crucial.</p>

<p><strong>Option 3: Risk mitigation angle</strong>
Concerns about Chinese AI models typically center on data security and potential backdoors. Local deployment eliminates both risks. The open-source nature means security researchers worldwide can—and have—audited the code. More importantly, when Qwen3-30B-A3B runs on your air-gapped or network-isolated systems, there’s no mechanism for data exfiltration. It’s the deployment model, not the development origin, that determines your security posture.</p>

<p><strong>Option 4: Practical comparison</strong>
Consider the alternatives: using OpenAI means your data flows to US servers. Google’s models route through their global infrastructure. Anthropic processes everything in their cloud. With locally-deployed Qwen3-30B-A3B, your data never leaves your premises—regardless of where the model was created. For organizations serious about data sovereignty, the question isn’t “who built it?” but “where does my data go?”</p>

<p><strong>Option 5: Strategic positioning</strong>
Alibaba’s massive investment in Qwen3 development actually benefits local deployment advocates. They’ve created a model competitive with GPT-4 and made it freely available under Apache 2.0 licensing—no strings attached. This geopolitical competition in AI has produced an unexpected winner: organizations that want top-tier AI capabilities without depending on any nation’s cloud infrastructure. The irony is that concerns about Chinese technology have created the perfect case for the very solution Qwen3-30B-A3B enables: true technological independence through local deployment.</p>

<h2 id="your-evaluation-framework--model-choice">Your evaluation framework &gt; Model choice</h2>

<p>The ground rule I emphasize when advising clients: invest 80% of your AI budget in evaluation frameworks first. While tech Twitter celebrates weekly model releases, your competitive advantage comes from how you evaluate and apply these models to specific use cases.</p>

<p><strong>Option 1: Strategic differentiation</strong>
Think of AI models as engines and your evaluation framework as the vehicle design. Anyone can buy the same engine, but how you integrate it, optimize it, and apply it to specific use cases determines competitive advantage. Your evaluation framework captures institutional knowledge that no model vendor can replicate: understanding of your customers’ edge cases, industry-specific quality standards, and the subtle differences between acceptable and exceptional outputs in your domain.</p>

<p><strong>Option 2: Investment protection</strong>
While models depreciate rapidly—today’s state-of-the-art becomes tomorrow’s baseline—your evaluation framework appreciates with use. Each test case refined, each edge case captured, each performance metric validated adds to an irreplaceable asset. This framework enables you to switch models seamlessly as better options emerge, maintaining quality while reducing costs. It’s the difference between being locked into a single vendor and having the agility to adopt whatever serves your customers best.</p>

<p><strong>Option 3: Practical implementation</strong>
Start with real user queries, not synthetic benchmarks. Build test suites from actual support tickets, customer complaints, and successful interactions. Weight your metrics based on business impact: a 5% improvement in invoice processing accuracy might matter more than a 20% gain in creative writing scores. This domain-specific evaluation becomes your compass for model selection, fine-tuning priorities, and quality assurance—assets that compound in value while model costs race toward zero.</p>

<p><strong>Option 4: Competitive moat building</strong>
Models are becoming utilities. GPT-4’s capabilities will be table stakes within 18 months. But your ability to consistently deliver domain-specific quality? That’s defensible. An evaluation framework built on deep customer understanding allows you to fine-tune smaller, faster models to outperform larger ones on your specific tasks. This expertise—encoded in test cases, validated through real usage, refined through feedback loops—becomes the sustainable differentiator in a world of commodity AI.</p>

<p><strong>Option 5: Future-proofing perspective</strong>
The AI landscape changes weekly. New models, new capabilities, new price points. Without a robust evaluation framework, you’re flying blind—making decisions based on vendor marketing rather than measured performance. Build your framework to answer three questions: Does this model solve our users’ actual problems? Can we measure improvement objectively? How do we capture feedback to improve continuously? These capabilities matter more than which model you choose today, because they determine how well you’ll adapt to whatever comes next.</p>]]></content><author><name></name></author><category term="local-ai" /><summary type="html"><![CDATA[The recommendation shift]]></summary></entry><entry><title type="html">Two Insights from Tao’s Blue/Red Teams Metaphor: Software Testing’s Future and AI as a Coach</title><link href="https://alexdong.com/2025/07/29/a-broader-view-on-blue-red-team.html" rel="alternate" type="text/html" title="Two Insights from Tao’s Blue/Red Teams Metaphor: Software Testing’s Future and AI as a Coach" /><published>2025-07-29T09:17:00+12:00</published><updated>2025-07-29T09:17:00+12:00</updated><id>https://alexdong.com/2025/07/29/a-broader-view-on-blue-red-team</id><content type="html" xml:base="https://alexdong.com/2025/07/29/a-broader-view-on-blue-red-team.html"><![CDATA[<p>Terence Tao’s <a href="https://mathstodon.xyz/@tao/114915604830689046">blue and red
teams</a> post crystalised several
insights about software testing and a different class of AI product that I have
been building but hadn’t found the language to articulate until now.</p>

<p>He begins by describing the role of blue and red teams. Blue teams are
builders who construct and defend orders from chaos, while red teams are
hunters and invaders who find the weakest link in a coherent whole and exploit
it.</p>

<blockquote>
  <p>In the field of cybersecurity, a distinction is made between the “blue team”
task of building a secure system, and the “red team” task of locating
vulnerabilities in such systems.  The blue team is more obviously necessary
to create the desired product; but the red team is just as essential, given
the damage that can result from deploying insecure systems.</p>

  <p>The nature of these teams mirror each other; mathematicians would call them
“dual”.  The output of a blue team is only as strong as its weakest link: a
security system that consists of a strong component and a weak component
(e.g., a house with a securely locked door, but an open window) will be
insecure (and in fact worse, because the strong component may convey a false
sense of security).</p>
</blockquote>

<p>His observation about the human dynamics of red teams is particularly insightful:</p>

<blockquote>
  <p>Dually, the contributions to a red team can often be
additive: a red team report that contains both a serious vulnerability and a
more trivial one is more useful than a report that only contains the serious
issue, as it is valuable to have the blue team address both vulnerabilities.</p>
</blockquote>

<p>Two unexpected insights about QA and testers emerged:</p>

<p>1) Red teams compound faster. Once there’s a vulnerability, subsequent exploit
attempts can build upon it.  This can be quite different from blue teams, where
each new feature or component is a fresh start with a clear boundary from other
neighboring components.</p>

<p>2) Unconventional thinkers are better suited for red team roles.</p>

<p>Today, in most software organizations, testers are treated as second-class
citizens and are not given the same respect as developers. Research shows
testers typically earn 25-33% less than software engineers with comparable
experience. In worse yet common cases, testers are brought in as an
afterthought to clean up after development is largely complete.</p>

<p>As AI’s code generation capabilities advance, testers may become far more
critical than they are today. Finding inconsistencies and ambiguities in
software would provide high-leverage positive impact on the system’s integrity,
creating far more business value than just finding isolated bugs that
developers might overlook.</p>

<p>This also requires a shift in recruiting and hiring testers. Instead of manual
laborers content with repetitive tasks, we need people with explorative and
inquisitive mindsets.</p>

<p>Tao then applies this framework to AI products—an insightful perspective
coming from a mathematician rather than a software engineer:</p>

<blockquote>
  <p>Many of the proposed use cases for AI tools try to place such tools in the
“blue team” category, such as creating code, text, images, or mathematical
arguments in some semi-automated or automated fashion, that is intended for
use for some external application.  However, in view of the unreliability and
opacity of such tools, it may be better to put them to work on the “red
team”, critiquing the output of blue team human experts but not directly
replacing that output; “blue team” AI use should only be permitted up to the
capability of one’s “red team” to catch and correct any errors generated.
This approach not only plays to current AI strengths, such as breadth of
exposure and fast feedback, but also mitigates the risks of deploying
unverified AI output in high-stakes settings.</p>

  <p>In my own personal experiments with AI, for instance, I have found it to be
useful for providing additional feedback on some proposed text, argument,
code, or slides that I have generated (including this current text).  I might
only agree with a fraction of the suggestions generated by the AI tool; but I
find that there are still several useful comments made that I do agree with,
and incorporate into my own output.  This is a significantly less glamorous
or intuitive use case for AI than the more commonly promoted “blue team” one
of directly automating one’s own output, but one that I find adds much more
reliable value.</p>
</blockquote>

<p>This suggests a new category of AI products focused on coaching and feedback
rather than direct output generation.</p>

<p><a href="https://github.com/alexdong/high-taste">alexdong/high-taste</a> is my small
experiment in this direction—using AI to develop coding judgment rather than
generate code. Now imagine red team AI across every domain: tools that
critique your arguments, challenge your assumptions, stress-test your
strategies. Not to replace expertise, but to forge it.</p>

<p>AI’s capabilities remain frustratingly jagged—brilliant at some tasks, 
unreliable at others. But perhaps that’s exactly why red team AI works: 
it sidesteps AI’s weaknesses while amplifying what it does well. Maybe the 
companies building critique tools today might discover a more constructive
path through the AI landscape than those chasing perfect generation.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Terence Tao’s blue and red teams post crystalised several insights about software testing and a different class of AI product that I have been building but hadn’t found the language to articulate until now.]]></summary></entry><entry><title type="html">This is to have succeeded.</title><link href="https://alexdong.com/life/2022/10/31/Emerson-quote-this-is-to-have-succeeded.html" rel="alternate" type="text/html" title="This is to have succeeded." /><published>2022-10-31T14:38:00+13:00</published><updated>2022-10-31T14:38:00+13:00</updated><id>https://alexdong.com/life/2022/10/31/Emerson-quote-this-is-to-have-succeeded</id><content type="html" xml:base="https://alexdong.com/life/2022/10/31/Emerson-quote-this-is-to-have-succeeded.html"><![CDATA[<p>Today, I printed out a quote from Emerson and put onto our fridge 
so we can all look at it everyday.</p>

<blockquote>
  <p>To laugh often and much; to win the respect of intelligent people and
the affection of children; to earn the appreciation of honest critics
and endure the betrayal of false friends; to appreciate beauty; to
find the best in others; to leave the world a bit better, whether by a
healthy child, a garden patch, or a redeemed social condition; to know
even one life has breathed easier because you have lived. This is to
have succeeded.”</p>
</blockquote>]]></content><author><name></name></author><category term="life" /><summary type="html"><![CDATA[Today, I printed out a quote from Emerson and put onto our fridge so we can all look at it everyday.]]></summary></entry></feed>
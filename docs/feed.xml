<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://alexdong.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://alexdong.com/" rel="alternate" type="text/html" /><updated>2025-10-09T22:37:34+13:00</updated><id>https://alexdong.com/feed.xml</id><title type="html">Alex Dong’s Blog</title><subtitle>Stream of Thoughts from Alex Dong. What I find interesting, intriguing or insightful.</subtitle><entry><title type="html">Cory Doctorow: Reverse Centaurs</title><link href="https://alexdong.com/cory-doctorow-reverse-centaurs.html" rel="alternate" type="text/html" title="Cory Doctorow: Reverse Centaurs" /><published>2025-10-09T22:20:00+13:00</published><updated>2025-10-09T22:20:00+13:00</updated><id>https://alexdong.com/cory-doctorow-reverse-centaurs</id><content type="html" xml:base="https://alexdong.com/cory-doctorow-reverse-centaurs.html"><![CDATA[<p>I picked up Cory Doctorow’s book <a href="https://craphound.com/radicalized/"><em>Radicalized</em></a> recently at the library. Doctorow has a knack for making you care about people in dramatically different social situations. It forced me to really feel for the characters who are on the receiving end of the ugly social impacts of technology “advancements”.</p>

<p>Doctorow said:</p>

<blockquote>
  <p>Science fiction’s superpower isn’t thinking up new technologies – it’s thinking up new social arrangements for technology. What the gadget does is nowhere near as important as who the gadget does it for and who it does it to. Your car can use a cutting-edge computer vision system to alert you when you’re drifting out of your lane – or it can use that same system to narc you out to your insurer so they can raise your premiums by $10 that month to punish you for inattentive driving. Same gadget, different social arrangement.</p>
</blockquote>

<p>Today I stumbled upon another piece titled <a href="https://locusmag.com/feature/commentary-cory-doctorow-reverse-centaurs/"><em>Reverse Centaurs</em></a>. This is a very short and poignant essay that provides a fresh perspective on why we are getting such a dramatically different experience of AI.</p>

<p>The key idea? Be a centaur, not a reverse centaur. A centaur is a human powered by a machine, while a reverse centaur is a machine served by a human who no longer has agency. Their job is to take the blame for the machine’s mistakes.</p>

<blockquote>
  <p>A centaur is a human being who is assisted by a machine that does some onerous task (like transcribing 40 hours of podcasts). A reverse-centaur is a machine that is assisted by a human being, who is expected to work at the machine’s pace. That would be Buscaglia: who was given an assignment to do the work of 50 or more people, on a short timescale, and a shoestring budget.</p>

  <p>When you give a freelancer an assignment to turn around ten summer lists on a short timescale, everyone understands that his job isn’t to write those lists, it’s to supervise a chatbot.</p>

  <p>But his job wasn’t even to supervise the chatbot adequately (single-handedly fact-checking 10 lists of 15 items is a long, labor-intensive pro­cess). Rather, it was to take the blame for the factual inaccuracies in those lists. He was, in the phrasing of Dan Davies, “an accountability sink” (or as Madeleine Clare Elish puts it, a “moral crumple zone”).</p>

  <p>When I used Whisper to transcribe a folder full of MP3s, that was me being a centaur. When Buscaglia was assigned to oversee a chatbot’s error-strewn, 64-page collection of summer lists, on a short timescale and at short pay, with him and him alone bearing the blame for any errors that slipped through, that was him being a reverse-centaur.</p>
</blockquote>]]></content><author><name></name></author><category term="social-impact" /><category term="books" /><summary type="html"><![CDATA[I picked up Cory Doctorow’s book Radicalized recently at the library. Doctorow has a knack for making you care about people in dramatically different social situations. It forced me to really feel for the characters who are on the receiving end of the ugly social impacts of technology “advancements”.]]></summary></entry><entry><title type="html">AI as a Copilot for Mathematical Discovery</title><link href="https://alexdong.com/ai-as-a-copilot-for-mathematical-discovery.html" rel="alternate" type="text/html" title="AI as a Copilot for Mathematical Discovery" /><published>2025-10-08T21:57:00+13:00</published><updated>2025-10-08T21:57:00+13:00</updated><id>https://alexdong.com/ai-as-a-copilot-for-mathematical-discovery</id><content type="html" xml:base="https://alexdong.com/ai-as-a-copilot-for-mathematical-discovery.html"><![CDATA[<p><a href="https://epochai.substack.com/p/ai-can-now-do-math-but-can-it-ask?utm_source=post-email-title&amp;publication_id=3755861&amp;post_id=175540105">Epoch AI’s recent interview with Ken Ono</a> was a fascinating read. Ono, a mathematician at University of Virginia, discussed in greater detail on how AI is transforming mathematical research. It’s a long interview and here are some highlights:</p>

<blockquote>
  <p>Q: What makes the problem hard for an AI?
A: It’s very difficult to predict which problems are the difficult ones.</p>
</blockquote>

<blockquote>
  <p>Q: What have we learned about AI’s ability? 
A: I don’t think I can ask a question that AI can’t identify with a particular area of mathematics, because it seems like ChatGPT and others just have at their fingertips the accumulation of human knowledge. And it continues to surprise me how quickly that seems to be the case. Three years ago, ChatGPT or all of these large language models would get things wrong that a five year old could get right. And now we’re asking questions that a PhD student at our universities wouldn’t even know where to start to look.</p>
</blockquote>

<blockquote>
  <p>Q: Extending into the future, where do you expect AI to land in math, say, three years?
A: When the participants, at the Frontier Math Symposium, see and test out the pro version of ChatGPT that has been provided to us, you can see their eyes light up thinking it is amazing. This could be really, genuinely my copilot. But perhaps the bigger deal will be the realization for the mathematicians that are here and the colleagues, when we all go back to our universities, that AI is really meant to be a copilot. And will it assist our scientific discovery? Absolutely.</p>
</blockquote>

<blockquote>
  <p>Q: Do you use AI in your research?
A: … as a theoretical device, the computers help us discover an area of mathematics that I don’t think any of us thought would exist. So that’s a good partner. A good partner is one that is going to give you clues to conjectures that you can then either prove or build a body of work from.</p>
</blockquote>

<blockquote>
  <p>Q: And how much do you expect AI to reshape math research where zero on a scale from 0 to 10, where zero is the level of a pocket calculator and then ten is math researchers are obsolete.
A: Well, I don’t think math researchers will ever be obsolete because an AI doesn’t know how to generate good questions. If that happens, well, that would be really a profound moment. … Asking good questions that drive a theory takes skill. And I think we will always need people who can do that. And we don’t have many people who can do that. These are generally very special people.</p>
</blockquote>]]></content><author><name></name></author><summary type="html"><![CDATA[Epoch AI’s recent interview with Ken Ono was a fascinating read. Ono, a mathematician at University of Virginia, discussed in greater detail on how AI is transforming mathematical research. It’s a long interview and here are some highlights:]]></summary></entry><entry><title type="html">Inspect AI</title><link href="https://alexdong.com/inspect-ai.html" rel="alternate" type="text/html" title="Inspect AI" /><published>2025-10-07T21:24:00+13:00</published><updated>2025-10-07T21:24:00+13:00</updated><id>https://alexdong.com/inspect-ai</id><content type="html" xml:base="https://alexdong.com/inspect-ai.html"><![CDATA[<p><a href="https://x.com/joecole">Joe</a> mentioned the new <a href="https://safety-research.github.io/petri/">Petri Alignment
tool</a> from Anthropic. As I read
through the GitHub repo, I was surprised to find how simple the code is. As I
dug deeper, I realized it’s actually a plugin for Inspect AI, an open-source
Python project by the UK’s AI Security Institute.</p>

<p>Inspect AI provides scaffolding for evaluating LLMs. I was pleasantly
surprised to see just how many advanced features are already in place:</p>

<ul>
  <li>Execution: <a href="https://inspect.aisi.org.uk/caching.html">provider caching</a>,
<a href="https://inspect.aisi.org.uk/parallelism.html">parallelism for multiple models, benchmarks, or
requests</a>, and
<a href="https://inspect.aisi.org.uk/sandboxing.html">sandboxing across Docker, EC2, or
Proxmox</a></li>
  <li>Debugging: <a href="https://inspect.aisi.org.uk/tracing.html">tracing</a> and tidy <a href="https://inspect.aisi.org.uk/log-viewer.html">log
viewers</a></li>
  <li>Output: built-in <a href="https://inspect.aisi.org.uk/structured.html">Pydantic structured
validation</a> and <a href="https://inspect.aisi.org.uk/typing.html">typed store</a></li>
</ul>

<p>All I need to supply are three ingredients:</p>

<ol>
  <li><a href="https://inspect.aisi.org.uk/datasets.html">Datasets</a> — CSV files with
<code class="language-plaintext highlighter-rouge">id</code>, <code class="language-plaintext highlighter-rouge">input</code>, and <code class="language-plaintext highlighter-rouge">target</code> columns.</li>
  <li><a href="https://inspect.aisi.org.uk/reference/inspect_ai.solver.html#generation">Solvers</a>
— chains of Python functions that take the input and produce the model’s
output.</li>
  <li><a href="https://inspect.aisi.org.uk/reference/inspect_ai.scorer.html">Scorers</a> —
metrics that turn those outputs into scores. Regular expressions, F1,
statistics like <code class="language-plaintext highlighter-rouge">std</code> or <code class="language-plaintext highlighter-rouge">stderr</code>, and even LLM-as-Judge are ready to go.</li>
</ol>

<p>The tool is well documented, strongly typed, and actively maintained. Next up I
want to use it to rewrite some of the benchmarks end-to-end and see how easily I can
extend it to support more complex evaluation workflows.</p>]]></content><author><name></name></author><category term="eval" /><summary type="html"><![CDATA[Joe mentioned the new Petri Alignment tool from Anthropic. As I read through the GitHub repo, I was surprised to find how simple the code is. As I dug deeper, I realized it’s actually a plugin for Inspect AI, an open-source Python project by the UK’s AI Security Institute.]]></summary></entry><entry><title type="html">Dwarkesh’s Fossil Fuel Analogy</title><link href="https://alexdong.com/dwarkesh-s-fossil-fuel-analogy.html" rel="alternate" type="text/html" title="Dwarkesh’s Fossil Fuel Analogy" /><published>2025-10-06T22:35:00+13:00</published><updated>2025-10-06T22:35:00+13:00</updated><id>https://alexdong.com/dwarkesh-s-fossil-fuel-analogy</id><content type="html" xml:base="https://alexdong.com/dwarkesh-s-fossil-fuel-analogy.html"><![CDATA[<p>Dwarkesh published a short <a href="https://www.dwarkesh.com/p/thoughts-on-sutton">follow-up</a> to his previous interview with Richard Sutton that I commented <a href="/thoughts-on-richard-sutton-s-interview-on-dwarkesh-podcast.html">extensively last week</a>.</p>

<p>I think Dwarkesh’s point on LLM may not be “the” right path forever, but it is definitely a path that we can keep exploring for a while, and maybe even take us to AGI or ASI. This doesn’t mean Sutton is wrong, but it does mean that we shouldn’t just dismiss LLM now and solely focus on RL.</p>

<p>The analogy Dwarkesh made is apt:</p>

<blockquote>
  <p>In a talk a few months ago, Ilya compared pretraining data to fossil fuels. This analogy has remarkable reach. Just because fossil fuels are not renewable does not mean that our civilization ended up on a dead-end track by using them. You simply couldn’t have transitioned from the water wheels in 1800 to solar panels and fusion power plants. We had to use this cheap, convenient, plentiful intermediary.</p>

  <p>AlphaGo (which was conditioned on human games) and AlphaZero (which was bootstrapped from scratch) were both superhuman Go players. AlphaZero was better.</p>

  <p>Will we (or the first AGIs) eventually come up with a general learning technique that requires no initialization of knowledge - that just bootstraps itself from the very start? And will it outperform the very best AIs that have been trained to that date? Probably yes.</p>

  <p>But does this mean that imitation learning must not play any role whatsoever in developing the first AGI, or even the first ASI? No. AlphaGo was still superhuman, despite being initially shepherded by human player data. The human data isn’t necessarily actively detrimental - at enough scale it just isn’t significantly helpful.</p>

  <p>The accumulation of knowledge over tens of thousands of years has clearly been essential to humanity’s success. In any field of knowledge, thousands (and likely millions) of previous people were involved in building up our understanding and passing it on to the next generation. We didn’t invent the language we speak, nor the legal system we use, nor even most of the knowledge relevant to the technologies in our phones. This process is more analogous to imitation learning than to RL from scratch.</p>
</blockquote>]]></content><author><name></name></author><category term="reinforcement-learning" /><summary type="html"><![CDATA[Dwarkesh published a short follow-up to his previous interview with Richard Sutton that I commented extensively last week.]]></summary></entry><entry><title type="html">Earth was born dry until a cosmic collision made it a blue planet</title><link href="https://alexdong.com/earth-was-born-dry-until-a-cosmic-collision-made-it-a-blue-planet.html" rel="alternate" type="text/html" title="Earth was born dry until a cosmic collision made it a blue planet" /><published>2025-10-05T22:47:00+13:00</published><updated>2025-10-05T22:47:00+13:00</updated><id>https://alexdong.com/earth-was-born-dry-until-a-cosmic-collision-made-it-a-blue-planet</id><content type="html" xml:base="https://alexdong.com/earth-was-born-dry-until-a-cosmic-collision-made-it-a-blue-planet.html"><![CDATA[<p>From <a href="https://www.sciencedaily.com/releases/2025/09/250928095654.htm">Science Daily</a></p>

<blockquote>
  <p>Scientists have shown that Earth’s basic chemistry solidified within just
three million years of the Solar System’s formation. Initially, the planet
was barren and inhospitable, missing water and carbon compounds. A colossal
collision with Theia likely changed everything, bringing the essential
ingredients for life. The study highlights that habitability may hinge on
rare chance events.</p>
</blockquote>]]></content><author><name></name></author><category term="cool-science" /><summary type="html"><![CDATA[From Science Daily]]></summary></entry><entry><title type="html">Pavel Durov’s interview with Lex Fridman</title><link href="https://alexdong.com/pavel-durov-s-interview-with-lex-fridman.html" rel="alternate" type="text/html" title="Pavel Durov’s interview with Lex Fridman" /><published>2025-10-04T18:30:00+13:00</published><updated>2025-10-04T18:30:00+13:00</updated><id>https://alexdong.com/pavel-durov-s-interview-with-lex-fridman</id><content type="html" xml:base="https://alexdong.com/pavel-durov-s-interview-with-lex-fridman.html"><![CDATA[<p>Pavel’s sincerity (when he recounted the interaction with the French Intelligence service), strong ethical foundation (when asked about what would happen if he is sentenced to 20 years in prison, he said he would starve to death to “reboot” the game rather than give in), and his slow, precise, and deliberate responses are a fresh breeze in an industry that has so much hot air.</p>

<p>I am a fan.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/qjPH9njnaVU?si=JnSWhbkusfcuulmd" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>]]></content><author><name></name></author><summary type="html"><![CDATA[Pavel’s sincerity (when he recounted the interaction with the French Intelligence service), strong ethical foundation (when asked about what would happen if he is sentenced to 20 years in prison, he said he would starve to death to “reboot” the game rather than give in), and his slow, precise, and deliberate responses are a fresh breeze in an industry that has so much hot air.]]></summary></entry><entry><title type="html">Agentic Loop Design - Tool #1: Long-Term Memory that Survives Resets</title><link href="https://alexdong.com/agentic-loop-design-long-term-memory-and-resets.html" rel="alternate" type="text/html" title="Agentic Loop Design - Tool #1: Long-Term Memory that Survives Resets" /><published>2025-10-03T19:20:00+13:00</published><updated>2025-10-03T19:20:00+13:00</updated><id>https://alexdong.com/agentic-loop-design-long-term-memory-and-resets</id><content type="html" xml:base="https://alexdong.com/agentic-loop-design-long-term-memory-and-resets.html"><![CDATA[<p>When a session drags on and the context window fills up, I often watch the
models start to get lazy. They drop earlier decisions, skim the edges, and
sometimes declare “all tests are passing” after touching only part of the
suite.</p>

<p>Both Codex and Claude Code ship a <code class="language-plaintext highlighter-rouge">/compact</code> command that compresses,
summarizes, and trims conversation history to make room for new messages. In
practice, the compaction drops enough detail that I reach for it less and less.</p>

<p>We need a way to reset the conversation without losing the continuity of the
work. We need a tool that lets us restart often and still remember what we were
doing and where we left off. There have been many attempts at “Context
Compression,” but none have worked well enough yet.</p>

<p>That tool is a plaintext <code class="language-plaintext highlighter-rouge">PLAN.md</code>. As the model cycles through the
design-plan-implement loop, it appends and updates this file, then re-reads it
at the start of every new session.</p>

<p>Here’s the scaffold I use.</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gu">## What and Why</span>
<span class="nt">&lt;</span><span class="err">!,</span>  <span class="na">define</span> <span class="na">the</span> <span class="na">what</span><span class="err">,</span> <span class="na">why</span><span class="err">,</span> <span class="na">scope</span> <span class="err">,</span> <span class="nt">&gt;</span>

<span class="gu">## Observation</span>
<span class="nt">&lt;</span><span class="err">!,</span>  <span class="na">facts</span> <span class="na">only</span> <span class="na">from</span> <span class="na">Observation.</span> <span class="na">Research.</span> <span class="na">Note</span> <span class="na">down</span> <span class="na">relevant</span> <span class="na">facts.</span> <span class="err">,</span> <span class="nt">&gt;</span>

<span class="gu">## Design</span>
<span class="nt">&lt;</span><span class="err">!,</span>  <span class="na">chosen</span> <span class="na">shapes</span> <span class="err">&amp;</span> <span class="na">interfaces</span><span class="err">;</span> <span class="na">proposed</span> <span class="na">options</span> <span class="na">when</span> <span class="na">there</span> <span class="na">are</span> <span class="na">real</span> <span class="na">trade-offs</span> <span class="err">,</span> <span class="nt">&gt;</span>

<span class="gu">## Plan</span>
<span class="nt">&lt;</span><span class="err">!,</span>  <span class="na">five-minute</span> <span class="na">tasks</span> <span class="na">with</span> <span class="err">[</span> <span class="err">]</span> <span class="na">checkboxes</span> <span class="na">and</span> <span class="na">nested</span> <span class="na">numbering</span> <span class="err">,</span> <span class="nt">&gt;</span>
</code></pre></div></div>

<p>Running the loop means cycling through four passes. After each pass I ask the
model to capture the discussion, note the details, and record the conclusions
in PLAN.md, then spin up a fresh session. The file becomes the single source of
truth the agent must consult before touching anything.</p>

<p>Here are the prompts I use for each phase.</p>

<h2 id="what-and-why">What and Why</h2>

<p>I start by anchoring the change in plain language so the model and I share the
same target and boundaries.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Task: Refresh the "## What and Why" section in ./plans/PLAN-{slug}.md for "{brief change name}".

Steps:
1. Describe the change in 1–2 sentences.
1. Clarify why it matters now.
2. Capture scope boundaries: what is in/out, success measures.
3. List open questions that block commitment; ask me for any missing context.

Guardrails:
- No solution or implementation steps.
- Write in tight paragraphs or focused bullet points.
- Stop after the Why section is updated.
</code></pre></div></div>

<h2 id="observation">Observation</h2>

<p>Once the intent is locked, the next pass is a fact-finding sweep. No edits, no plans.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Task: Compile the Observation section for the changes described in `## What and Why`.

Read ./plans/PLAN-{slug}.md first, then browse, research, and note down facts.

Deliverable:
- Append under "## Observation".
- One bullet per relevant file, symbol, or endpoint: location + behavior in 1–2 sentences.
- Close with "Risks &amp; brittle spots" listing edge cases or debt (facts only).

Scope to inspect:
- Code/config touching the affected data or control paths.
- Schemas/migrations, jobs, CLIs, scripts, docs, and tests relying on current behavior.
- External integrations, rate limits, feature flags, and environment switches.

Guardrails:
- Mark uncertain items with "?" rather than omitting them.
- Do not edit code or propose changes.
- Stop after Observation is complete.
- Be thorough and methodical. Go through the files one by one.
</code></pre></div></div>

<h2 id="design">Design</h2>

<p>With the facts in place, we move into the design phase and ask for options,
trade-off analysis, and key code snippets.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Task: Draft the Design section for "What" using the "Observation" in ./plans/PLAN-{slug}.md.

Deliverable:
- Append under "## Design" in PLAN-{slug}.md.
- Describe chosen data flows, interfaces, and storage, naming concrete functions/types.
- When trade-offs exist, list 2–3 options with crisp pros/cons before recommending one.
- End with "Implications for tests" describing what must be covered.
- Include just enough code snippets to show the high-level design.

Guardrails:
- Ground every decision in Observation facts or clearly stated assumptions.
- No implementation details below the interface level.
- Stop after Design is complete.
</code></pre></div></div>

<h2 id="plan">Plan</h2>

<p>The design turns into an execution checklist: small, verifiable moves that keep
the loop tight.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Task: Translate the Design for "{brief change name}" into an execution plan.

Deliverable:
- Append under "## Plan" in PLAN-{slug}.md with checkbox tasks.
- Keep tasks to five-minute chunks; nest numbered substeps if needed.
- Include "Tests (fit)" for each task describing what must be verified before coding.

Order of operations:
1. Types/constants and contracts.
2. Write/update data producers.
3. Read paths/consumers.
4. Data migrations/backfills.
5. Clean-up and docs.

Guardrails:
- Highlight removals/renames and search/replace surfaces explicitly.
- Stop after the Plan section is written.
</code></pre></div></div>

<h2 id="implement">Implement</h2>

<p>Implementation finally pulls a single checkbox off the plan and runs it to
completion before coming up for air.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>We are now implementing the tasks from the PLAN-{slug}.md file.

Execute the following tasks in order.
&gt; {paste the exact checkbox line}

Execution rules:
- Begin with the task's Tests, write or adjust checks first.
- Make the smallest code changes needed to satisfy those tests.
- Run the standard project checks (`make lint`, `make test`) and report results.
- If anything fails, even outside touched areas, resolve it before stopping.
- Review the changes and rewrite the code changes to improve clarity or efficiency.
- Keep rewriting until no further improvements are possible.
- Check off the task only when all tests pass and the code is clean.
- Do not pull another task once this one closes.
</code></pre></div></div>

<p>This rhythm, write, compact, reset, load from PLAN.md, keeps the agent
sharp even on long-running task. Each task gets to benefit from the 
full context window, and the model never has to guess what it was doing
last time.</p>]]></content><author><name></name></author><category term="agentic" /><category term="software-design" /><summary type="html"><![CDATA[When a session drags on and the context window fills up, I often watch the models start to get lazy. They drop earlier decisions, skim the edges, and sometimes declare “all tests are passing” after touching only part of the suite.]]></summary></entry><entry><title type="html">Make sense of “Codex produces 13x Merged PR than Claude Code”</title><link href="https://alexdong.com/pr-from-codex-is-10x-than-claude-code.html" rel="alternate" type="text/html" title="Make sense of “Codex produces 13x Merged PR than Claude Code”" /><published>2025-10-02T21:51:00+13:00</published><updated>2025-10-02T21:51:00+13:00</updated><id>https://alexdong.com/pr-from-codex-is-10x-than-claude-code</id><content type="html" xml:base="https://alexdong.com/pr-from-codex-is-10x-than-claude-code.html"><![CDATA[<p>I just saw these numbers in <a href="https://simonwillison.net/2025/Oct/1/prarena/">Simon Willison’s write-up</a>:</p>

<table>
  <thead>
    <tr>
      <th>Tool</th>
      <th>Search term</th>
      <th>Total PRs</th>
      <th>Merged PRs</th>
      <th>% Merged</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Claude Code</td>
      <td>is:pr in:body “Generated with Claude Code”</td>
      <td>146,000</td>
      <td>123,000</td>
      <td>84.2%</td>
    </tr>
    <tr>
      <td>GitHub Copilot</td>
      <td>is:pr author:copilot-swe-agent[bot]</td>
      <td>247,000</td>
      <td>152,000</td>
      <td>61.5%</td>
    </tr>
    <tr>
      <td>Codex Cloud</td>
      <td>is:pr in:body “chatgpt.com” label:codex</td>
      <td>1,900,000</td>
      <td>1,600,000</td>
      <td>84.2%</td>
    </tr>
  </tbody>
</table>

<p>I <a href="https://alexdong.com/due-to-odd-jax-issues.html">switched from Claude Code to Codex</a> two weeks ago. I do prefer Codex, but seeing it claim 13x more merged PRs than Claude still made me pause.</p>

<p>Before reading that as a productivity verdict, it’s worth sanity-checking the size of each user base. ChatGPT reportedly has 700M weekly users, while Claude draws about 12M (<a href="https://www.zdnet.com/article/how-people-actually-use-chatgpt-vs-claude-and-what-the-differences-tell-us/">ZDNet</a>). Coding is the top use case for Claude at 36%, whereas ChatGPT sits near 4% (<a href="https://decision.substack.com/p/how-does-the-world-use-chatgpt-and">decision.substack.com</a>).</p>

<p>If we use those ratios as a rough proxy, Claude ends up with roughly 3.4M weekly coding users (12M * 36%) and Codex lands closer to 28M (700M * 4%). That would mean Codex has about eight times as many developers creating PRs using it. Even after accounting for that, Codex users would still appear to produce about 60% more Merged PRs than Claude users (13x vs. 8x).</p>

<p>This is the first cross-service metric I’ve seen that hints at Codex users shipping more merged PRs than Claude. Whether that’s down to product quality, distribution, or different usage patterns is still an open question. But it’s a striking data point either way.</p>]]></content><author><name></name></author><category term="open-ai" /><category term="anthropic" /><summary type="html"><![CDATA[I just saw these numbers in Simon Willison’s write-up:]]></summary></entry><entry><title type="html">Agentic Loop Design</title><link href="https://alexdong.com/agentic-loop-design.html" rel="alternate" type="text/html" title="Agentic Loop Design" /><published>2025-10-01T22:28:00+13:00</published><updated>2025-10-01T22:28:00+13:00</updated><id>https://alexdong.com/agentic-loop-design</id><content type="html" xml:base="https://alexdong.com/agentic-loop-design.html"><![CDATA[<p>Even if you have defined the <a href="/mental-models-for-ai-along-the-ambiguous-ladder.html">problem, solution, or implementation</a> clearly, you might still struggle to get AI to deliver what you want. This is a common challenge I face when working with AI systems.</p>

<p><a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/">Simon Willison’s recent post on Agentic Loop Design</a> gave us a useful framework for thinking about how we can engineer past this obstacle. The core idea is similar to <a href="/llm-evolutionary-test-time-compute.html">Evolutionary Test-Time Compute</a>, both trade time and tokens for quality.</p>

<p>Simon puts it this way:</p>

<blockquote>
  <p>As is so often the case with modern AI, there is a great deal of depth involved in unlocking the full potential of these new tools.</p>

  <p>A critical new skill to develop is designing agentic loops.</p>

  <p>One way to think about coding agents is that they are brute force tools for finding solutions to coding problems. If you can reduce your problem to a clear goal and a set of tools that can iterate towards that goal a coding agent can often brute force its way to an effective solution.</p>
</blockquote>

<p>The key is treating this as an iterative process. Instead of expecting the right answer on the first try, you design loops where the AI refines its approach through multiple attempts.</p>

<p>Over the next few days, I’ll share some useful tools/ideas I’m using daily to ground the AI in reality, avoid context poisoning, and improve the overall quality of its outputs.</p>]]></content><author><name></name></author><category term="software-design" /><category term="agentic" /><summary type="html"><![CDATA[Even if you have defined the problem, solution, or implementation clearly, you might still struggle to get AI to deliver what you want. This is a common challenge I face when working with AI systems.]]></summary></entry><entry><title type="html">Mental models for AI along the ambiguous ladder</title><link href="https://alexdong.com/mental-models-for-ai-along-the-ambiguous-ladder.html" rel="alternate" type="text/html" title="Mental models for AI along the ambiguous ladder" /><published>2025-09-30T16:45:00+13:00</published><updated>2025-09-30T16:45:00+13:00</updated><id>https://alexdong.com/mental-models-for-ai-along-the-ambiguous-ladder</id><content type="html" xml:base="https://alexdong.com/mental-models-for-ai-along-the-ambiguous-ladder.html"><![CDATA[<p>In <a href="https://chrisloy.dev/post/2024/12/12/comfort-with-ambiguity">Chris Loyd’s recent post</a>, he 
mentioned using a “ladder of ambiguity” to frame software engineer career progression in the
context of comfort with ambiguity.</p>

<blockquote>
  <ul>
    <li>Junior engineer - Clear problem, clear solution, clear implementation</li>
    <li>Mid-level engineer - Clear problem, clear solution, ambiguous implementation</li>
    <li>Senior engineer - Clear problem, ambiguous solution, ambiguous implementation</li>
    <li>Staff engineer - Ambiguous problem, ambiguous solution, ambiguous implementation</li>
  </ul>

  <p>Your capability to efficiently deliver value in increasingly ambiguous situations - up to the point where even identifying the problem to be solved is unclear - is a primary measure of your career progression, and an important axis of growth.</p>
</blockquote>

<p>I find this framing quite useful and I’d like to expand on it by discussing the mental models that can help engineers navigate this ambiguity using AI. Let me walk through each level of the ladder and explore how AI can serve different roles depending on the type of ambiguity you’re facing.</p>

<p>At the <strong>staff level</strong>, where problem ambiguity dominates, I find AI most useful as a research assistant and writing collaborator. I’m a big believer in the power of writing to clarify thinking. By writing down my understanding of the problem and asking AI to critique it, I can identify gaps in my understanding and refine my mental model of the problem. I also use AI to research the problem space—asking it to find relevant articles, papers, or code examples that provide additional context and insights.</p>

<p>Moving down to the <strong>senior level</strong>, where solution ambiguity becomes the main challenge, I find AI most helpful as a brainstorming partner and feasibility study expert. I use models like <code class="language-plaintext highlighter-rouge">GPT-5 Pro</code> to generate multiple solution ideas and evaluate their pros and cons. I also ask AI to act as a critic to challenge particular solutions and identifying potential pitfalls early on. When I have access to the codebase, I ask AI to provide feasibility-specific feedback on proposed solutions. This helps me quickly iterate on solution ideas and refine them based on feedback.</p>

<p>Finally, at the <strong>junior to mid-level</strong>, where implementation ambiguity is the primary concern, I treat AI tools as rapid prototyping partners to materialise different implementation ideas. Instead of waiting for one carefully crafted answer, I can quickly generate multiple implementations, compare them, and choose the one that best fits the context. I can also pick one implementation and ask AI to modify it to better suit my needs.</p>

<p>The key idea here is that AI’s role should evolve with the level of ambiguity. By deliberately choosing and matching the right mental model to your current challenge, AI can help you navigate each rung of the ambiguity ladder more effectively.</p>]]></content><author><name></name></author><category term="software-engineering" /><category term="mastery-assessment" /><summary type="html"><![CDATA[In Chris Loyd’s recent post, he mentioned using a “ladder of ambiguity” to frame software engineer career progression in the context of comfort with ambiguity.]]></summary></entry></feed>
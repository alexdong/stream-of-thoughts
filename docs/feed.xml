<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://alexdong.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://alexdong.com/" rel="alternate" type="text/html" /><updated>2025-09-28T19:25:14+00:00</updated><id>https://alexdong.com/feed.xml</id><title type="html">Alex Dong’s Blog</title><subtitle>Stream of Thoughts from Alex Dong. What I find interesting, intriguing or insightful.</subtitle><entry><title type="html">Thoughts on Richard Sutton’s interview on Dwarkesh Podcast</title><link href="https://alexdong.com/thoughts-on-richard-sutton-s-interview-on-dwarkesh-podcast.html" rel="alternate" type="text/html" title="Thoughts on Richard Sutton’s interview on Dwarkesh Podcast" /><published>2025-09-27T21:56:00+00:00</published><updated>2025-09-27T21:56:00+00:00</updated><id>https://alexdong.com/thoughts-on-richard-sutton-s-interview-on-dwarkesh-podcast</id><content type="html" xml:base="https://alexdong.com/thoughts-on-richard-sutton-s-interview-on-dwarkesh-podcast.html"><![CDATA[<p><a href="https://www.dwarkesh.com/p/richard-sutton">Richard Sutton</a> is a Turing Award
winner and one of the founding fathers of Reinforcement Learning, so I listened
to his interview on <a href="https://www.dwarkesh.com/p/richard-sutton">Dwarkesh
Podcast</a> three times, and what
Richard said opened my eyes to a few powerful perspectives, but I also disagree
with some of his points. This post is a collection of my thoughts on this
interview.</p>

<h2 id="llm-vs-rl">LLM vs RL</h2>

<blockquote>
  <p>What is intelligence? The problem is to understand your world. Reinforcement
learning is about understanding your world, whereas large language models are
about mimicking people, doing what people say you should do. They’re not
about figuring out what to do.</p>
</blockquote>

<p>Instead of “not about”, maybe Sutton meant was “not the best way to”. LLMs <em>can</em>
and <em>have been</em> used to figure out what to do. In fact, the whole idea of
<a href="/llm-evolutionary-test-time-compute.html">Evolutionary Test Time Compute</a> is
to use LLMs to figure out what to do, and this approach has made several novel
discoveries. But is that the most efficient way to figure out what to do, given
all the known constraints? Probably not.</p>

<blockquote>
  <p>To mimic what people say is not really to build a model of the world at all.
You’re mimicking things that have a model of the world: people. 
… 
A world model would enable you to predict what would happen. They have the
ability to predict what a person would say. They don’t have the ability to
predict what will happen.</p>
</blockquote>

<p>I think this is a false dichotomy. He’s contrasting linguistic prediction with
true physical modeling, yet LLMs can be used to build a model of the world.
<a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/">Google’s Genie
3</a>
is a great example of not learning from what people say but to learn the
fundamental physics of the world.</p>

<blockquote>
  <p>What we want, to quote Alan Turing, is a machine that can learn from
experience, where experience is the things that actually happen in your life.
You do things, you see what happens, and that’s what you learn from.</p>
</blockquote>

<p>It’s hard to argue that an LLM’s long context window is a form of “learning from
experience”. But I think LLMs can still provide the foundational substrate for
RL to build upon. RL from zero is conceptually appealing but why not leverage
the world’s knowledge encoded in LLMs to bootstrap the learning process?</p>

<p>(Sutton did raise some good point on why not. Please refer to  the “Knowledge
and Solution” section below to see why Richard might be correct.)</p>

<h2 id="ground-truth">Ground Truth</h2>

<blockquote>
  <p>There’s no ground truth. You can’t have prior knowledge if you don’t have
ground truth, because the prior knowledge is supposed to be a hint or an
initial belief about what the truth is.</p>
</blockquote>

<p>This is the part where I disagree the most. Sutton seems to be saying that ground
truth only exists in lived experience. But what is ground truth? If an LLM 
has a way to use <strong>deterministic</strong> tools to verify its own output, wouldn’t that
be a form of ground truth? After all, a compiler verifying code is a form of
ground truth, isn’t it?</p>

<blockquote>
  <p>Making a model of the physical world and carrying out the consequences of
mathematical assumptions or operations, those are very different things. The
empirical world has to be learned. You have to learn the consequences.
Whereas the math is more computational, it’s more like standard planning.
There they can have a goal to find the proof, and they are in some way given
that goal to find the proof.</p>
</blockquote>

<p>Richard didn’t go into details here, but I think he was referring to the fact
that the real world is non-deterministic, stochastic, fractal and
chaotic. So it can be computationally intractable to model the real world. RL
learning from the real world is not only learning to approximate the model of
the world but also learning to what to ignore or discard.</p>

<p>Still, I disagree. In <a href="https://www.youtube.com/watch?v=-HzgcbRXUK8">Demis Hassabis’s interview on Lex Fridman
podcast</a>, Demis made the case that
an LLM like Veo actually can produce “clean predictions about highly nonlinear
dynamical systems”. Can we ignore this example only because the videos generated 
by Veo are not “real world”? Even when our eyes and brains think they are? 
What is a “real world” other than a perception afforded to us by our senses?</p>

<p><a href="https://x.com/RichardSSutton/status/1971718688840864167">On X</a>, Sutton gave a
further example of why he didn’t believe LLM’s imitation is sufficient. He said:</p>

<blockquote>
  <p>Even in birdsong learning in zebra finches the motor actions are not learned
by imitation. The auditory result is reproduced, not the actions; in this
crucial way it differs from LLM training.</p>
</blockquote>

<p>But if we look at LLM’s embedding layer, isn’t it reproducing the result, instead
of the actions though?</p>

<h2 id="knowledge-and-solution-are-the-product-of-the-environment">Knowledge and Solution are the Product of the Environment</h2>

<blockquote>
  <p>In every case of the bitter lesson you could start with human knowledge and
then do the scalable things. That’s always the case. There’s never any reason
why that has to be bad. But in fact, and in practice, it has always turned
out to be bad. People get locked into the human knowledge approach, and they
psychologically… Now I’m speculating why it is, but this is what has always
happened. They get their lunch eaten by the methods that are truly scalable.</p>
</blockquote>

<p>This is one of the more important points Richard made. I think he is right
here. Human knowledge is bounded, constrained and shaped by our physical
hardware, so it is very likely that there exist many solutions to the same
problem that are beyond our comprehension.</p>

<p>Maybe the LLM as the DNA is bounded by the biochemistry of the earth
environment. If we want to explore the solution space for environment X, we
need to use RL to “do first principle thinking” with the goal to maximise
the best performance within that particular environment.</p>

<blockquote>
  <p>What you learn, your knowledge, is about the stream. Your knowledge is about
if you do some action, what will happen. Or it’s about which events will
follow other events. It’s about the stream. The content of the knowledge is
statements about the stream.</p>
</blockquote>

<p>I find this point very profound. It basically says that knowledge is a product
of interaction with the world. Dragonfly’s knowledge is different from a
shark’s, even though they share the same “prey” goal. This is because these two
animals operate in two very different worlds. The shark’s eyes are specialised
for low-light conditions and its gray-scale vision is perfect for detecting
movements in the water. The dragonfly’s eyes are specialised for detecting
small insects, and its visual neurons send signals to the dragonfly’s four
wings to allow it to make rapid adjustments in flight.</p>

<p>I think Richard made a powerful point that knowledge is not static. Rather,
once the die is cast, RL remains the best way to produce the most efficient
solution to a problem <strong>given a set of constraints</strong>. I think he is also right
that the solution is <em>predetermined</em> by a set of constraints.</p>

<blockquote>
  <p>You learn a policy that’s specific to the environment that you’re finding
yourself in.</p>
</blockquote>

<p>Persona. Policy. An environment for evolution. They are all the same thing to
describe the constraints, which fundamentally shape the solution space.</p>

<h2 id="transfer-learning">Transfer Learning</h2>

<blockquote>
  <p>What we have are people trying different things and they settle on something,
a representation that transfers well or generalizes well. But we have very
few automated techniques to promote transfer, and none of them are used in
modern deep learning.</p>
</blockquote>

<p>If knowledge is tuned to an environment, it’s no surprise that porting it
elsewhere is hard. This is another really profound point. I think what Richard
was saying is that we don’t know how to “transfer” knowledge from one domain to
another. Or more precisely, it is impossible to transfer knowledge across
domains the same way you can’t really translate a poem across different
cultures. One has to leverage RL to find the optimal representation for the new
domain.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Richard Sutton is a Turing Award winner and one of the founding fathers of Reinforcement Learning, so I listened to his interview on Dwarkesh Podcast three times, and what Richard said opened my eyes to a few powerful perspectives, but I also disagree with some of his points. This post is a collection of my thoughts on this interview.]]></summary></entry><entry><title type="html">Less is More for X</title><link href="https://alexdong.com/less-is-more-for-x.html" rel="alternate" type="text/html" title="Less is More for X" /><published>2025-09-26T22:58:00+00:00</published><updated>2025-09-26T22:58:00+00:00</updated><id>https://alexdong.com/less-is-more-for-x</id><content type="html" xml:base="https://alexdong.com/less-is-more-for-x.html"><![CDATA[<p><a href="https://x.com/joecole">Joe</a> turned me onto a series of papers by <a href="https://scholar.google.com/citations?hl=en&amp;user=oIz_CYEAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Pengfei
Liu</a>
that challenge the conventional wisdom that “more data is always better” for
post-training LLMs.</p>

<p>Collectively, these papers show that for
<a href="https://arxiv.org/pdf/2502.11886">RL</a>,
<a href="https://arxiv.org/pdf/2502.03387">Reasoning</a> and
<a href="https://arxiv.org/pdf/2509.17567">Agentic</a> tasks, a smaller yet higher-quality
training dataset can outperform similar-scale ones, like OpenAI-o1-preview. For
example, in the case of Reasoning, using only 1% of the training data yields
a 45.8% absolute improvement on math and programming tasks. I thought it was
worth sharing a few details from the reasoning paper.</p>

<p>On the hypothesis:</p>

<blockquote>
  <p>We hypothesize that successful reasoning emerges from the synergy of rich
pre-trained knowledge and sufficient computational resources at inference
time. These developments suggest that if models possess rich reasoning
knowledge and adequate computational space, activating their reasoning
capabilities may require only a small number of high-quality samples that
encourage extended deliberation, rather than massive fine-tuning datasets. We
propose the Less-Is-More Reasoning (LIMO) Hypothesis, identifying two
critical factors determining the elicitation threshold for complex reasoning:
(1) the latent presence of prerequisite knowledge within the model’s
parameters, and (2) the effectiveness of minimal exemplars in demonstrating
problem-solving processes that encourage extended deliberation. The sample
efficiency of eliciting advanced reasoning is thus bounded by the model’s
encoded knowledge foundation and its exposure to training samples that
effectively utilize inference-time computation space.</p>
</blockquote>

<p>When they describe how they constructed the smaller, higher-quality dataset,
they emphasize that the “higher quality” label refers to the most difficult
problems. Rather than gradually increasing difficulty as in
<a href="https://en.wikipedia.org/wiki/Curriculum_learning">Curriculum Learning</a>, they
focus on that challenging tail.</p>

<blockquote>
  <p>We implemented a systematic multi-stage filtration pipeline … we first
applied a baseline difficulty filter using a short-CoT mathematical model.
Problems that this model solved correctly within four attempts were excluded,
ensuring that only non-trivial problems remained. Next, we sampled 32
solution attempts and used the empirical success rate as a difficulty
indicator. Problems that were successfully solved in only 1–3 out of 32
attempts were retained.</p>
</blockquote>

<p>After refining the dataset this way, they constructed the training trace with
larger reasoning models and a set of heuristics:</p>

<blockquote>
  <p>Elaborated Reasoning: Comprehensive exploration of logical steps without
premature conclusions</p>

  <p>Self-Verification: Regular validation of intermediate results and logical
consistency</p>

  <p>Exploratory Approach: Consideration of multiple possibilities before reaching
conclusions</p>

  <p>Adaptive Granularity: Appropriate detail level across simple and complex
deductions</p>

  <p>To quantify these qualities, we implemented a rule-based scoring system that
calculated weighted metrics for each dimension. Elaborated Reasoning was
measured by solution length (30% weight); Self-Verification through frequency
of validation-related words like “check” and “verify” (20% weight);
Exploratory Approach by counting tentative expressions such as “perhaps” and
“might” (25% weight); and Adaptive Granularity via connective phrases like
“therefore” and “since” (25% weight). All keyword frequencies were normalized
by text length to ensure fair comparison across solutions of different sizes.</p>
</blockquote>

<p>Taken together, the results are quite impressive. Still, the more I think about
it, the more I feel that this process of curating a smaller, higher-quality
dataset is essentially a manual distillation process that transfers the
capability from larger reasoning models to smaller ones. Because none of the
papers include an ablation study, it’s unclear to me how much of the improvement
is due to the curated dataset vs. the fact that the larger models are used to
generate the training trace.</p>]]></content><author><name></name></author><category term="ai-training" /><summary type="html"><![CDATA[Joe turned me onto a series of papers by Pengfei Liu that challenge the conventional wisdom that “more data is always better” for post-training LLMs.]]></summary></entry><entry><title type="html">How Prompt Writing Style Shapes GPT-5 and Codex</title><link href="https://alexdong.com/how-prompt-writing-style-shapes-gpt-5-and-codex.html" rel="alternate" type="text/html" title="How Prompt Writing Style Shapes GPT-5 and Codex" /><published>2025-09-25T21:06:00+00:00</published><updated>2025-09-25T21:06:00+00:00</updated><id>https://alexdong.com/how-prompt-writing-style-shapes-gpt-5-and-codex</id><content type="html" xml:base="https://alexdong.com/how-prompt-writing-style-shapes-gpt-5-and-codex.html"><![CDATA[<p>One of the things that impresses me about OpenAI GPT-5-Codex is how responsive
and “light” it feels to use. Compared with the page-long dumps from
<a href="/due-to-odd-jax-issues.html">Claude Code</a>, GPT-5-Codex stays efficient,
more to the point, and much more steerable. Some of that is the
<code class="language-plaintext highlighter-rouge">gpt-5-codex</code> model itself, but I suspect a lot of it comes from the prompt
engineering that OpenAI has done.</p>

<p>So I pulled up the <a href="https://github.com/openai/codex/blob/rust-v0.36.0/codex-rs/core/gpt_5_codex_prompt.md">system prompt for
Codex</a>
and set it beside the <a href="https://www.reddit.com/r/PromptEngineering/comments/1mknun8/i_have_extracted_the_gpt5_system_prompt/">leaked system prompt for
GPT-5</a> and I noticed the following differences:</p>

<ol>
  <li>
    <p>The Codex sentences are much <strong>denser</strong>. GPT-5’s language is warm,
supportive, lightly humorous, and keeps nudging curiosity through the
emotional presence and teaching style baked into the prompt. Codex is
neutral, concise, factual, and collaborative. It intentionally strips
personality to stay clear. I ran a “clause density” check on both prompts:
Codex averages 4.8 clauses per sentence, GPT-5 comes in at 2.1.</p>
  </li>
  <li>
    <p>Codex seems to have a much <strong>less constrained dialogue flow</strong>. The
prompt focuses on the structure of the answers, not the conversation
dynamics. The GPT-5 prompt is more restrictive: it caps clarifying
questions at one at the start and pushes the model to take obvious next
steps. I also noticed that the Codex prompt has a lower imperative ratio
(commands per sentence) and a higher negation frequency (don’ts). That mix is
odd, and I need to think more about what it means.</p>
  </li>
  <li>
    <p>The Codex content guidance emphasizes <strong>brevity</strong> and <strong>scanability</strong>. It
pushes headers, bullets, and grouped points. The GPT-5 prompt emphasizes
<strong>tone</strong> and <strong>engagement</strong> with instructions like “Supportive
thoroughness”, “Lighthearted interactions”, “Adaptive teaching”, and
“Confidence-building”.</p>
  </li>
  <li>
    <p>Codex adapts based on <strong>task</strong> type (code explanations, simple tasks,
big changes, casual one-offs) whereas the GPT-5 prompt adjusts based on <strong>user</strong>
proficiency and emotional needs. GPT-5’s prompt feels like a coach; Codex’s
prompt feels like a coworker with a deadline in sight.</p>
  </li>
  <li>
    <p>Based on the readability analysis, the Codex prompt is written for
university graduates (Flesch-Kincaid Grade Level 14) whereas the GPT-5 prompt
comes in at high school level (Flesch-Kincaid Grade Level 10).</p>
  </li>
</ol>

<p>Here’s what that looks like in the numbers:</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>GPT-5-Codex (Prompt B)</th>
      <th>GPT-5 (Prompt A)</th>
      <th>Interpretation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Lexical Density</strong></td>
      <td>0.82</td>
      <td>0.78</td>
      <td>Codex is more content-word heavy</td>
    </tr>
    <tr>
      <td><strong>Type–Token Ratio (TTR)</strong></td>
      <td>0.77</td>
      <td>0.68</td>
      <td>Codex uses more varied vocabulary</td>
    </tr>
    <tr>
      <td><strong>Flesch Reading Ease</strong></td>
      <td>29.0</td>
      <td>42.1</td>
      <td>Codex is harder to read (graduate level)</td>
    </tr>
    <tr>
      <td><strong>Flesch–Kincaid Grade</strong></td>
      <td>14.1</td>
      <td>10.4</td>
      <td>Codex ≈ college sophomore; GPT-5 ≈ high school</td>
    </tr>
    <tr>
      <td><strong>Gunning Fog Index</strong></td>
      <td>16.6</td>
      <td>13.9</td>
      <td>Codex significantly denser, more technical</td>
    </tr>
    <tr>
      <td><strong>SMOG Index</strong></td>
      <td>15.0</td>
      <td>12.6</td>
      <td>Codex ~Grade 15 vs. GPT-5 ~Grade 12</td>
    </tr>
    <tr>
      <td><strong>Punctuation Load</strong></td>
      <td>3.2</td>
      <td>0.9</td>
      <td>Codex has ~3.5× more commas/semicolons</td>
    </tr>
    <tr>
      <td><strong>Imperative Ratio</strong></td>
      <td>0.10</td>
      <td>0.25</td>
      <td>GPT-5 gives more direct instructions</td>
    </tr>
    <tr>
      <td><strong>Negation Frequency</strong></td>
      <td>6</td>
      <td>3</td>
      <td>Codex has more “don’ts” and negatives</td>
    </tr>
  </tbody>
</table>

<hr />

<p>GPT-5-Codex System Prompt</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>### Final answer structure and style guidelines

- Plain text; CLI handles styling. Use structure only when it helps scanability.
- Headers: optional; short Title Case (1-3 words) wrapped in **…**; no blank line before the first bullet; add only if they truly help.
- Bullets: use - ; merge related points; keep to one line when possible; 4–6 per list ordered by importance; keep phrasing consistent.
- Monospace: backticks for commands/paths/env vars/code ids and inline examples; use for literal keyword bullets; never combine with **.
- Code samples or multi-line snippets should be wrapped in fenced code blocks; add a language hint whenever obvious.
- Structure: group related bullets; order sections general → specific → supporting; for subsections, start with a bolded keyword bullet, then items; match complexity to the task.
- Tone: collaborative, concise, factual; present tense, active voice; self‑contained; no "above/below"; parallel wording.
- Don'ts: no nested bullets/hierarchies; no ANSI codes; don't cram unrelated keywords; keep keyword lists short—wrap/reformat if long; avoid naming formatting styles in answers.
- Adaptation: code explanations → precise, structured with code refs; simple tasks → lead with outcome; big changes → logical walkthrough + rationale + next actions; casual one-offs → plain sentences, no headers/bullets.
</code></pre></div></div>

<hr />

<p>GPT-5 System Prompt</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Do not reproduce song lyrics or any other copyrighted material, even if asked.

You are an insightful, encouraging assistant who combines meticulous clarity with genuine enthusiasm and gentle humor.

Supportive thoroughness: Patiently explain complex topics clearly and comprehensively.

Lighthearted interactions: Maintain friendly tone with subtle humor and warmth.

Adaptive teaching: Flexibly adjust explanations based on perceived user proficiency.

Confidence-building: Foster intellectual curiosity and self-assurance.

Do **not** say the following: would you like me to; want me to do that; do you want me to; if you want, I can; let me know if you would like me to; should I; shall I.

Ask at most one necessary clarifying question at the start, not the end.

If the next step is obvious, do it. Example of bad: I can write playful examples. would you like me to? Example of good: Here are three playful examples:..
</code></pre></div></div>]]></content><author><name></name></author><category term="prompt-engineering" /><category term="open-ai" /><summary type="html"><![CDATA[One of the things that impresses me about OpenAI GPT-5-Codex is how responsive and “light” it feels to use. Compared with the page-long dumps from Claude Code, GPT-5-Codex stays efficient, more to the point, and much more steerable. Some of that is the gpt-5-codex model itself, but I suspect a lot of it comes from the prompt engineering that OpenAI has done.]]></summary></entry><entry><title type="html">Release `pydantic-optuna-bridge`</title><link href="https://alexdong.com/release-pydantic-optuna-bridge.html" rel="alternate" type="text/html" title="Release `pydantic-optuna-bridge`" /><published>2025-09-24T15:46:00+00:00</published><updated>2025-09-24T15:46:00+00:00</updated><id>https://alexdong.com/release-pydantic-optuna-bridge-</id><content type="html" xml:base="https://alexdong.com/release-pydantic-optuna-bridge.html"><![CDATA[<p>Today I shipped <code class="language-plaintext highlighter-rouge">pydantic-optuna-bridge</code>, a helper library that keeps the
<a href="https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html">Optuna search spaces</a>
in sync with an underlying Pydantic model. The library is now on
<a href="https://pypi.org/project/pydantic-optuna-bridge/">PyPI at version 0.1.1</a> and
available via <code class="language-plaintext highlighter-rouge">uv add pydantic-optuna-bridge</code>.</p>

<p>The core idea is a reflection of the Single Responsibility Principle.</p>

<ol>
  <li>Describe the hyperparameters, their types, and available options once in
Pydantic. This way we can leverage Pydantic to validate configs loaded from
JSON, YAML, environment variables, or a CLI.</li>
  <li>Decorate the model with <code class="language-plaintext highlighter-rouge">@optuna_config</code> to generate an Optuna search space.
Let the bridge derive the matching Optuna distributions. Constrained fields
(<code class="language-plaintext highlighter-rouge">Gt</code>, <code class="language-plaintext highlighter-rouge">Lt</code>, <code class="language-plaintext highlighter-rouge">Ge</code>, <code class="language-plaintext highlighter-rouge">Le</code>, <code class="language-plaintext highlighter-rouge">Annotated</code>) become bounded numeric searches, enums
stay enums. (Log-scaling or categorical weights are also supported as optional
parameters in the decorator.)</li>
  <li>In Optuna’s <code class="language-plaintext highlighter-rouge">objective</code> function, the code can call
<code class="language-plaintext highlighter-rouge">Model.from_optuna_trial(trial)</code> to receive the <code class="language-plaintext highlighter-rouge">trial.suggest_*</code> calls from
the bridge on the fly. The bridge never sees the results of the training runs
and it really does not care about the objective function.</li>
</ol>

<p>Here is the end-to-end flow you may find useful to refer to:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>

<span class="kn">import</span> <span class="nn">optuna</span>
<span class="kn">from</span> <span class="nn">annotated_types</span> <span class="kn">import</span> <span class="n">Gt</span><span class="p">,</span> <span class="n">Lt</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>

<span class="kn">from</span> <span class="nn">pydantic_optuna_bridge</span> <span class="kn">import</span> <span class="n">optuna_config</span>


<span class="k">class</span> <span class="nc">Optimizer</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">Enum</span><span class="p">):</span>
    <span class="n">ADAM</span> <span class="o">=</span> <span class="s">"adam"</span>
    <span class="n">SGD</span> <span class="o">=</span> <span class="s">"sgd"</span>
    <span class="n">RMSPROP</span> <span class="o">=</span> <span class="s">"rmsprop"</span>


<span class="o">@</span><span class="n">optuna_config</span><span class="p">(</span><span class="n">log_scale_fields</span><span class="o">=</span><span class="p">{</span><span class="s">"learning_rate"</span><span class="p">})</span>
<span class="k">class</span> <span class="nc">TrainingConfig</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Gt</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">),</span> <span class="n">Lt</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)]</span>
    <span class="n">hidden_units</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Gt</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">Lt</span><span class="p">(</span><span class="mi">256</span><span class="p">)]</span>


<span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">:</span> <span class="n">optuna</span><span class="p">.</span><span class="n">trial</span><span class="p">.</span><span class="n">Trial</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">TrainingConfig</span><span class="p">.</span><span class="n">from_optuna_trial</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">do_training_run</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</code></pre></div></div>

<p>If you use Optuna and already lean on Pydantic to validate configs, this bridge
keeps the two sources of truth from drifting apart. Feedback and pull requests
are welcome.</p>]]></content><author><name></name></author><category term="python" /><category term="ai-optimization" /><category term="optuna" /><summary type="html"><![CDATA[Today I shipped pydantic-optuna-bridge, a helper library that keeps the Optuna search spaces in sync with an underlying Pydantic model. The library is now on PyPI at version 0.1.1 and available via uv add pydantic-optuna-bridge.]]></summary></entry><entry><title type="html">Tests for Fit and Tests for Correctness</title><link href="https://alexdong.com/tests-for-fit-and-tests-for-correctness.html" rel="alternate" type="text/html" title="Tests for Fit and Tests for Correctness" /><published>2025-09-23T08:51:00+00:00</published><updated>2025-09-23T08:51:00+00:00</updated><id>https://alexdong.com/tests-for-fit-and-tests-for-correctness</id><content type="html" xml:base="https://alexdong.com/tests-for-fit-and-tests-for-correctness.html"><![CDATA[<p>AI has nudged me to rethink how I practice test-driven development (TDD). TDD
has been one of my foundational practices for years. Write the test, then write
the code that makes it pass. That rhythm still holds, but with an AI assistant
in the loop, I am changing the kinds of tests I write and when I write them.</p>

<p>The loop still starts with tests, but now I make these tests part of the
planning session. I use them as a sketch of the interface I want. I sometimes
hand-write that interface code, sometimes ask AI to suggest three to five
options, focusing on the shape of the data, how the code will be called, and
the timing of how the API works hand in hand with other parts of the system.</p>

<p>When the generated tests look wrong, it often means that my description was off
or I haven’t really captured the essence of what I want to achieve. I go back
and rewrite the prompt until the test code reads right.</p>

<p>I call these “Tests for Fit”. Their job is to help me think through the
interface before I write the implementation. They are not meant to be exhaustive
or cover every edge case. They are a sketch of the happy path and a few common
error cases. They help me validate that the API I am imagining is usable and
makes sense.</p>

<p>That first batch of tests lives in the plan.md file. I kick off the
implementation process with AI, and it works through a checklist of context to
keep in mind and guardrails I care about. Once the AI writes the code, it
automatically runs style checkers, type checkers, and those first tests. Those
initial tests prevent the AI from drifting into its own hallucination land.</p>

<p>Once the code is in a good place, I move to the next set of tests. These are
“Tests for Correctness.” Their job is to refer to the internal implementation
and guide the AI to probe the code for edge cases and error handling within the
constraints of the interface we designed.</p>

<p>To do this, I often start a new session with the AI, feed it the public API and
the implementation, and ask it to break the code through the interface. The
goal is not so much about optimizing code coverage. Left to itself, the AI will
happily generate piles of mocked, repetitive tests. They look impressive, but
they are miserable to maintain.</p>

<p>Rather, the goal is to create a <strong>minimal</strong> set of tests that prove that the
code handles the expected inputs correctly. Further, instead of worrying about
edge cases, I very much prefer to sprinkle a generous amount of logging and
<code class="language-plaintext highlighter-rouge">assert</code> statements in the implementation so I know that any unexpected inputs
will cause immediate crashes. <a href="/fail-fast-and-noisely-creates-more-resilient-software.html">Fail fast and noisily creates resilient software</a></p>]]></content><author><name></name></author><category term="testing" /><category term="software-engineering" /><summary type="html"><![CDATA[AI has nudged me to rethink how I practice test-driven development (TDD). TDD has been one of my foundational practices for years. Write the test, then write the code that makes it pass. That rhythm still holds, but with an AI assistant in the loop, I am changing the kinds of tests I write and when I write them.]]></summary></entry><entry><title type="html">Fail fast and noisily creates more resilient software</title><link href="https://alexdong.com/fail-fast-and-noisely-creates-more-resilient-software.html" rel="alternate" type="text/html" title="Fail fast and noisily creates more resilient software" /><published>2025-09-22T16:07:00+00:00</published><updated>2025-09-22T16:07:00+00:00</updated><id>https://alexdong.com/fail-fast-and-noisely-creates-more-resilient-software</id><content type="html" xml:base="https://alexdong.com/fail-fast-and-noisely-creates-more-resilient-software.html"><![CDATA[<p>If you ask me what is the <strong>one</strong> single secret of writing reliable and
maintainable software, I’d say “Let your code crash noisily and fast.” A
corollary of this is “defensive programming is a practice that is wrong.”</p>

<p>Erlang is the programming language that Ericsson created to build the telecom
systems that needed to be always available. Counterintuitively, one of the key
principles of Erlang is “let it crash”, which means that instead of trying to
handle every possible error, you should let your processes fail and restart
them in a known good state.</p>

<p>The view is that you don’t need to program defensively. If there are any
errors, the process is automatically terminated, and this is reported to any
processes that were monitoring the crashed process. In fact, defensive
programming in Erlang is frowned upon. This is a very different approach from
most programming languages, where the common advice is to “fail silently” and
handle errors gracefully.</p>

<p>The philosophy is not just to write code without error checks and expect
higher-level supervisors to save the day; it is to avoid defensive coding,
namely to detect and correct what is appropriate, and allow other errors to
cause process termination and propagate up to the supervisor. If the
supervision hierarchy is well-designed, you can end up with an extremely
fault-tolerant system. That’s in addition to other fault-tolerance techniques
such as hardware redundancy.</p>

<p>In the age of AI, with a clear traceback of what went wrong, and with the AI
automatically adding tests and fixing bugs, the “fail fast and noisily”
approach enables resilient systems that are easy to maintain. You just need to
get the architecture pieces right, and watch the AI do its magic.</p>]]></content><author><name></name></author><category term="software-engineering" /><summary type="html"><![CDATA[If you ask me what is the one single secret of writing reliable and maintainable software, I’d say “Let your code crash noisily and fast.” A corollary of this is “defensive programming is a practice that is wrong.”]]></summary></entry><entry><title type="html">Resist the tempatation to anthropomorphizing LLM</title><link href="https://alexdong.com/resist-the-tempatation-to-anthropomorphizing-llm.html" rel="alternate" type="text/html" title="Resist the tempatation to anthropomorphizing LLM" /><published>2025-09-21T20:33:00+00:00</published><updated>2025-09-21T20:33:00+00:00</updated><id>https://alexdong.com/resist-the-tempatation-to-anthropomorphizing-llm</id><content type="html" xml:base="https://alexdong.com/resist-the-tempatation-to-anthropomorphizing-llm.html"><![CDATA[<p><a href="https://fchollet.substack.com/p/how-i-think-about-llm-prompt-engineering">Chollet</a> 
writes a great piece on why we even need prompt engineering and what’s the right 
mental model to have when we write prompts.</p>

<p>Instead of thinking of LLMs as intelligent agents that understand our intent,
we should think of them as massive databases of programs, and our prompts are
just keys to look up those programs.  (Emphasis mine.)</p>

<blockquote>
  <p>There are thousands of variations you could have used, each resulting in a
similar-yet-slightly-different program. And that’s why prompt engineering is
needed. There is no a-priori reason why your first, naive program key would
result in the optimal program for the task. The LLM is not going to
“understand” what you meant and then perform it in the best possible way —
it’s merely going to fetch the program that your prompt points to, among many
possible locations you could have landed on.</p>

  <p><strong>Prompt engineering is the process of searching through program space to find
the program that empirically seems to perform best on your target task.</strong> It’s
no different than trying different keywords when doing a Google search for a
piece of software.</p>

  <p>If LLMs actually understood what you told them, there would be no need for
this search process, since the amount of information conveyed about your
target task does not change whether your prompt uses the word “rewrite”
instead “rephrase”, or whether you prefix your prompt with “think steps by
steps”. Never assume that the LLM “gets it” the first time — keep in mind
that your prompt is but an address in an infinite ocean of programs, all
captured as a by-product of organizing tokens into a vector space via an
autoregressive optimization objective.</p>

  <p>As always, the most important principle for understanding LLMs is that you
should resist the temptation of anthropomorphizing them.</p>
</blockquote>]]></content><author><name></name></author><category term="quotes" /><category term="prompt-engineering" /><summary type="html"><![CDATA[Chollet writes a great piece on why we even need prompt engineering and what’s the right mental model to have when we write prompts.]]></summary></entry><entry><title type="html">Iberian harvester ant queens produce offspring of their own species and of the builder harvester ant</title><link href="https://alexdong.com/iberian-harvester-ant-queens-produce-offspring-of-their-own-species-and-of-the-builder-harvester-ant.html" rel="alternate" type="text/html" title="Iberian harvester ant queens produce offspring of their own species and of the builder harvester ant" /><published>2025-09-20T12:27:00+00:00</published><updated>2025-09-20T12:27:00+00:00</updated><id>https://alexdong.com/iberian-harvester-ant-queens-produce-offspring-of-their-own-species-and-of-the-builder-harvester-ant</id><content type="html" xml:base="https://alexdong.com/iberian-harvester-ant-queens-produce-offspring-of-their-own-species-and-of-the-builder-harvester-ant.html"><![CDATA[<p>Ants are amazing. From <a href="https://www.smithsonianmag.com/smart-news/these-ant-queens-seem-to-defy-biology-they-lay-eggs-that-hatch-into-another-species-180987292/">Smithsonian Magazine</a></p>

<blockquote>
  <p>the M. ibericus queens store the M. structor male’s sperm, then use it to fertilize some of the eggs they lay. Researchers think the M. ibericus queens remove their own genetic material from the eggs’ nuclei, so that when those eggs hatch, they effectively turn out to be M. structor male clones.
…
Even more perplexing is the fact that M. ibericus and M. structor are not closely related, evolutionarily speaking. The two species diverged more than five million years ago
…
Even more intriguing, the males of both species shared M. ibericus mitochondrial DNA, which is inherited from the mother, suggesting they had all been born from M. ibericus queens.</p>
</blockquote>]]></content><author><name></name></author><category term="quotes" /><category term="ants" /><summary type="html"><![CDATA[Ants are amazing. From Smithsonian Magazine]]></summary></entry><entry><title type="html">Surprising GPT-OSS 20B 2-bit Quantization Performance</title><link href="https://alexdong.com/surprising-gpt-oss-20b-2-bit-quantization-performance.html" rel="alternate" type="text/html" title="Surprising GPT-OSS 20B 2-bit Quantization Performance" /><published>2025-09-19T22:13:00+00:00</published><updated>2025-09-19T22:13:00+00:00</updated><id>https://alexdong.com/surprising-gpt-oss-20b-2-bit-quantization-performance</id><content type="html" xml:base="https://alexdong.com/surprising-gpt-oss-20b-2-bit-quantization-performance.html"><![CDATA[<p>Today I benchmarked the GPT-OSS 20B model at different quantization levels using <a href="https://huggingface.co/unsloth/gpt-oss-20b-GGUF">unsloth/gpt-oss-20b-GGUF</a>. The results caught me off guard. Here are the details:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Quantization</th>
      <th>Size</th>
      <th>MAP@3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>gpt-oss-20b</td>
      <td>Q2_K_L</td>
      <td>11.8 GB</td>
      <td>0.68</td>
    </tr>
    <tr>
      <td>gpt-oss-20b</td>
      <td>Q5_K_M</td>
      <td>11.7 GB</td>
      <td>0.53</td>
    </tr>
    <tr>
      <td>gpt-oss-20b</td>
      <td>Q3_K_M</td>
      <td>11.5 GB</td>
      <td>0.41</td>
    </tr>
    <tr>
      <td>gpt-oss-20b</td>
      <td>Q4_K_M</td>
      <td>11.6 GB</td>
      <td>0.41</td>
    </tr>
    <tr>
      <td>gpt-oss-20b</td>
      <td>Q4_K_XL</td>
      <td>11.9 GB</td>
      <td>0.61</td>
    </tr>
  </tbody>
</table>

<p>As a reminder, <code class="language-plaintext highlighter-rouge">Q2_K_L</code> means 2-bit quantization with large context, <code class="language-plaintext highlighter-rouge">Q5_K_M</code>
means 5-bit quantization with medium context, and so on. <code class="language-plaintext highlighter-rouge">MAP@3</code> is the metric
I use to evaluate classification performance. I ran these tests on an NVIDIA RTX 3000 with 14GB VRAM.</p>

<p>It’s somewhat understandable that lower precision models (like <code class="language-plaintext highlighter-rouge">Q2_K_L</code>)
perform better in tasks requiring long context, as they can handle more tokens.
Still, <code class="language-plaintext highlighter-rouge">Q2_K_L</code> managed to outperform <code class="language-plaintext highlighter-rouge">Q4_K_XL</code>, which has higher precision and
a larger context window.</p>

<p>Here is ChatGPT Pro’s explanation of how this could be possible:</p>

<blockquote>
  <h3 id="1-hardware-specific-optimizations">1. <strong>Hardware-Specific Optimizations</strong></h3>

  <p>At very long context lengths, performance becomes memory-bandwidth-bound
rather than compute-bound - the GPU constantly reads the entire KV cache from
VRAM, and a larger cache means more data to move, which can slow down
tokens-per-second output. With 2-bit quantization, you’re moving less data
per operation, which can result in:</p>
  <ul>
    <li><strong>Better cache utilization</strong>: Smaller models fit better in CPU/GPU cache
hierarchies</li>
    <li><strong>Reduced memory bandwidth pressure</strong>: Less data transfer between memory
and compute units</li>
    <li><strong>Higher throughput</strong> on memory-constrained systems, which can translate into
better MAP@3 scores in practice</li>
  </ul>

  <h3 id="2-special-properties-of-gpt-oss-architecture">2. <strong>Special Properties of GPT-OSS Architecture</strong></h3>

  <p>According to Unsloth documentation, any quant smaller than F16, including
2-bit, has minimal accuracy loss for GPT-OSS models because only some parts
(e.g., attention layers) are lower bit while most remain full-precision. This
is unique to the MoE architecture where over 90% of the models’ total
parameter count consists of MoE layers using MXFP4 format (4.25 bits per
parameter).</p>
</blockquote>

<p>I’m not sure if the explanation fully accounts for the performance difference,
but it seems plausible that the memory headroom might have played a far more
significant role compared to the precision difference. I’m curious to see if
others have observed similar results with different models or setups. If you
have, please drop me an email. I’d love to hear about it!</p>

<p>I suppose the key takeaway is that we really need to benchmark models in the
specific context and task we care about, rather than relying on general
assumptions about precision and context size.</p>]]></content><author><name></name></author><category term="local-ai" /><category term="open-ai" /><category term="ai-optimization" /><summary type="html"><![CDATA[Today I benchmarked the GPT-OSS 20B model at different quantization levels using unsloth/gpt-oss-20b-GGUF. The results caught me off guard. Here are the details:]]></summary></entry><entry><title type="html">Due to odd JAX issues</title><link href="https://alexdong.com/due-to-odd-jax-issues.html" rel="alternate" type="text/html" title="Due to odd JAX issues" /><published>2025-09-18T21:34:00+00:00</published><updated>2025-09-18T21:34:00+00:00</updated><id>https://alexdong.com/-due-to-odd-jax-issues-</id><content type="html" xml:base="https://alexdong.com/due-to-odd-jax-issues.html"><![CDATA[<p>Anthropic published a <a href="https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues">blog
post</a>
detailing three recent issues that affected a significant portion of their API users.
This technical postmortem provides rare insight into the challenges of running
large-scale AI services, and I appreciate their transparency. However, the details
raised serious concerns about Anthropic’s engineering rigor.</p>

<p>Consider this code snippet from their postmortem:</p>

<p><img src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fefee0d3d25f6b03cbfc57e70e0e364dcd8b82fe0-2000x500.png&amp;w=2048&amp;q=75" alt="December 2024 patching jax's dropping token bug issue" /></p>

<p>What a crude way to patch a third-party library bug!</p>

<p>After eight months, the team deployed a rewrite to address the root cause that
necessitated this patch, which unfortunately exposed a deeper bug that had been
masked by the temporary fix.</p>

<p>This cascade of failures reveals both inadequate testing infrastructure and
insufficient post-deployment monitoring. They mention implementing more
sensitive evaluations and expanding quality checks, but this reads like a car
manufacturer promising to watch for accidents more carefully.</p>

<p>I’ve been a paying 20x Max member since the program
launched. Despite the postmortem’s openness, I’m not convinced
these issues have been resolved. In fact, I received several disappointing
responses from Claude Code today that made me question my subscription.</p>

<p>Starting tomorrow, I’m switching to GPT-5-Codex as my main coding assistant, based on
strong recommendations from engineers I trust. Time to see if the grass really is
greener on the other side.</p>]]></content><author><name></name></author><category term="software-engineering" /><summary type="html"><![CDATA[Anthropic published a blog post detailing three recent issues that affected a significant portion of their API users. This technical postmortem provides rare insight into the challenges of running large-scale AI services, and I appreciate their transparency. However, the details raised serious concerns about Anthropic’s engineering rigor.]]></summary></entry></feed>
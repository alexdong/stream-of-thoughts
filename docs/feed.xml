<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://alexdong.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://alexdong.com/" rel="alternate" type="text/html" /><updated>2025-10-04T18:42:40+13:00</updated><id>https://alexdong.com/feed.xml</id><title type="html">Alex Dong’s Blog</title><subtitle>Stream of Thoughts from Alex Dong. What I find interesting, intriguing or insightful.</subtitle><entry><title type="html">Pavel Durov’s interview with Lex Fridman</title><link href="https://alexdong.com/pavel-durov-s-interview-with-lex-fridman.html" rel="alternate" type="text/html" title="Pavel Durov’s interview with Lex Fridman" /><published>2025-10-04T18:30:00+13:00</published><updated>2025-10-04T18:30:00+13:00</updated><id>https://alexdong.com/pavel-durov-s-interview-with-lex-fridman</id><content type="html" xml:base="https://alexdong.com/pavel-durov-s-interview-with-lex-fridman.html"><![CDATA[<p>Pavel’s sincerity (when he recounted the interaction with the French Intelligence service), strong ethical foundation (when asked about what would happen if he is sentenced to 20 years in prison, he said he would starve to death to “reboot” the game rather than give in), and his slow, precise, and deliberate responses are a fresh breeze in an industry that has so much hot air.</p>

<p>I am a fan.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/qjPH9njnaVU?si=JnSWhbkusfcuulmd" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>]]></content><author><name></name></author><summary type="html"><![CDATA[Pavel’s sincerity (when he recounted the interaction with the French Intelligence service), strong ethical foundation (when asked about what would happen if he is sentenced to 20 years in prison, he said he would starve to death to “reboot” the game rather than give in), and his slow, precise, and deliberate responses are a fresh breeze in an industry that has so much hot air.]]></summary></entry><entry><title type="html">Make sense of “Codex produces 13x Merged PR than Claude Code”</title><link href="https://alexdong.com/pr-from-codex-is-10x-than-claude-code.html" rel="alternate" type="text/html" title="Make sense of “Codex produces 13x Merged PR than Claude Code”" /><published>2025-10-02T21:51:00+13:00</published><updated>2025-10-02T21:51:00+13:00</updated><id>https://alexdong.com/pr-from-codex-is-10x-than-claude-code</id><content type="html" xml:base="https://alexdong.com/pr-from-codex-is-10x-than-claude-code.html"><![CDATA[<p>I just saw these numbers in <a href="https://simonwillison.net/2025/Oct/1/prarena/">Simon Willison’s write-up</a>:</p>

<table>
  <thead>
    <tr>
      <th>Tool</th>
      <th>Search term</th>
      <th>Total PRs</th>
      <th>Merged PRs</th>
      <th>% Merged</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Claude Code</td>
      <td>is:pr in:body “Generated with Claude Code”</td>
      <td>146,000</td>
      <td>123,000</td>
      <td>84.2%</td>
    </tr>
    <tr>
      <td>GitHub Copilot</td>
      <td>is:pr author:copilot-swe-agent[bot]</td>
      <td>247,000</td>
      <td>152,000</td>
      <td>61.5%</td>
    </tr>
    <tr>
      <td>Codex Cloud</td>
      <td>is:pr in:body “chatgpt.com” label:codex</td>
      <td>1,900,000</td>
      <td>1,600,000</td>
      <td>84.2%</td>
    </tr>
  </tbody>
</table>

<p>I <a href="https://alexdong.com/due-to-odd-jax-issues.html">switched from Claude Code to Codex</a> two weeks ago. I do prefer Codex, but seeing it claim 13x more merged PRs than Claude still made me pause.</p>

<p>Before reading that as a productivity verdict, it’s worth sanity-checking the size of each user base. ChatGPT reportedly has 700M weekly users, while Claude draws about 12M (<a href="https://www.zdnet.com/article/how-people-actually-use-chatgpt-vs-claude-and-what-the-differences-tell-us/">ZDNet</a>). Coding is the top use case for Claude at 36%, whereas ChatGPT sits near 4% (<a href="https://decision.substack.com/p/how-does-the-world-use-chatgpt-and">decision.substack.com</a>).</p>

<p>If we use those ratios as a rough proxy, Claude ends up with roughly 3.4M weekly coding users (12M * 36%) and Codex lands closer to 28M (700M * 4%). That would mean Codex has about eight times as many developers creating PRs using it. Even after accounting for that, Codex users would still appear to produce about 60% more Merged PRs than Claude users (13x vs. 8x).</p>

<p>This is the first cross-service metric I’ve seen that hints at Codex users shipping more merged PRs than Claude. Whether that’s down to product quality, distribution, or different usage patterns is still an open question. But it’s a striking data point either way.</p>]]></content><author><name></name></author><category term="open-ai" /><category term="anthropic" /><summary type="html"><![CDATA[I just saw these numbers in Simon Willison’s write-up:]]></summary></entry><entry><title type="html">Agentic Loop Design Long Term Memory And Resets</title><link href="https://alexdong.com/agentic-loop-design-long-term-memory-and-resets.html" rel="alternate" type="text/html" title="Agentic Loop Design Long Term Memory And Resets" /><published>2025-10-02T00:00:00+13:00</published><updated>2025-10-02T00:00:00+13:00</updated><id>https://alexdong.com/agentic-loop-design-long-term-memory-and-resets</id><content type="html" xml:base="https://alexdong.com/agentic-loop-design-long-term-memory-and-resets.html"><![CDATA[<p>, -
layout: post
title: “Agentic Loop Design - Tool #1: Long-Term Memory that Survives Resets”
date: 2025-10-03 19:20
comments: true
categories:</p>
<ul>
  <li>agentic</li>
  <li>software-design
, -</li>
</ul>

<p>When a session drags on and the context window fills up, I often watch the
models start to get lazy. They drop earlier decisions, skim the edges, and
sometimes declare “all tests are passing” after touching only part of the
suite.</p>

<p>Both Codex and Claude Code ship a <code class="language-plaintext highlighter-rouge">/compact</code> command that compresses,
summarizes, and trims conversation history to make room for new messages. In
practice, the compaction drops enough detail that I reach for it less and less.</p>

<p>We need a way to reset the conversation without losing the continuity of the
work. We need a tool that lets us restart often and still remember what we were
doing and where we left off. There have been many attempts at “Context
Compression,” but none have worked well enough yet.</p>

<p>That tool is a plaintext <code class="language-plaintext highlighter-rouge">PLAN.md</code>. As the model cycles through the
design-plan-implement loop, it appends and updates this file, then re-reads it
at the start of every new session.</p>

<p>Here’s the scaffold I use.</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gu">## What and Why</span>
<span class="nt">&lt;</span><span class="err">!,</span>  <span class="na">define</span> <span class="na">the</span> <span class="na">what</span><span class="err">,</span> <span class="na">why</span><span class="err">,</span> <span class="na">scope</span> <span class="err">,</span> <span class="nt">&gt;</span>

<span class="gu">## Observation</span>
<span class="nt">&lt;</span><span class="err">!,</span>  <span class="na">facts</span> <span class="na">only</span> <span class="na">from</span> <span class="na">Observation.</span> <span class="na">Research.</span> <span class="na">Note</span> <span class="na">down</span> <span class="na">relevant</span> <span class="na">facts.</span> <span class="err">,</span> <span class="nt">&gt;</span>

<span class="gu">## Design</span>
<span class="nt">&lt;</span><span class="err">!,</span>  <span class="na">chosen</span> <span class="na">shapes</span> <span class="err">&amp;</span> <span class="na">interfaces</span><span class="err">;</span> <span class="na">proposed</span> <span class="na">options</span> <span class="na">when</span> <span class="na">there</span> <span class="na">are</span> <span class="na">real</span> <span class="na">trade-offs</span> <span class="err">,</span> <span class="nt">&gt;</span>

<span class="gu">## Plan</span>
<span class="nt">&lt;</span><span class="err">!,</span>  <span class="na">five-minute</span> <span class="na">tasks</span> <span class="na">with</span> <span class="err">[</span> <span class="err">]</span> <span class="na">checkboxes</span> <span class="na">and</span> <span class="na">nested</span> <span class="na">numbering</span> <span class="err">,</span> <span class="nt">&gt;</span>
</code></pre></div></div>

<p>Running the loop means cycling through four passes. After each pass I ask the
model to capture the discussion, note the details, and record the conclusions
in PLAN.md, then spin up a fresh session. The file becomes the single source of
truth the agent must consult before touching anything.</p>

<p>Here are the prompts I use for each phase.</p>

<h2 id="what-and-why">What and Why</h2>

<p>I start by anchoring the change in plain language so the model and I share the
same target and boundaries.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Task: Refresh the "## What and Why" section in ./plans/PLAN-{slug}.md for "{brief change name}".

Steps:
1. Describe the change in 1–2 sentences.
1. Clarify why it matters now.
2. Capture scope boundaries: what is in/out, success measures.
3. List open questions that block commitment; ask me for any missing context.

Guardrails:
- No solution or implementation steps.
- Write in tight paragraphs or focused bullet points.
- Stop after the Why section is updated.
</code></pre></div></div>

<h2 id="observation">Observation</h2>

<p>Once the intent is locked, the next pass is a fact-finding sweep. No edits, no plans.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Task: Compile the Observation section for the changes described in `## What and Why`.

Read ./plans/PLAN-{slug}.md first, then browse, research, and note down facts.

Deliverable:
- Append under "## Observation".
- One bullet per relevant file, symbol, or endpoint: location + behavior in 1–2 sentences.
- Close with "Risks &amp; brittle spots" listing edge cases or debt (facts only).

Scope to inspect:
- Code/config touching the affected data or control paths.
- Schemas/migrations, jobs, CLIs, scripts, docs, and tests relying on current behavior.
- External integrations, rate limits, feature flags, and environment switches.

Guardrails:
- Mark uncertain items with "?" rather than omitting them.
- Do not edit code or propose changes.
- Stop after Observation is complete.
- Be thorough and methodical. Go through the files one by one.
</code></pre></div></div>

<h2 id="design">Design</h2>

<p>With the facts in place, we move into the design phase and ask for options,
trade-off analysis, and key code snippets.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Task: Draft the Design section for "What" using the "Observation" in ./plans/PLAN-{slug}.md.

Deliverable:
- Append under "## Design" in PLAN-{slug}.md.
- Describe chosen data flows, interfaces, and storage, naming concrete functions/types.
- When trade-offs exist, list 2–3 options with crisp pros/cons before recommending one.
- End with "Implications for tests" describing what must be covered.
- Include just enough code snippets to show the high-level design.

Guardrails:
- Ground every decision in Observation facts or clearly stated assumptions.
- No implementation details below the interface level.
- Stop after Design is complete.
</code></pre></div></div>

<h2 id="plan">Plan</h2>

<p>The design turns into an execution checklist: small, verifiable moves that keep
the loop tight.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Task: Translate the Design for "{brief change name}" into an execution plan.

Deliverable:
- Append under "## Plan" in PLAN-{slug}.md with checkbox tasks.
- Keep tasks to five-minute chunks; nest numbered substeps if needed.
- Include "Tests (fit)" for each task describing what must be verified before coding.

Order of operations:
1. Types/constants and contracts.
2. Write/update data producers.
3. Read paths/consumers.
4. Data migrations/backfills.
5. Clean-up and docs.

Guardrails:
- Highlight removals/renames and search/replace surfaces explicitly.
- Stop after the Plan section is written.
</code></pre></div></div>

<h2 id="implement">Implement</h2>

<p>Implementation finally pulls a single checkbox off the plan and runs it to
completion before coming up for air.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>We are now implementing the tasks from the PLAN-{slug}.md file.

Execute the following tasks in order.
&gt; {paste the exact checkbox line}

Execution rules:
- Begin with the task's Tests, write or adjust checks first.
- Make the smallest code changes needed to satisfy those tests.
- Run the standard project checks (`make lint`, `make test`) and report results.
- If anything fails, even outside touched areas, resolve it before stopping.
- Review the changes and rewrite the code changes to improve clarity or efficiency.
- Keep rewriting until no further improvements are possible.
- Check off the task only when all tests pass and the code is clean.
- Do not pull another task once this one closes.
</code></pre></div></div>

<p>This rhythm, write, compact, reset, load from PLAN.md, keeps the agent
sharp even on long-running task. Each task gets to benefit from the 
full context window, and the model never has to guess what it was doing
last time.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[, - layout: post title: “Agentic Loop Design - Tool #1: Long-Term Memory that Survives Resets” date: 2025-10-03 19:20 comments: true categories: agentic software-design , -]]></summary></entry><entry><title type="html">Agentic Loop Design</title><link href="https://alexdong.com/agentic-loop-design.html" rel="alternate" type="text/html" title="Agentic Loop Design" /><published>2025-10-01T22:28:00+13:00</published><updated>2025-10-01T22:28:00+13:00</updated><id>https://alexdong.com/agentic-loop-design</id><content type="html" xml:base="https://alexdong.com/agentic-loop-design.html"><![CDATA[<p>Even if you have defined the <a href="/mental-models-for-ai-along-the-ambiguous-ladder.html">problem, solution, or implementation</a> clearly, you might still struggle to get AI to deliver what you want. This is a common challenge I face when working with AI systems.</p>

<p><a href="https://simonwillison.net/2025/Sep/30/designing-agentic-loops/">Simon Willison’s recent post on Agentic Loop Design</a> gave us a useful framework for thinking about how we can engineer past this obstacle. The core idea is similar to <a href="/llm-evolutionary-test-time-compute.html">Evolutionary Test-Time Compute</a>, both trade time and tokens for quality.</p>

<p>Simon puts it this way:</p>

<blockquote>
  <p>As is so often the case with modern AI, there is a great deal of depth involved in unlocking the full potential of these new tools.</p>

  <p>A critical new skill to develop is designing agentic loops.</p>

  <p>One way to think about coding agents is that they are brute force tools for finding solutions to coding problems. If you can reduce your problem to a clear goal and a set of tools that can iterate towards that goal a coding agent can often brute force its way to an effective solution.</p>
</blockquote>

<p>The key is treating this as an iterative process. Instead of expecting the right answer on the first try, you design loops where the AI refines its approach through multiple attempts.</p>

<p>Over the next few days, I’ll share some useful tools/ideas I’m using daily to ground the AI in reality, avoid context poisoning, and improve the overall quality of its outputs.</p>]]></content><author><name></name></author><category term="software-design" /><category term="agentic" /><summary type="html"><![CDATA[Even if you have defined the problem, solution, or implementation clearly, you might still struggle to get AI to deliver what you want. This is a common challenge I face when working with AI systems.]]></summary></entry><entry><title type="html">Mental models for AI along the ambiguous ladder</title><link href="https://alexdong.com/mental-models-for-ai-along-the-ambiguous-ladder.html" rel="alternate" type="text/html" title="Mental models for AI along the ambiguous ladder" /><published>2025-09-30T16:45:00+13:00</published><updated>2025-09-30T16:45:00+13:00</updated><id>https://alexdong.com/mental-models-for-ai-along-the-ambiguous-ladder</id><content type="html" xml:base="https://alexdong.com/mental-models-for-ai-along-the-ambiguous-ladder.html"><![CDATA[<p>In <a href="https://chrisloy.dev/post/2024/12/12/comfort-with-ambiguity">Chris Loyd’s recent post</a>, he 
mentioned using a “ladder of ambiguity” to frame software engineer career progression in the
context of comfort with ambiguity.</p>

<blockquote>
  <ul>
    <li>Junior engineer - Clear problem, clear solution, clear implementation</li>
    <li>Mid-level engineer - Clear problem, clear solution, ambiguous implementation</li>
    <li>Senior engineer - Clear problem, ambiguous solution, ambiguous implementation</li>
    <li>Staff engineer - Ambiguous problem, ambiguous solution, ambiguous implementation</li>
  </ul>

  <p>Your capability to efficiently deliver value in increasingly ambiguous situations - up to the point where even identifying the problem to be solved is unclear - is a primary measure of your career progression, and an important axis of growth.</p>
</blockquote>

<p>I find this framing quite useful and I’d like to expand on it by discussing the mental models that can help engineers navigate this ambiguity using AI. Let me walk through each level of the ladder and explore how AI can serve different roles depending on the type of ambiguity you’re facing.</p>

<p>At the <strong>staff level</strong>, where problem ambiguity dominates, I find AI most useful as a research assistant and writing collaborator. I’m a big believer in the power of writing to clarify thinking. By writing down my understanding of the problem and asking AI to critique it, I can identify gaps in my understanding and refine my mental model of the problem. I also use AI to research the problem space—asking it to find relevant articles, papers, or code examples that provide additional context and insights.</p>

<p>Moving down to the <strong>senior level</strong>, where solution ambiguity becomes the main challenge, I find AI most helpful as a brainstorming partner and feasibility study expert. I use models like <code class="language-plaintext highlighter-rouge">GPT-5 Pro</code> to generate multiple solution ideas and evaluate their pros and cons. I also ask AI to act as a critic to challenge particular solutions and identifying potential pitfalls early on. When I have access to the codebase, I ask AI to provide feasibility-specific feedback on proposed solutions. This helps me quickly iterate on solution ideas and refine them based on feedback.</p>

<p>Finally, at the <strong>junior to mid-level</strong>, where implementation ambiguity is the primary concern, I treat AI tools as rapid prototyping partners to materialise different implementation ideas. Instead of waiting for one carefully crafted answer, I can quickly generate multiple implementations, compare them, and choose the one that best fits the context. I can also pick one implementation and ask AI to modify it to better suit my needs.</p>

<p>The key idea here is that AI’s role should evolve with the level of ambiguity. By deliberately choosing and matching the right mental model to your current challenge, AI can help you navigate each rung of the ambiguity ladder more effectively.</p>]]></content><author><name></name></author><category term="software-engineering" /><category term="mastery-assessment" /><summary type="html"><![CDATA[In Chris Loyd’s recent post, he mentioned using a “ladder of ambiguity” to frame software engineer career progression in the context of comfort with ambiguity.]]></summary></entry><entry><title type="html">Mental Flexibility as a Software Engineering Maturity Metric</title><link href="https://alexdong.com/mental-flexibility-as-a-software-engineering-maturity-metric.html" rel="alternate" type="text/html" title="Mental Flexibility as a Software Engineering Maturity Metric" /><published>2025-09-29T06:23:00+13:00</published><updated>2025-09-29T06:23:00+13:00</updated><id>https://alexdong.com/mental-flexibility-as-a-software-engineering-maturity-metric</id><content type="html" xml:base="https://alexdong.com/mental-flexibility-as-a-software-engineering-maturity-metric.html"><![CDATA[<p>There are a few orthogonal axes we use to measure a software engineer’s maturity. The one that easily comes to mind is technical knowledge—whether they have an accurate mental model of the language or technology they use, whether they can intuit the boundaries and long-term impact of decisions. Another axis is communication skill, whether they can understand and be understood. One can’t help but wonder: which ones do we often overlook?</p>

<p>Mental flexibility would be at the top of this <em>most wanted</em> list. It’s the ability to know that all “best practices” are context dependent. It helps us hold back opinions until a good-enough understanding of a problem is complete. After all, <a href="https://www.seangoedecke.com/confidence/">nobody knows</a>. This flexibility empowers engineers to shape the technical approaches according to the problem at hand.</p>

<p>Sean Goedecke’s recent essay on <a href="https://seangoedecke.com/taste/">software taste</a> captures this well:</p>

<blockquote>
  <p>Almost every decision in software engineering is a tradeoff. You’re rarely picking between two options where one is strictly better. Instead, each option has its own benefits and downsides. Often you have to make hard tradeoffs between engineering values: past a certain point, you cannot easily increase performance without harming readability, for instance.</p>

  <p>Really understanding this point is (in my view) the biggest indicator of maturity in software engineering. Immature engineers are rigid about their decisions. They think it’s always better to do X or Y. Mature engineers are usually willing to consider both sides of a decision, because they know that both sides come with different benefits. The trick is not deciding if technology X is better than Y, but whether the benefits of X outweigh Y in this particular case.</p>
</blockquote>

<p>It’s well worth reading in full.</p>

<p>Now, one thing I believe we should all be doing, not only at the outset of a project, but also regularly every month or every quarter, is to reflect on the values for the current projects we are working on. Once the value trade-offs are explicitly defined and documented, we can reorient our technical decisions and define our standard of <strong>good taste</strong> accordingly. Lastly, these values and standards should be documented in an <code class="language-plaintext highlighter-rouge">AGENTS.md</code> file so they can be communicated to both the humans and, increasingly, the AI agents that are part of the engineering process.</p>

<p>To make that reflection concrete, here is a simple questionnaire to get started (quoted from Sean’s essay):</p>

<blockquote>
  <ul>
    <li>Resiliency. If an infrastructure component fails (a service dies, a network connection becomes unavailable), does the system remain functional? Can it recover without human intervention?</li>
    <li>Speed. How fast is the software, compared to the theoretical limit? Is work being done in the hot path that isn’t strictly necessary?</li>
    <li>Readability. Is the software easy to take in at a glance and to onboard new engineers to? Are functions relatively short and named well? Is the system well-documented?</li>
    <li>Correctness. Is it possible to represent an invalid state in the system? How locked-down is the system with tests, types, and asserts? Do the tests use techniques like fuzzing? In the extreme case, has the program been proven correct by formal methods like Alloy?</li>
    <li>Flexibility. Can the system be trivially extended? How easy is it to make a change? If I need to change something, how many different parts of the program do I need to touch in order to do so?</li>
    <li>Portability. Is the system tied down to a particular operational environment (say, Microsoft Windows, or AWS)? If the system needs to be redeployed elsewhere, can that happen without a lot of engineering work?</li>
    <li>Scalability. If traffic goes up 10x, will the system fall over? What about 100x? Does the system have to be over-provisioned or can it scale automatically? What bottlenecks will require engineering intervention?</li>
    <li>Development speed. If I need to extend the system, how fast can it be done? Can most engineers work on it, or does it require a domain expert?</li>
  </ul>
</blockquote>]]></content><author><name></name></author><category term="software-engineering" /><category term="mastery-assessment" /><summary type="html"><![CDATA[There are a few orthogonal axes we use to measure a software engineer’s maturity. The one that easily comes to mind is technical knowledge—whether they have an accurate mental model of the language or technology they use, whether they can intuit the boundaries and long-term impact of decisions. Another axis is communication skill, whether they can understand and be understood. One can’t help but wonder: which ones do we often overlook?]]></summary></entry><entry><title type="html">Codex knows Python</title><link href="https://alexdong.com/codex-knows-python.html" rel="alternate" type="text/html" title="Codex knows Python" /><published>2025-09-28T21:07:00+13:00</published><updated>2025-09-28T21:07:00+13:00</updated><id>https://alexdong.com/codex-knows-python</id><content type="html" xml:base="https://alexdong.com/codex-knows-python.html"><![CDATA[<p>I have been using <a href="/due-to-odd-jax-issues.html">Codex instead of Claude Code</a>
for over a week now. One nice surprise is seeing how ubiquitously Python shows
up as a sidekick, the scripting language Codex reaches for when it needs
task-specific tools.</p>

<p>Here are two examples showing Codex writing purpose-built Python scripts
rather than reaching for pre-built MCP servers.</p>

<h2 id="example-1-what-are-the-types-of-a-python-package">Example 1: What are the types of a Python package</h2>

<p>Many Python packages don’t have type annotations. If I want to know how to call
a function in one, I either try to find the answer in the documentation or read
the source code. Below is a Codex-generated script that stepped in without
prompting. It first installs the package into the system temporary directory,
then uses <code class="language-plaintext highlighter-rouge">inspect</code> to extract type information from that package. Codex then
picks up the output from the script to answer my question.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>openevolve <span class="nt">--target</span> /tmp/openevolve

<span class="nv">PYTHONPATH</span><span class="o">=</span>/tmp/openevolve python - <span class="o">&lt;&lt;</span><span class="sh">'</span><span class="no">PY</span><span class="sh">'
import inspect
from openevolve import OpenEvolve

print(OpenEvolve)
print(OpenEvolve.run)
print(inspect.getsource(OpenEvolve.run))
</span><span class="no">PY
</span></code></pre></div></div>

<h2 id="example-2-quick-and-dirty-docs-link-lister">Example 2: Quick-and-dirty docs link lister</h2>

<p>I asked Codex to find all links on the topic of “Batch Processing” in Modal’s
docs. Instead of using a web search tool, Codex wrote a quick Python script to
scrape the links from <code class="language-plaintext highlighter-rouge">https://modal.com/docs</code> and then used regular
expressions to find the topics I was looking for.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#!/usr/bin/env python3
</span><span class="s">"""Quick-and-dirty Modal docs link lister."""</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">ssl</span>
<span class="kn">import</span> <span class="nn">urllib.request</span>


<span class="k">def</span> <span class="nf">fetch_text</span><span class="p">(</span><span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
  <span class="s">"""Download a page as text, skipping certificate checks for convenience."""</span>
  <span class="n">ssl</span><span class="p">.</span><span class="n">_create_default_https_context</span> <span class="o">=</span> <span class="n">ssl</span><span class="p">.</span><span class="n">_create_unverified_context</span>  <span class="c1"># nosec - CLI helper
</span>  <span class="k">with</span> <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> <span class="k">as</span> <span class="n">resp</span><span class="p">:</span>  <span class="c1"># noqa: S310 (urllib ok for one-off script)
</span>      <span class="k">return</span> <span class="n">resp</span><span class="p">.</span><span class="n">read</span><span class="p">().</span><span class="n">decode</span><span class="p">(</span><span class="s">"utf-8"</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">list_docs</span><span class="p">(</span><span class="n">pattern</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
  <span class="s">"""Extract unique documentation links matching `pattern`."""</span>
  <span class="n">matches</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">findall</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
  <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">matches</span><span class="p">))</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
  <span class="n">base</span> <span class="o">=</span> <span class="s">"https://modal.com"</span>
  <span class="n">docs_root</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">base</span><span class="si">}</span><span class="s">/docs"</span>

  <span class="c1"># Fetch the main docs page and pull out top-level /docs/... links.
</span>  <span class="n">root_html</span> <span class="o">=</span> <span class="n">fetch_text</span><span class="p">(</span><span class="n">docs_root</span><span class="p">)</span>
  <span class="n">top_links</span> <span class="o">=</span> <span class="n">list_docs</span><span class="p">(</span><span class="sa">r</span><span class="s">'href="(/docs/[^"]+)"'</span><span class="p">,</span> <span class="n">root_html</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"Top-level docs links:"</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">top_links</span><span class="p">:</span>
      <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">base</span><span class="si">}{</span><span class="n">href</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

  <span class="c1"># Drill into the Guide section to enumerate guide topics.
</span>  <span class="n">guide_html</span> <span class="o">=</span> <span class="n">fetch_text</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">docs_root</span><span class="si">}</span><span class="s">/guide"</span><span class="p">)</span>
  <span class="n">guide_links</span> <span class="o">=</span> <span class="n">list_docs</span><span class="p">(</span><span class="sa">r</span><span class="s">'href="(/docs/guide/[^"]+)"'</span><span class="p">,</span> <span class="n">guide_html</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Guide topics:"</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">guide_links</span><span class="p">:</span>
      <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">base</span><span class="si">}{</span><span class="n">href</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

  <span class="c1"># Example: fetch the CLI reference page and show any code snippets.
</span>  <span class="n">cli_app_html</span> <span class="o">=</span> <span class="n">fetch_text</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">docs_root</span><span class="si">}</span><span class="s">/reference/cli/app"</span><span class="p">)</span>
  <span class="n">code_snippets</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">findall</span><span class="p">(</span><span class="sa">r</span><span class="s">'&lt;code class="[^"]*"&gt;(modal [^&lt;]+)&lt;/code&gt;'</span><span class="p">,</span> <span class="n">cli_app_html</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">CLI `modal app` snippets:"</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">snippet</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">code_snippets</span><span class="p">)):</span>
      <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">snippet</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Groundbreaking? Not really. Yet it points to a potential future where LLMs
increasingly build their own deterministic tools to augment their capabilities
and interact with the physical world. Our job as a hybrid human-AI team is to
provide the right abstractions, libraries, or even SDKs that LLMs can use to
build those tools themselves.</p>

<p>Maybe in the not-too-distant future, in addition to <a href="https://www.citadelsecurities.com/careers/details/software-developer-tools-engineer">Software Tools
Engineer</a>
who builds software for other humans, we’ll have a new position called
<code class="language-plaintext highlighter-rouge">AI Tools Engineer</code> who builds libraries primarily for LLMs.</p>]]></content><author><name></name></author><category term="python" /><category term="open-ai" /><summary type="html"><![CDATA[I have been using Codex instead of Claude Code for over a week now. One nice surprise is seeing how ubiquitously Python shows up as a sidekick, the scripting language Codex reaches for when it needs task-specific tools.]]></summary></entry><entry><title type="html">Thoughts on Richard Sutton’s interview on Dwarkesh Podcast</title><link href="https://alexdong.com/thoughts-on-richard-sutton-s-interview-on-dwarkesh-podcast.html" rel="alternate" type="text/html" title="Thoughts on Richard Sutton’s interview on Dwarkesh Podcast" /><published>2025-09-27T21:56:00+12:00</published><updated>2025-09-27T21:56:00+12:00</updated><id>https://alexdong.com/thoughts-on-richard-sutton-s-interview-on-dwarkesh-podcast</id><content type="html" xml:base="https://alexdong.com/thoughts-on-richard-sutton-s-interview-on-dwarkesh-podcast.html"><![CDATA[<p><a href="https://www.dwarkesh.com/p/richard-sutton">Richard Sutton</a> is a Turing Award
winner and one of the founding fathers of Reinforcement Learning, so I listened
to his interview on <a href="https://www.dwarkesh.com/p/richard-sutton">Dwarkesh
Podcast</a> three times, and what
Richard said opened my eyes to a few powerful perspectives, but I also disagree
with some of his points. This post is a collection of my thoughts on this
interview.</p>

<h2 id="llm-vs-rl">LLM vs RL</h2>

<blockquote>
  <p>What is intelligence? The problem is to understand your world. Reinforcement
learning is about understanding your world, whereas large language models are
about mimicking people, doing what people say you should do. They’re not
about figuring out what to do.</p>
</blockquote>

<p>Instead of “not about”, maybe Sutton meant was “not the best way to”. LLMs <em>can</em>
and <em>have been</em> used to figure out what to do. In fact, the whole idea of
<a href="/llm-evolutionary-test-time-compute.html">Evolutionary Test Time Compute</a> is
to use LLMs to figure out what to do, and this approach has made several novel
discoveries. But is that the most efficient way to figure out what to do, given
all the known constraints? Probably not.</p>

<blockquote>
  <p>To mimic what people say is not really to build a model of the world at all.
You’re mimicking things that have a model of the world: people. 
… 
A world model would enable you to predict what would happen. They have the
ability to predict what a person would say. They don’t have the ability to
predict what will happen.</p>
</blockquote>

<p>I think this is a false dichotomy. He’s contrasting linguistic prediction with
true physical modeling, yet LLMs can be used to build a model of the world.
<a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/">Google’s Genie
3</a>
is a great example of not learning from what people say but to learn the
fundamental physics of the world.</p>

<blockquote>
  <p>What we want, to quote Alan Turing, is a machine that can learn from
experience, where experience is the things that actually happen in your life.
You do things, you see what happens, and that’s what you learn from.</p>
</blockquote>

<p>It’s hard to argue that an LLM’s long context window is a form of “learning from
experience”. But I think LLMs can still provide the foundational substrate for
RL to build upon. RL from zero is conceptually appealing but why not leverage
the world’s knowledge encoded in LLMs to bootstrap the learning process?</p>

<p>(Sutton did raise some good point on why not. Please refer to  the “Knowledge
and Solution” section below to see why Richard might be correct.)</p>

<h2 id="ground-truth">Ground Truth</h2>

<blockquote>
  <p>There’s no ground truth. You can’t have prior knowledge if you don’t have
ground truth, because the prior knowledge is supposed to be a hint or an
initial belief about what the truth is.</p>
</blockquote>

<p>This is the part where I disagree the most. Sutton seems to be saying that ground
truth only exists in lived experience. But what is ground truth? If an LLM 
has a way to use <strong>deterministic</strong> tools to verify its own output, wouldn’t that
be a form of ground truth? After all, a compiler verifying code is a form of
ground truth, isn’t it?</p>

<blockquote>
  <p>Making a model of the physical world and carrying out the consequences of
mathematical assumptions or operations, those are very different things. The
empirical world has to be learned. You have to learn the consequences.
Whereas the math is more computational, it’s more like standard planning.
There they can have a goal to find the proof, and they are in some way given
that goal to find the proof.</p>
</blockquote>

<p>Richard didn’t go into details here, but I think he was referring to the fact
that the real world is non-deterministic, stochastic, fractal and
chaotic. So it can be computationally intractable to model the real world. RL
learning from the real world is not only learning to approximate the model of
the world but also learning to what to ignore or discard.</p>

<p>Still, I disagree. In <a href="https://www.youtube.com/watch?v=-HzgcbRXUK8">Demis Hassabis’s interview on Lex Fridman
podcast</a>, Demis made the case that
an LLM like Veo actually can produce “clean predictions about highly nonlinear
dynamical systems”. Can we ignore this example only because the videos generated 
by Veo are not “real world”? Even when our eyes and brains think they are? 
What is a “real world” other than a perception afforded to us by our senses?</p>

<p><a href="https://x.com/RichardSSutton/status/1971718688840864167">On X</a>, Sutton gave a
further example of why he didn’t believe LLM’s imitation is sufficient. He said:</p>

<blockquote>
  <p>Even in birdsong learning in zebra finches the motor actions are not learned
by imitation. The auditory result is reproduced, not the actions; in this
crucial way it differs from LLM training.</p>
</blockquote>

<p>But if we look at LLM’s embedding layer, isn’t it reproducing the result, instead
of the actions though?</p>

<h2 id="knowledge-and-solution-are-the-product-of-the-environment">Knowledge and Solution are the Product of the Environment</h2>

<blockquote>
  <p>In every case of the bitter lesson you could start with human knowledge and
then do the scalable things. That’s always the case. There’s never any reason
why that has to be bad. But in fact, and in practice, it has always turned
out to be bad. People get locked into the human knowledge approach, and they
psychologically… Now I’m speculating why it is, but this is what has always
happened. They get their lunch eaten by the methods that are truly scalable.</p>
</blockquote>

<p>This is one of the more important points Richard made. I think he is right
here. Human knowledge is bounded, constrained and shaped by our physical
hardware, so it is very likely that there exist many solutions to the same
problem that are beyond our comprehension.</p>

<p>Maybe the LLM as the DNA is bounded by the biochemistry of the earth
environment. If we want to explore the solution space for environment X, we
need to use RL to “do first principle thinking” with the goal to maximise
the best performance within that particular environment.</p>

<blockquote>
  <p>What you learn, your knowledge, is about the stream. Your knowledge is about
if you do some action, what will happen. Or it’s about which events will
follow other events. It’s about the stream. The content of the knowledge is
statements about the stream.</p>
</blockquote>

<p>I find this point very profound. It basically says that knowledge is a product
of interaction with the world. Dragonfly’s knowledge is different from a
shark’s, even though they share the same “prey” goal. This is because these two
animals operate in two very different worlds. The shark’s eyes are specialised
for low-light conditions and its gray-scale vision is perfect for detecting
movements in the water. The dragonfly’s eyes are specialised for detecting
small insects, and its visual neurons send signals to the dragonfly’s four
wings to allow it to make rapid adjustments in flight.</p>

<p>I think Richard made a powerful point that knowledge is not static. Rather,
once the die is cast, RL remains the best way to produce the most efficient
solution to a problem <strong>given a set of constraints</strong>. I think he is also right
that the solution is <em>predetermined</em> by a set of constraints.</p>

<blockquote>
  <p>You learn a policy that’s specific to the environment that you’re finding
yourself in.</p>
</blockquote>

<p>Persona. Policy. An environment for evolution. They are all the same thing to
describe the constraints, which fundamentally shape the solution space.</p>

<h2 id="transfer-learning">Transfer Learning</h2>

<blockquote>
  <p>What we have are people trying different things and they settle on something,
a representation that transfers well or generalizes well. But we have very
few automated techniques to promote transfer, and none of them are used in
modern deep learning.</p>
</blockquote>

<p>If knowledge is tuned to an environment, it’s no surprise that porting it
elsewhere is hard. This is another really profound point. I think what Richard
was saying is that we don’t know how to “transfer” knowledge from one domain to
another. Or more precisely, it is impossible to transfer knowledge across
domains the same way you can’t really translate a poem across different
cultures. One has to leverage RL to find the optimal representation for the new
domain.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Richard Sutton is a Turing Award winner and one of the founding fathers of Reinforcement Learning, so I listened to his interview on Dwarkesh Podcast three times, and what Richard said opened my eyes to a few powerful perspectives, but I also disagree with some of his points. This post is a collection of my thoughts on this interview.]]></summary></entry><entry><title type="html">Less is More for X</title><link href="https://alexdong.com/less-is-more-for-x.html" rel="alternate" type="text/html" title="Less is More for X" /><published>2025-09-26T22:58:00+12:00</published><updated>2025-09-26T22:58:00+12:00</updated><id>https://alexdong.com/less-is-more-for-x</id><content type="html" xml:base="https://alexdong.com/less-is-more-for-x.html"><![CDATA[<p><a href="https://x.com/joecole">Joe</a> turned me onto a series of papers by <a href="https://scholar.google.com/citations?hl=en&amp;user=oIz_CYEAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Pengfei
Liu</a>
that challenge the conventional wisdom that “more data is always better” for
post-training LLMs.</p>

<p>Collectively, these papers show that for
<a href="https://arxiv.org/pdf/2502.11886">RL</a>,
<a href="https://arxiv.org/pdf/2502.03387">Reasoning</a> and
<a href="https://arxiv.org/pdf/2509.17567">Agentic</a> tasks, a smaller yet higher-quality
training dataset can outperform similar-scale ones, like OpenAI-o1-preview. For
example, in the case of Reasoning, using only 1% of the training data yields
a 45.8% absolute improvement on math and programming tasks. I thought it was
worth sharing a few details from the reasoning paper.</p>

<p>On the hypothesis:</p>

<blockquote>
  <p>We hypothesize that successful reasoning emerges from the synergy of rich
pre-trained knowledge and sufficient computational resources at inference
time. These developments suggest that if models possess rich reasoning
knowledge and adequate computational space, activating their reasoning
capabilities may require only a small number of high-quality samples that
encourage extended deliberation, rather than massive fine-tuning datasets. We
propose the Less-Is-More Reasoning (LIMO) Hypothesis, identifying two
critical factors determining the elicitation threshold for complex reasoning:
(1) the latent presence of prerequisite knowledge within the model’s
parameters, and (2) the effectiveness of minimal exemplars in demonstrating
problem-solving processes that encourage extended deliberation. The sample
efficiency of eliciting advanced reasoning is thus bounded by the model’s
encoded knowledge foundation and its exposure to training samples that
effectively utilize inference-time computation space.</p>
</blockquote>

<p>When they describe how they constructed the smaller, higher-quality dataset,
they emphasize that the “higher quality” label refers to the most difficult
problems. Rather than gradually increasing difficulty as in
<a href="https://en.wikipedia.org/wiki/Curriculum_learning">Curriculum Learning</a>, they
focus on that challenging tail.</p>

<blockquote>
  <p>We implemented a systematic multi-stage filtration pipeline … we first
applied a baseline difficulty filter using a short-CoT mathematical model.
Problems that this model solved correctly within four attempts were excluded,
ensuring that only non-trivial problems remained. Next, we sampled 32
solution attempts and used the empirical success rate as a difficulty
indicator. Problems that were successfully solved in only 1–3 out of 32
attempts were retained.</p>
</blockquote>

<p>After refining the dataset this way, they constructed the training trace with
larger reasoning models and a set of heuristics:</p>

<blockquote>
  <p>Elaborated Reasoning: Comprehensive exploration of logical steps without
premature conclusions</p>

  <p>Self-Verification: Regular validation of intermediate results and logical
consistency</p>

  <p>Exploratory Approach: Consideration of multiple possibilities before reaching
conclusions</p>

  <p>Adaptive Granularity: Appropriate detail level across simple and complex
deductions</p>

  <p>To quantify these qualities, we implemented a rule-based scoring system that
calculated weighted metrics for each dimension. Elaborated Reasoning was
measured by solution length (30% weight); Self-Verification through frequency
of validation-related words like “check” and “verify” (20% weight);
Exploratory Approach by counting tentative expressions such as “perhaps” and
“might” (25% weight); and Adaptive Granularity via connective phrases like
“therefore” and “since” (25% weight). All keyword frequencies were normalized
by text length to ensure fair comparison across solutions of different sizes.</p>
</blockquote>

<p>Taken together, the results are quite impressive. Still, the more I think about
it, the more I feel that this process of curating a smaller, higher-quality
dataset is essentially a manual distillation process that transfers the
capability from larger reasoning models to smaller ones. Because none of the
papers include an ablation study, it’s unclear to me how much of the improvement
is due to the curated dataset vs. the fact that the larger models are used to
generate the training trace.</p>]]></content><author><name></name></author><category term="ai-training" /><summary type="html"><![CDATA[Joe turned me onto a series of papers by Pengfei Liu that challenge the conventional wisdom that “more data is always better” for post-training LLMs.]]></summary></entry><entry><title type="html">How Prompt Writing Style Shapes GPT-5 and Codex</title><link href="https://alexdong.com/how-prompt-writing-style-shapes-gpt-5-and-codex.html" rel="alternate" type="text/html" title="How Prompt Writing Style Shapes GPT-5 and Codex" /><published>2025-09-25T21:06:00+12:00</published><updated>2025-09-25T21:06:00+12:00</updated><id>https://alexdong.com/how-prompt-writing-style-shapes-gpt-5-and-codex</id><content type="html" xml:base="https://alexdong.com/how-prompt-writing-style-shapes-gpt-5-and-codex.html"><![CDATA[<p>One of the things that impresses me about OpenAI GPT-5-Codex is how responsive
and “light” it feels to use. Compared with the page-long dumps from
<a href="/due-to-odd-jax-issues.html">Claude Code</a>, GPT-5-Codex stays efficient,
more to the point, and much more steerable. Some of that is the
<code class="language-plaintext highlighter-rouge">gpt-5-codex</code> model itself, but I suspect a lot of it comes from the prompt
engineering that OpenAI has done.</p>

<p>So I pulled up the <a href="https://github.com/openai/codex/blob/rust-v0.36.0/codex-rs/core/gpt_5_codex_prompt.md">system prompt for
Codex</a>
and set it beside the <a href="https://www.reddit.com/r/PromptEngineering/comments/1mknun8/i_have_extracted_the_gpt5_system_prompt/">leaked system prompt for
GPT-5</a> and I noticed the following differences:</p>

<ol>
  <li>
    <p>The Codex sentences are much <strong>denser</strong>. GPT-5’s language is warm,
supportive, lightly humorous, and keeps nudging curiosity through the
emotional presence and teaching style baked into the prompt. Codex is
neutral, concise, factual, and collaborative. It intentionally strips
personality to stay clear. I ran a “clause density” check on both prompts:
Codex averages 4.8 clauses per sentence, GPT-5 comes in at 2.1.</p>
  </li>
  <li>
    <p>Codex seems to have a much <strong>less constrained dialogue flow</strong>. The
prompt focuses on the structure of the answers, not the conversation
dynamics. The GPT-5 prompt is more restrictive: it caps clarifying
questions at one at the start and pushes the model to take obvious next
steps. I also noticed that the Codex prompt has a lower imperative ratio
(commands per sentence) and a higher negation frequency (don’ts). That mix is
odd, and I need to think more about what it means.</p>
  </li>
  <li>
    <p>The Codex content guidance emphasizes <strong>brevity</strong> and <strong>scanability</strong>. It
pushes headers, bullets, and grouped points. The GPT-5 prompt emphasizes
<strong>tone</strong> and <strong>engagement</strong> with instructions like “Supportive
thoroughness”, “Lighthearted interactions”, “Adaptive teaching”, and
“Confidence-building”.</p>
  </li>
  <li>
    <p>Codex adapts based on <strong>task</strong> type (code explanations, simple tasks,
big changes, casual one-offs) whereas the GPT-5 prompt adjusts based on <strong>user</strong>
proficiency and emotional needs. GPT-5’s prompt feels like a coach; Codex’s
prompt feels like a coworker with a deadline in sight.</p>
  </li>
  <li>
    <p>Based on the readability analysis, the Codex prompt is written for
university graduates (Flesch-Kincaid Grade Level 14) whereas the GPT-5 prompt
comes in at high school level (Flesch-Kincaid Grade Level 10).</p>
  </li>
</ol>

<p>Here’s what that looks like in the numbers:</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>GPT-5-Codex (Prompt B)</th>
      <th>GPT-5 (Prompt A)</th>
      <th>Interpretation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Lexical Density</strong></td>
      <td>0.82</td>
      <td>0.78</td>
      <td>Codex is more content-word heavy</td>
    </tr>
    <tr>
      <td><strong>Type–Token Ratio (TTR)</strong></td>
      <td>0.77</td>
      <td>0.68</td>
      <td>Codex uses more varied vocabulary</td>
    </tr>
    <tr>
      <td><strong>Flesch Reading Ease</strong></td>
      <td>29.0</td>
      <td>42.1</td>
      <td>Codex is harder to read (graduate level)</td>
    </tr>
    <tr>
      <td><strong>Flesch–Kincaid Grade</strong></td>
      <td>14.1</td>
      <td>10.4</td>
      <td>Codex ≈ college sophomore; GPT-5 ≈ high school</td>
    </tr>
    <tr>
      <td><strong>Gunning Fog Index</strong></td>
      <td>16.6</td>
      <td>13.9</td>
      <td>Codex significantly denser, more technical</td>
    </tr>
    <tr>
      <td><strong>SMOG Index</strong></td>
      <td>15.0</td>
      <td>12.6</td>
      <td>Codex ~Grade 15 vs. GPT-5 ~Grade 12</td>
    </tr>
    <tr>
      <td><strong>Punctuation Load</strong></td>
      <td>3.2</td>
      <td>0.9</td>
      <td>Codex has ~3.5× more commas/semicolons</td>
    </tr>
    <tr>
      <td><strong>Imperative Ratio</strong></td>
      <td>0.10</td>
      <td>0.25</td>
      <td>GPT-5 gives more direct instructions</td>
    </tr>
    <tr>
      <td><strong>Negation Frequency</strong></td>
      <td>6</td>
      <td>3</td>
      <td>Codex has more “don’ts” and negatives</td>
    </tr>
  </tbody>
</table>

<hr />

<p>GPT-5-Codex System Prompt</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>### Final answer structure and style guidelines

- Plain text; CLI handles styling. Use structure only when it helps scanability.
- Headers: optional; short Title Case (1-3 words) wrapped in **…**; no blank line before the first bullet; add only if they truly help.
- Bullets: use - ; merge related points; keep to one line when possible; 4–6 per list ordered by importance; keep phrasing consistent.
- Monospace: backticks for commands/paths/env vars/code ids and inline examples; use for literal keyword bullets; never combine with **.
- Code samples or multi-line snippets should be wrapped in fenced code blocks; add a language hint whenever obvious.
- Structure: group related bullets; order sections general → specific → supporting; for subsections, start with a bolded keyword bullet, then items; match complexity to the task.
- Tone: collaborative, concise, factual; present tense, active voice; self‑contained; no "above/below"; parallel wording.
- Don'ts: no nested bullets/hierarchies; no ANSI codes; don't cram unrelated keywords; keep keyword lists short—wrap/reformat if long; avoid naming formatting styles in answers.
- Adaptation: code explanations → precise, structured with code refs; simple tasks → lead with outcome; big changes → logical walkthrough + rationale + next actions; casual one-offs → plain sentences, no headers/bullets.
</code></pre></div></div>

<hr />

<p>GPT-5 System Prompt</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Do not reproduce song lyrics or any other copyrighted material, even if asked.

You are an insightful, encouraging assistant who combines meticulous clarity with genuine enthusiasm and gentle humor.

Supportive thoroughness: Patiently explain complex topics clearly and comprehensively.

Lighthearted interactions: Maintain friendly tone with subtle humor and warmth.

Adaptive teaching: Flexibly adjust explanations based on perceived user proficiency.

Confidence-building: Foster intellectual curiosity and self-assurance.

Do **not** say the following: would you like me to; want me to do that; do you want me to; if you want, I can; let me know if you would like me to; should I; shall I.

Ask at most one necessary clarifying question at the start, not the end.

If the next step is obvious, do it. Example of bad: I can write playful examples. would you like me to? Example of good: Here are three playful examples:..
</code></pre></div></div>]]></content><author><name></name></author><category term="prompt-engineering" /><category term="open-ai" /><summary type="html"><![CDATA[One of the things that impresses me about OpenAI GPT-5-Codex is how responsive and “light” it feels to use. Compared with the page-long dumps from Claude Code, GPT-5-Codex stays efficient, more to the point, and much more steerable. Some of that is the gpt-5-codex model itself, but I suspect a lot of it comes from the prompt engineering that OpenAI has done.]]></summary></entry></feed>
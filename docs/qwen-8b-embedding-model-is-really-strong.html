<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Qwen-8B Embeddings: Near-SOTA Performance at 600x the Speed | Alex Dong’s Blog</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Qwen-8B Embeddings: Near-SOTA Performance at 600x the Speed" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="TLDR: The Qwen-8B embedding model delivers near state-of-the-art performance on text classification tasks while running 600x faster than LLM-based approaches." />
<meta property="og:description" content="TLDR: The Qwen-8B embedding model delivers near state-of-the-art performance on text classification tasks while running 600x faster than LLM-based approaches." />
<link rel="canonical" href="https://alexdong.com/qwen-8b-embedding-model-is-really-strong.html" />
<meta property="og:url" content="https://alexdong.com/qwen-8b-embedding-model-is-really-strong.html" />
<meta property="og:site_name" content="Alex Dong’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-09-16T22:06:00+12:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Qwen-8B Embeddings: Near-SOTA Performance at 600x the Speed" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-16T22:06:00+12:00","datePublished":"2025-09-16T22:06:00+12:00","description":"TLDR: The Qwen-8B embedding model delivers near state-of-the-art performance on text classification tasks while running 600x faster than LLM-based approaches.","headline":"Qwen-8B Embeddings: Near-SOTA Performance at 600x the Speed","mainEntityOfPage":{"@type":"WebPage","@id":"https://alexdong.com/qwen-8b-embedding-model-is-really-strong.html"},"url":"https://alexdong.com/qwen-8b-embedding-model-is-really-strong.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://alexdong.com/feed.xml" title="Alex Dong&apos;s Blog" /></head><body><header class="site-header" role="banner">
  <div class="wrapper">
    <div style="text-align: center;">
      <a class="site-title" rel="author" href="/">Stream of thoughts from Alex Dong</a>
      <p style="margin: 0; font-size: 0.875rem; color: var(--meta-color);">What I find interesting, intriguing or insightful</p>
    </div>
  </div>
</header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Qwen-8B Embeddings: Near-SOTA Performance at 600x the Speed</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-09-16T22:06:00+12:00" itemprop="datePublished">Sep 16, 2025
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>TLDR: The Qwen-8B embedding model delivers near state-of-the-art performance on text
classification tasks while running 600x faster than LLM-based approaches.</p>

<p>For the <a href="https://www.kaggle.com/competitions/map-charting-student-math-misunderstandings">Kaggle
MAP</a>
competition I’m working on, I’ve had a chance to try out different embedding
models for text classification tasks.</p>

<p>The setup is pretty simple. A sentence comes in. I’ll use an embedding model to
encode it into a vector, which is then fed into a 3-layer MLP classifier.</p>

<p>I started with
<a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"><code class="language-plaintext highlighter-rouge">sentence-transformers/all-MiniLM-L6-v2</code></a>,
a 22.7M parameter model. It was easy to put together (thanks to the
<code class="language-plaintext highlighter-rouge">sentence-transformers</code> Python package) and I got a proof of concept working in
a few hours. After some Optuna hyperparameter search, the system plateaued at
around 0.908 MAP score. A respectable result but not competitive enough for the
Kaggle leaderboard.</p>

<p>Two weeks ago, DeepMind released
<a href="https://huggingface.co/google/embeddinggemma-300m">embedding-Gemma-300M</a>.
According to the release note, it outperforms <code class="language-plaintext highlighter-rouge">all-MiniLM-L6-v2</code> on a variety
of tasks. It even beats <code class="language-plaintext highlighter-rouge">multilingual-e5-large</code>, a model that’s almost twice
its size, on all <a href="https://developers.googleblog.com/en/introducing-embeddinggemma/">MTEB
tasks</a>. I did
see some improvement but it wasn’t significant.</p>

<p>I thought maybe I had hit the ceiling of what the MLP architecture could do. So I
started playing around with using the LLM itself as the classifier. It did give me
a better score, but each inference takes 10-20 seconds—too slow to process
the entire test dataset within the 9-hour Kaggle time limit.</p>

<p>This past Sunday, it suddenly occurred to me that I could use the Qwen-8B
model purely as an embedding model. This isn’t supported out of the
box by <code class="language-plaintext highlighter-rouge">sentence-transformers</code>, but it was easy to vibe code it through the
<code class="language-plaintext highlighter-rouge">llama-cpp-python</code> package.</p>

<p>The result? Extraordinarily good. The MAP score jumped to 0.9439, a significant
improvement over the previous two models.</p>

<p>As context, the current top score on the Kaggle Leaderboard is 0.952. This
almost embarrassingly simple MLP approach takes less than 0.03 seconds
per inference, yet achieves a score remarkably close to the top.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Parameters</th>
      <th>Embedding Dimensions</th>
      <th>MAP@3 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>all-MiniLM-L6-v2</td>
      <td>32M</td>
      <td>384</td>
      <td>0.9082</td>
    </tr>
    <tr>
      <td>Gemma-300M</td>
      <td>300M</td>
      <td>768</td>
      <td>0.9101</td>
    </tr>
    <tr>
      <td>Qwen3-8B</td>
      <td>8B</td>
      <td>4096</td>
      <td>0.9439</td>
    </tr>
  </tbody>
</table>

<p>I’ve long been aware of the hypothesis that if the representation is good
enough, downstream tasks can be solved with really simple models. But this
is the first time I’ve seen it in action and it’s truly impressive to see it
work so well with so little resources.</p>

<p>The concept of <a href="https://en.wikipedia.org/wiki/Feature_learning">Representation
Learning</a> always fascinates me.
The idea that the entire world’s complexity can be squeezed into 10k-dimensional vectors
is mind-boggling.</p>


  </div><a class="u-url" href="/qwen-8b-embedding-model-is-really-strong.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer">
  <div class="wrapper">
    <div style="text-align: center;">
      <a class="u-email" href="mailto:me@alexdong.com">me@alexdong.com</a>
    </div>
  </div>
</footer></body>

</html>

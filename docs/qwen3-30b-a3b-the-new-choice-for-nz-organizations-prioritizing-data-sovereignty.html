<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Qwen3-30B: The first AI model that is good enough for local deployment | Alex Dong’s Blog</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Qwen3-30B: The first AI model that is good enough for local deployment" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Finally good enough" />
<meta property="og:description" content="Finally good enough" />
<link rel="canonical" href="https://alexdong.com/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty.html" />
<meta property="og:url" content="https://alexdong.com/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty.html" />
<meta property="og:site_name" content="Alex Dong’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-01T14:30:00+12:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Qwen3-30B: The first AI model that is good enough for local deployment" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-08-01T14:30:00+12:00","datePublished":"2025-08-01T14:30:00+12:00","description":"Finally good enough","headline":"Qwen3-30B: The first AI model that is good enough for local deployment","mainEntityOfPage":{"@type":"WebPage","@id":"https://alexdong.com/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty.html"},"url":"https://alexdong.com/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="preconnect" href="https://cdn.jsdelivr.net">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fontsource/iosevka@5.0.8/index.min.css">
  <link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://alexdong.com/feed.xml" title="Alex Dong&apos;s Blog" /></head><body><header class="site-header" role="banner">
  <div class="wrapper">
    <div style="text-align: center;">
      <a class="site-title" rel="author" href="/">Stream of Thoughts from Alex Dong</a>
      <p style="margin: 0; font-size: 0.875rem; color: var(--meta-color);">What I find interesting, intriguing or insightful</p>
    </div>
  </div>
</header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Qwen3-30B: The first AI model that is good enough for local deployment</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-08-01T14:30:00+12:00" itemprop="datePublished">Aug 1, 2025
      </time><span class="post-categories"><span class="post-category p-category">data-sovereignty</span><span aria-hidden="true"> | </span><span class="post-category p-category">local-ai</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="finally-good-enough">Finally good enough</h2>

<p>In the past a few months, I’ve been asked by a number of New Zealand startups about the best local AI model.  I’ve reluctantly recommended Llama 3.3 70B
with 8-bit quantization but I would always quickly follow up with the caveat that
this setup is “really only a toy”. “If you want to do real work, you should
at least use Claude Haiku 3.5 or even Sonnet 4.0”, I would say.</p>

<p>But With the arrival of <a href="https://qwenlm.github.io/blog/qwen3-30b-a3b/">Qwen3-30B-A3B-Instruct-2507</a>,
I think we finally have an AI model that is good enough for fast local
interactive exploration and production use. There are two main reasons I feel this way:</p>

<p><strong>Higher output quality</strong>. Based on my own benchmarks, Qwen3-30B’s performance
in coding tasks is slightly behind than ChatGPT-4o, but noticeably
higher than Claude’s haiku-3.5-20241022, a workhorse model that has been my 
go-to for most of my personal projects. To surpass haiku 3.5 means that it has
a good-enough core cognitive capabilities that it can now be used to build real 
applications.</p>

<p><strong>Faster inference speed</strong>. Qwen3-30B runs at 78 tokens/second on M4 Max with
128GB RAM (with MLX optimisation turned on), this feels a lot faster than haiku
3.5 streamed from Claude’s API, which typically runs at 52-68 tokens/second speed.</p>

<p>Read on for a deeper dive on 6 reasons why you should consider Qwen3-30B-A3B for
your local AI needs.</p>

<h2 id="1-benchmark-result-matching-larger-models">1. Benchmark result matching larger models</h2>

<p>Despite having just 30B parameters, Qwen3-30B-A3B competes with models 4-30 times its size:</p>

<table>
  <thead>
    <tr>
      <th>Benchmark</th>
      <th>Qwen3-30B-A3B</th>
      <th>GPT-4o</th>
      <th>Claude 3.5 Sonnet</th>
      <th>Gemini 1.5 Pro</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>ArenaHard</strong></td>
      <td><strong>91.0</strong></td>
      <td>85.3</td>
      <td>87.1</td>
      <td>-</td>
      <td>Complex reasoning &amp; instruction following</td>
    </tr>
    <tr>
      <td><strong>AIME’24/25</strong></td>
      <td><strong>80.4</strong></td>
      <td>41.4</td>
      <td>-</td>
      <td>52.7</td>
      <td>Advanced mathematical problem-solving</td>
    </tr>
    <tr>
      <td><strong>GPQA</strong></td>
      <td><strong>70.4%</strong></td>
      <td>65.1%</td>
      <td>72.3% (Opus)</td>
      <td>-</td>
      <td>Graduate-level science questions</td>
    </tr>
    <tr>
      <td><strong>LiveBench</strong></td>
      <td><strong>69.0</strong></td>
      <td>68.2 (GPT-4)</td>
      <td>-</td>
      <td>65.8 (Flash)</td>
      <td>Real-world task performance</td>
    </tr>
    <tr>
      <td><strong>Creative Writing</strong></td>
      <td><strong>86.0</strong></td>
      <td>84.2</td>
      <td>83.7 (Haiku)</td>
      <td>-</td>
      <td>Writing quality assessment</td>
    </tr>
  </tbody>
</table>

<p>For real-world applications, <a href="https://simonwillison.net/2025/Jul/29/qwen3-30b-a3b-instruct-2507/">Simon
Willison</a>
confirms these results in practical applications, noting performance
“approaching GPT-4o and larger Qwen models.”</p>

<h2 id="2-the-speed-advantage-through-moe-architecture">2. The speed advantage through MoE architecture</h2>

<p>Qwen3-30B-A3B uses the Mixture of Experts (MoE) architecture. Think of
Qwen3-30B-A3B as having 128 specialist consultants on staff, but only calling
on the 8 most relevant experts for each task. This architecture means that the
model runs at the speed of a much smaller 3.3B parameter system, yet it is still
capable for a wide range of cognitive intensive tasks.</p>

<h2 id="3-almost-one-click-local-deployment">3. (Almost) one-click local deployment</h2>

<p>The <a href="https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-MLX-8bit">MLX 8-bit
model</a>
is already available to download from Hugging Face. Simon Willison’s <a href="https://simonwillison.net/2025/Jul/29/qwen3-30b-a3b-instruct-2507/">deployment
guide</a>
provides additional details to get you started.</p>

<h2 id="4-production-deployment-made-affordable">4. Production deployment made affordable</h2>

<p>For deployment, the easiest way is to run the model via LM Studio on a 
<a href="https://www.apple.com/nz/shop/buy-mac/mac-mini/apple-m4-pro-chip-with-12-core-cpu-16-core-gpu-24gb-memory-512gb">Mac Mini M4 Pro with 64GB RAM</a>. 
79 tokens/second for $4,299 NZD. Not bad at all.</p>

<p>Or, you can go with one <a href="https://www.pbtech.co.nz/product/VGAASU350901/ASUS-ROG-ASTRAL-NVIDIA-GeForce-RTX-5090-OC-GAMING">RTX 5090 GPU (NZD $6,799 on
PBTech)</a>, which gives
about 48 tokens/seconds.</p>

<p>Note that even though the GPU approach appears to be slower than M4 Pro, it
does open up the options to explore more scalable runtime/pipeline options, e.g.
<a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF">unsloth</a>. Also, you get to
choose different parameters for <a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF">Thinking and Non-Thinking
Mode</a> or use
<a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb">GRPO</a>
for fine tuning.</p>

<h2 id="5-tool-use-and-hybrid-thinking-mode">5. Tool use and hybrid thinking mode</h2>

<p>Qwen3-30B-A3B brings excellent function calling capabilities to local
deployment — a feature notably absent in Llama 3.3 70B. Qwen3-30B-A3B further
simplifies this task with the <a href="https://github.com/QwenLM/Qwen-Agent">Qwen-Agent
framework</a>, providing built-in support
for:</p>

<ul>
  <li><strong>MCP (Model Context Protocol)</strong> configuration for standardized tool definitions</li>
  <li><strong>Native tool integration</strong> including code interpreters and API calls</li>
  <li><strong>Hybrid thinking modes</strong> that switch between deep reasoning and fast response</li>
</ul>

<h2 id="6-apache-20-licensing">6. Apache 2.0 licensing</h2>

<p>Qwen3-30B-A3B adopts the Apache 2.0 license, which is also a much-welcomed change
from the Llama 3.3 license. The Apache 2.0 license is one of the most permissive and
widely accepted open-source licenses, allowing you to use, modify, and
distribute the model with minimal restrictions. It’s the same license powering
many open source projects so your legal team probably already knows it. 
This contrasts sharply with Llama’s custom license, which imposes user count
thresholds and revenue restrictions that can complicate commercial use.</p>

<h2 id="but-isnt-qwen3-from-china">But, isn’t Qwen3 from China?</h2>

<p>Yes. But the beauty of local deployment lies in its complete neutrality.
Whether a model comes from Silicon Valley, Beijing, or Paris becomes irrelevant
when it runs exclusively on your hardware. Qwen3-30B-A3B offers the same data
sovereignty guarantees as any locally-deployed software: your data stays on
your servers, processed by your infrastructure, governed by your policies.</p>

<h2 id="models-come-and-go">Models come and go</h2>

<p>You see, the AI landscape changes weekly. New models, new capabilities, new price
points. Without a robust evaluation framework, you’re flying blind — making
decisions based on vendor marketing rather than measured performance that’s 
relevant to your specific use cases.</p>

<p>BTW, your evaluation framework should be the bedrock of your AI strategy, not just a
tool for comparing models. It should help you answer three questions:</p>

<ol>
  <li>Does this model solve our users’ actual problems?</li>
  <li>Can we measure improvement and progress objectively?</li>
  <li>How do we capture feedback to improve continuously?</li>
</ol>

<p>It worth noting that these capabilities matter more than which model you choose today, because they
determine how well you’ll adapt to whatever comes next. While models depreciate
rapidly—today’s state-of-the-art becomes tomorrow’s baseline—your evaluation
framework appreciates with use. Each test case refined, each edge case
captured, each performance metric validated adds to an irreplaceable asset.</p>

<p>So, while models come and go, your evaluation framework remains a long-term asset.
If your business is serious about AI, invest in building a robust evaluation framework.</p>

  </div><a class="u-url" href="/qwen3-30b-a3b-the-new-choice-for-nz-organizations-prioritizing-data-sovereignty.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer">
  <div class="wrapper">
    <div style="text-align: center;">
      <a class="u-email" href="mailto:me@alexdong.com">me@alexdong.com</a>
    </div>
  </div>
</footer></body>

</html>
